I0715 11:18:24.092494 18508 caffe.cpp:178] Use CPU.
I0715 11:18:24.093016 18508 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 50
base_lr: 0.01
display: 100
max_iter: 5000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0
snapshot: 1000
snapshot_prefix: "examples/siamese/My_mnist_siamese_0to1_sim"
solver_mode: CPU
net: "examples/siamese/mnist_siamese_train_test_sim.prototxt"
I0715 11:18:24.093222 18508 solver.cpp:91] Creating training net from net file: examples/siamese/mnist_siamese_train_test_sim.prototxt
I0715 11:18:24.093971 18508 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer pair_data
I0715 11:18:24.094034 18508 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0715 11:18:24.094202 18508 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test_sim"
state {
  phase: TRAIN
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/shaogang/caffe/examples/siamese/mnist_siamese_train_leveldb_0to1"
    batch_size: 64
    backend: LEVELDB
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "concat"
  type: "Concat"
  bottom: "feat"
  bottom: "feat_p"
  top: "comb"
  concat_param {
    axis: 1
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "comb"
  top: "fc1"
  param {
    name: "fc1_w"
    lr_mult: 1
  }
  param {
    name: "fc1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_fc1"
  type: "ReLU"
  bottom: "fc1"
  top: "fc1"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "fc1"
  top: "fc2"
  param {
    name: "fc2_w"
    lr_mult: 1
  }
  param {
    name: "fc2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2_fc2"
  type: "ReLU"
  bottom: "fc2"
  top: "fc2"
}
layer {
  name: "fc3"
  type: "InnerProduct"
  bottom: "fc2"
  top: "fc3"
  param {
    name: "fc3_w"
    lr_mult: 1
  }
  param {
    name: "fc3_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc3"
  bottom: "label"
  top: "loss"
}
I0715 11:18:24.094378 18508 layer_factory.hpp:77] Creating layer pair_data
I0715 11:18:24.094949 18508 net.cpp:91] Creating Layer pair_data
I0715 11:18:24.094985 18508 net.cpp:399] pair_data -> pair_data
I0715 11:18:24.095118 18508 net.cpp:399] pair_data -> label
I0715 11:18:24.107739 18512 db_leveldb.cpp:18] Opened leveldb /home/shaogang/caffe/examples/siamese/mnist_siamese_train_leveldb_0to1
I0715 11:18:24.108171 18508 data_layer.cpp:41] output data size: 64,2,28,28
I0715 11:18:24.108819 18508 net.cpp:141] Setting up pair_data
I0715 11:18:24.108866 18508 net.cpp:148] Top shape: 64 2 28 28 (100352)
I0715 11:18:24.108880 18508 net.cpp:148] Top shape: 64 (64)
I0715 11:18:24.108885 18508 net.cpp:156] Memory required for data: 401664
I0715 11:18:24.108899 18508 layer_factory.hpp:77] Creating layer slice_pair
I0715 11:18:24.108924 18508 net.cpp:91] Creating Layer slice_pair
I0715 11:18:24.108932 18508 net.cpp:425] slice_pair <- pair_data
I0715 11:18:24.108947 18508 net.cpp:399] slice_pair -> data
I0715 11:18:24.108963 18508 net.cpp:399] slice_pair -> data_p
I0715 11:18:24.108975 18508 net.cpp:141] Setting up slice_pair
I0715 11:18:24.108983 18508 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0715 11:18:24.108989 18508 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0715 11:18:24.108994 18508 net.cpp:156] Memory required for data: 803072
I0715 11:18:24.108999 18508 layer_factory.hpp:77] Creating layer conv1
I0715 11:18:24.109025 18508 net.cpp:91] Creating Layer conv1
I0715 11:18:24.109060 18508 net.cpp:425] conv1 <- data
I0715 11:18:24.109097 18508 net.cpp:399] conv1 -> conv1
I0715 11:18:24.109187 18508 net.cpp:141] Setting up conv1
I0715 11:18:24.109205 18508 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0715 11:18:24.109210 18508 net.cpp:156] Memory required for data: 3752192
I0715 11:18:24.109225 18508 layer_factory.hpp:77] Creating layer pool1
I0715 11:18:24.109237 18508 net.cpp:91] Creating Layer pool1
I0715 11:18:24.109242 18508 net.cpp:425] pool1 <- conv1
I0715 11:18:24.109249 18508 net.cpp:399] pool1 -> pool1
I0715 11:18:24.109282 18508 net.cpp:141] Setting up pool1
I0715 11:18:24.109289 18508 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0715 11:18:24.109294 18508 net.cpp:156] Memory required for data: 4489472
I0715 11:18:24.109299 18508 layer_factory.hpp:77] Creating layer conv2
I0715 11:18:24.109313 18508 net.cpp:91] Creating Layer conv2
I0715 11:18:24.109316 18508 net.cpp:425] conv2 <- pool1
I0715 11:18:24.109326 18508 net.cpp:399] conv2 -> conv2
I0715 11:18:24.109632 18508 net.cpp:141] Setting up conv2
I0715 11:18:24.109647 18508 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0715 11:18:24.109652 18508 net.cpp:156] Memory required for data: 5308672
I0715 11:18:24.109663 18508 layer_factory.hpp:77] Creating layer pool2
I0715 11:18:24.109670 18508 net.cpp:91] Creating Layer pool2
I0715 11:18:24.109675 18508 net.cpp:425] pool2 <- conv2
I0715 11:18:24.109684 18508 net.cpp:399] pool2 -> pool2
I0715 11:18:24.109695 18508 net.cpp:141] Setting up pool2
I0715 11:18:24.109702 18508 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0715 11:18:24.109706 18508 net.cpp:156] Memory required for data: 5513472
I0715 11:18:24.109711 18508 layer_factory.hpp:77] Creating layer ip1
I0715 11:18:24.109732 18508 net.cpp:91] Creating Layer ip1
I0715 11:18:24.109737 18508 net.cpp:425] ip1 <- pool2
I0715 11:18:24.109745 18508 net.cpp:399] ip1 -> ip1
I0715 11:18:24.113936 18508 net.cpp:141] Setting up ip1
I0715 11:18:24.113998 18508 net.cpp:148] Top shape: 64 500 (32000)
I0715 11:18:24.114002 18508 net.cpp:156] Memory required for data: 5641472
I0715 11:18:24.114015 18508 layer_factory.hpp:77] Creating layer relu1
I0715 11:18:24.114025 18508 net.cpp:91] Creating Layer relu1
I0715 11:18:24.114029 18508 net.cpp:425] relu1 <- ip1
I0715 11:18:24.114035 18508 net.cpp:386] relu1 -> ip1 (in-place)
I0715 11:18:24.114047 18508 net.cpp:141] Setting up relu1
I0715 11:18:24.114050 18508 net.cpp:148] Top shape: 64 500 (32000)
I0715 11:18:24.114053 18508 net.cpp:156] Memory required for data: 5769472
I0715 11:18:24.114055 18508 layer_factory.hpp:77] Creating layer ip2
I0715 11:18:24.114064 18508 net.cpp:91] Creating Layer ip2
I0715 11:18:24.114068 18508 net.cpp:425] ip2 <- ip1
I0715 11:18:24.114073 18508 net.cpp:399] ip2 -> ip2
I0715 11:18:24.114120 18508 net.cpp:141] Setting up ip2
I0715 11:18:24.114123 18508 net.cpp:148] Top shape: 64 10 (640)
I0715 11:18:24.114125 18508 net.cpp:156] Memory required for data: 5772032
I0715 11:18:24.114130 18508 layer_factory.hpp:77] Creating layer feat
I0715 11:18:24.114135 18508 net.cpp:91] Creating Layer feat
I0715 11:18:24.114138 18508 net.cpp:425] feat <- ip2
I0715 11:18:24.114142 18508 net.cpp:399] feat -> feat
I0715 11:18:24.114151 18508 net.cpp:141] Setting up feat
I0715 11:18:24.114153 18508 net.cpp:148] Top shape: 64 2 (128)
I0715 11:18:24.114156 18508 net.cpp:156] Memory required for data: 5772544
I0715 11:18:24.114162 18508 layer_factory.hpp:77] Creating layer conv1_p
I0715 11:18:24.114171 18508 net.cpp:91] Creating Layer conv1_p
I0715 11:18:24.114172 18508 net.cpp:425] conv1_p <- data_p
I0715 11:18:24.114177 18508 net.cpp:399] conv1_p -> conv1_p
I0715 11:18:24.114199 18508 net.cpp:141] Setting up conv1_p
I0715 11:18:24.114203 18508 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0715 11:18:24.114207 18508 net.cpp:156] Memory required for data: 8721664
I0715 11:18:24.114208 18508 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0715 11:18:24.114212 18508 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0715 11:18:24.114238 18508 layer_factory.hpp:77] Creating layer pool1_p
I0715 11:18:24.114244 18508 net.cpp:91] Creating Layer pool1_p
I0715 11:18:24.114246 18508 net.cpp:425] pool1_p <- conv1_p
I0715 11:18:24.114251 18508 net.cpp:399] pool1_p -> pool1_p
I0715 11:18:24.114259 18508 net.cpp:141] Setting up pool1_p
I0715 11:18:24.114264 18508 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0715 11:18:24.114265 18508 net.cpp:156] Memory required for data: 9458944
I0715 11:18:24.114267 18508 layer_factory.hpp:77] Creating layer conv2_p
I0715 11:18:24.114274 18508 net.cpp:91] Creating Layer conv2_p
I0715 11:18:24.114277 18508 net.cpp:425] conv2_p <- pool1_p
I0715 11:18:24.114282 18508 net.cpp:399] conv2_p -> conv2_p
I0715 11:18:24.114476 18508 net.cpp:141] Setting up conv2_p
I0715 11:18:24.114482 18508 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0715 11:18:24.114485 18508 net.cpp:156] Memory required for data: 10278144
I0715 11:18:24.114487 18508 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0715 11:18:24.114491 18508 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0715 11:18:24.114493 18508 layer_factory.hpp:77] Creating layer pool2_p
I0715 11:18:24.114497 18508 net.cpp:91] Creating Layer pool2_p
I0715 11:18:24.114500 18508 net.cpp:425] pool2_p <- conv2_p
I0715 11:18:24.114503 18508 net.cpp:399] pool2_p -> pool2_p
I0715 11:18:24.114509 18508 net.cpp:141] Setting up pool2_p
I0715 11:18:24.114512 18508 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0715 11:18:24.114514 18508 net.cpp:156] Memory required for data: 10482944
I0715 11:18:24.114516 18508 layer_factory.hpp:77] Creating layer ip1_p
I0715 11:18:24.114523 18508 net.cpp:91] Creating Layer ip1_p
I0715 11:18:24.114526 18508 net.cpp:425] ip1_p <- pool2_p
I0715 11:18:24.114531 18508 net.cpp:399] ip1_p -> ip1_p
I0715 11:18:24.120587 18508 net.cpp:141] Setting up ip1_p
I0715 11:18:24.120666 18508 net.cpp:148] Top shape: 64 500 (32000)
I0715 11:18:24.120671 18508 net.cpp:156] Memory required for data: 10610944
I0715 11:18:24.120677 18508 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0715 11:18:24.120682 18508 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0715 11:18:24.120685 18508 layer_factory.hpp:77] Creating layer relu1_p
I0715 11:18:24.120694 18508 net.cpp:91] Creating Layer relu1_p
I0715 11:18:24.120698 18508 net.cpp:425] relu1_p <- ip1_p
I0715 11:18:24.120707 18508 net.cpp:386] relu1_p -> ip1_p (in-place)
I0715 11:18:24.120715 18508 net.cpp:141] Setting up relu1_p
I0715 11:18:24.120719 18508 net.cpp:148] Top shape: 64 500 (32000)
I0715 11:18:24.120723 18508 net.cpp:156] Memory required for data: 10738944
I0715 11:18:24.120724 18508 layer_factory.hpp:77] Creating layer ip2_p
I0715 11:18:24.120733 18508 net.cpp:91] Creating Layer ip2_p
I0715 11:18:24.120736 18508 net.cpp:425] ip2_p <- ip1_p
I0715 11:18:24.120740 18508 net.cpp:399] ip2_p -> ip2_p
I0715 11:18:24.120791 18508 net.cpp:141] Setting up ip2_p
I0715 11:18:24.120796 18508 net.cpp:148] Top shape: 64 10 (640)
I0715 11:18:24.120798 18508 net.cpp:156] Memory required for data: 10741504
I0715 11:18:24.120803 18508 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0715 11:18:24.120807 18508 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0715 11:18:24.120810 18508 layer_factory.hpp:77] Creating layer feat_p
I0715 11:18:24.120815 18508 net.cpp:91] Creating Layer feat_p
I0715 11:18:24.120817 18508 net.cpp:425] feat_p <- ip2_p
I0715 11:18:24.120823 18508 net.cpp:399] feat_p -> feat_p
I0715 11:18:24.120831 18508 net.cpp:141] Setting up feat_p
I0715 11:18:24.120834 18508 net.cpp:148] Top shape: 64 2 (128)
I0715 11:18:24.120837 18508 net.cpp:156] Memory required for data: 10742016
I0715 11:18:24.120839 18508 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0715 11:18:24.120842 18508 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0715 11:18:24.120846 18508 layer_factory.hpp:77] Creating layer concat
I0715 11:18:24.120870 18508 net.cpp:91] Creating Layer concat
I0715 11:18:24.120873 18508 net.cpp:425] concat <- feat
I0715 11:18:24.120877 18508 net.cpp:425] concat <- feat_p
I0715 11:18:24.120880 18508 net.cpp:399] concat -> comb
I0715 11:18:24.120889 18508 net.cpp:141] Setting up concat
I0715 11:18:24.120893 18508 net.cpp:148] Top shape: 64 4 (256)
I0715 11:18:24.120895 18508 net.cpp:156] Memory required for data: 10743040
I0715 11:18:24.120898 18508 layer_factory.hpp:77] Creating layer fc1
I0715 11:18:24.120904 18508 net.cpp:91] Creating Layer fc1
I0715 11:18:24.120908 18508 net.cpp:425] fc1 <- comb
I0715 11:18:24.120913 18508 net.cpp:399] fc1 -> fc1
I0715 11:18:24.120926 18508 net.cpp:141] Setting up fc1
I0715 11:18:24.120930 18508 net.cpp:148] Top shape: 64 100 (6400)
I0715 11:18:24.120932 18508 net.cpp:156] Memory required for data: 10768640
I0715 11:18:24.120936 18508 layer_factory.hpp:77] Creating layer relu1_fc1
I0715 11:18:24.120941 18508 net.cpp:91] Creating Layer relu1_fc1
I0715 11:18:24.120944 18508 net.cpp:425] relu1_fc1 <- fc1
I0715 11:18:24.120947 18508 net.cpp:386] relu1_fc1 -> fc1 (in-place)
I0715 11:18:24.120951 18508 net.cpp:141] Setting up relu1_fc1
I0715 11:18:24.120954 18508 net.cpp:148] Top shape: 64 100 (6400)
I0715 11:18:24.120956 18508 net.cpp:156] Memory required for data: 10794240
I0715 11:18:24.120959 18508 layer_factory.hpp:77] Creating layer fc2
I0715 11:18:24.120965 18508 net.cpp:91] Creating Layer fc2
I0715 11:18:24.120966 18508 net.cpp:425] fc2 <- fc1
I0715 11:18:24.120971 18508 net.cpp:399] fc2 -> fc2
I0715 11:18:24.121013 18508 net.cpp:141] Setting up fc2
I0715 11:18:24.121016 18508 net.cpp:148] Top shape: 64 50 (3200)
I0715 11:18:24.121019 18508 net.cpp:156] Memory required for data: 10807040
I0715 11:18:24.121022 18508 layer_factory.hpp:77] Creating layer relu2_fc2
I0715 11:18:24.121026 18508 net.cpp:91] Creating Layer relu2_fc2
I0715 11:18:24.121029 18508 net.cpp:425] relu2_fc2 <- fc2
I0715 11:18:24.121032 18508 net.cpp:386] relu2_fc2 -> fc2 (in-place)
I0715 11:18:24.121037 18508 net.cpp:141] Setting up relu2_fc2
I0715 11:18:24.121039 18508 net.cpp:148] Top shape: 64 50 (3200)
I0715 11:18:24.121042 18508 net.cpp:156] Memory required for data: 10819840
I0715 11:18:24.121044 18508 layer_factory.hpp:77] Creating layer fc3
I0715 11:18:24.121049 18508 net.cpp:91] Creating Layer fc3
I0715 11:18:24.121052 18508 net.cpp:425] fc3 <- fc2
I0715 11:18:24.121057 18508 net.cpp:399] fc3 -> fc3
I0715 11:18:24.121063 18508 net.cpp:141] Setting up fc3
I0715 11:18:24.121067 18508 net.cpp:148] Top shape: 64 2 (128)
I0715 11:18:24.121069 18508 net.cpp:156] Memory required for data: 10820352
I0715 11:18:24.121073 18508 layer_factory.hpp:77] Creating layer loss
I0715 11:18:24.121078 18508 net.cpp:91] Creating Layer loss
I0715 11:18:24.121081 18508 net.cpp:425] loss <- fc3
I0715 11:18:24.121084 18508 net.cpp:425] loss <- label
I0715 11:18:24.121088 18508 net.cpp:399] loss -> loss
I0715 11:18:24.121099 18508 layer_factory.hpp:77] Creating layer loss
I0715 11:18:24.121119 18508 net.cpp:141] Setting up loss
I0715 11:18:24.121124 18508 net.cpp:148] Top shape: (1)
I0715 11:18:24.121125 18508 net.cpp:151]     with loss weight 1
I0715 11:18:24.121141 18508 net.cpp:156] Memory required for data: 10820356
I0715 11:18:24.121145 18508 net.cpp:217] loss needs backward computation.
I0715 11:18:24.121147 18508 net.cpp:217] fc3 needs backward computation.
I0715 11:18:24.121150 18508 net.cpp:217] relu2_fc2 needs backward computation.
I0715 11:18:24.121152 18508 net.cpp:217] fc2 needs backward computation.
I0715 11:18:24.121155 18508 net.cpp:217] relu1_fc1 needs backward computation.
I0715 11:18:24.121156 18508 net.cpp:217] fc1 needs backward computation.
I0715 11:18:24.121160 18508 net.cpp:217] concat needs backward computation.
I0715 11:18:24.121162 18508 net.cpp:217] feat_p needs backward computation.
I0715 11:18:24.121165 18508 net.cpp:217] ip2_p needs backward computation.
I0715 11:18:24.121167 18508 net.cpp:217] relu1_p needs backward computation.
I0715 11:18:24.121170 18508 net.cpp:217] ip1_p needs backward computation.
I0715 11:18:24.121186 18508 net.cpp:217] pool2_p needs backward computation.
I0715 11:18:24.121189 18508 net.cpp:217] conv2_p needs backward computation.
I0715 11:18:24.121192 18508 net.cpp:217] pool1_p needs backward computation.
I0715 11:18:24.121196 18508 net.cpp:217] conv1_p needs backward computation.
I0715 11:18:24.121198 18508 net.cpp:217] feat needs backward computation.
I0715 11:18:24.121201 18508 net.cpp:217] ip2 needs backward computation.
I0715 11:18:24.121203 18508 net.cpp:217] relu1 needs backward computation.
I0715 11:18:24.121206 18508 net.cpp:217] ip1 needs backward computation.
I0715 11:18:24.121208 18508 net.cpp:217] pool2 needs backward computation.
I0715 11:18:24.121212 18508 net.cpp:217] conv2 needs backward computation.
I0715 11:18:24.121214 18508 net.cpp:217] pool1 needs backward computation.
I0715 11:18:24.121217 18508 net.cpp:217] conv1 needs backward computation.
I0715 11:18:24.121220 18508 net.cpp:219] slice_pair does not need backward computation.
I0715 11:18:24.121223 18508 net.cpp:219] pair_data does not need backward computation.
I0715 11:18:24.121227 18508 net.cpp:261] This network produces output loss
I0715 11:18:24.121361 18508 net.cpp:274] Network initialization done.
I0715 11:18:24.125980 18508 solver.cpp:181] Creating test net (#0) specified by net file: examples/siamese/mnist_siamese_train_test_sim.prototxt
I0715 11:18:24.126107 18508 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0715 11:18:24.126425 18508 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test_sim"
state {
  phase: TEST
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/shaogang/caffe/examples/siamese/mnist_siamese_test_leveldb_789"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "concat"
  type: "Concat"
  bottom: "feat"
  bottom: "feat_p"
  top: "comb"
  concat_param {
    axis: 1
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "comb"
  top: "fc1"
  param {
    name: "fc1_w"
    lr_mult: 1
  }
  param {
    name: "fc1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_fc1"
  type: "ReLU"
  bottom: "fc1"
  top: "fc1"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "fc1"
  top: "fc2"
  param {
    name: "fc2_w"
    lr_mult: 1
  }
  param {
    name: "fc2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2_fc2"
  type: "ReLU"
  bottom: "fc2"
  top: "fc2"
}
layer {
  name: "fc3"
  type: "InnerProduct"
  bottom: "fc2"
  top: "fc3"
  param {
    name: "fc3_w"
    lr_mult: 1
  }
  param {
    name: "fc3_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc3"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
I0715 11:18:24.126680 18508 layer_factory.hpp:77] Creating layer pair_data
I0715 11:18:24.126871 18508 net.cpp:91] Creating Layer pair_data
I0715 11:18:24.126907 18508 net.cpp:399] pair_data -> pair_data
I0715 11:18:24.126955 18508 net.cpp:399] pair_data -> label
I0715 11:18:24.138478 18514 db_leveldb.cpp:18] Opened leveldb /home/shaogang/caffe/examples/siamese/mnist_siamese_test_leveldb_789
I0715 11:18:24.142622 18508 data_layer.cpp:41] output data size: 100,2,28,28
I0715 11:18:24.143393 18508 net.cpp:141] Setting up pair_data
I0715 11:18:24.143429 18508 net.cpp:148] Top shape: 100 2 28 28 (156800)
I0715 11:18:24.143458 18508 net.cpp:148] Top shape: 100 (100)
I0715 11:18:24.143463 18508 net.cpp:156] Memory required for data: 627600
I0715 11:18:24.143471 18508 layer_factory.hpp:77] Creating layer label_pair_data_1_split
I0715 11:18:24.143534 18508 net.cpp:91] Creating Layer label_pair_data_1_split
I0715 11:18:24.143553 18508 net.cpp:425] label_pair_data_1_split <- label
I0715 11:18:24.143560 18508 net.cpp:399] label_pair_data_1_split -> label_pair_data_1_split_0
I0715 11:18:24.143570 18508 net.cpp:399] label_pair_data_1_split -> label_pair_data_1_split_1
I0715 11:18:24.143543 18515 blocking_queue.cpp:50] Waiting for data
I0715 11:18:24.143579 18508 net.cpp:141] Setting up label_pair_data_1_split
I0715 11:18:24.143640 18508 net.cpp:148] Top shape: 100 (100)
I0715 11:18:24.143643 18508 net.cpp:148] Top shape: 100 (100)
I0715 11:18:24.143646 18508 net.cpp:156] Memory required for data: 628400
I0715 11:18:24.143649 18508 layer_factory.hpp:77] Creating layer slice_pair
I0715 11:18:24.143658 18508 net.cpp:91] Creating Layer slice_pair
I0715 11:18:24.143661 18508 net.cpp:425] slice_pair <- pair_data
I0715 11:18:24.143667 18508 net.cpp:399] slice_pair -> data
I0715 11:18:24.143674 18508 net.cpp:399] slice_pair -> data_p
I0715 11:18:24.143682 18508 net.cpp:141] Setting up slice_pair
I0715 11:18:24.143687 18508 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0715 11:18:24.143689 18508 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0715 11:18:24.143702 18508 net.cpp:156] Memory required for data: 1255600
I0715 11:18:24.143707 18508 layer_factory.hpp:77] Creating layer conv1
I0715 11:18:24.143717 18508 net.cpp:91] Creating Layer conv1
I0715 11:18:24.143719 18508 net.cpp:425] conv1 <- data
I0715 11:18:24.143724 18508 net.cpp:399] conv1 -> conv1
I0715 11:18:24.143751 18508 net.cpp:141] Setting up conv1
I0715 11:18:24.143767 18508 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0715 11:18:24.143770 18508 net.cpp:156] Memory required for data: 5863600
I0715 11:18:24.143777 18508 layer_factory.hpp:77] Creating layer pool1
I0715 11:18:24.143785 18508 net.cpp:91] Creating Layer pool1
I0715 11:18:24.143788 18508 net.cpp:425] pool1 <- conv1
I0715 11:18:24.143792 18508 net.cpp:399] pool1 -> pool1
I0715 11:18:24.143800 18508 net.cpp:141] Setting up pool1
I0715 11:18:24.143803 18508 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0715 11:18:24.143806 18508 net.cpp:156] Memory required for data: 7015600
I0715 11:18:24.143821 18508 layer_factory.hpp:77] Creating layer conv2
I0715 11:18:24.143828 18508 net.cpp:91] Creating Layer conv2
I0715 11:18:24.143831 18508 net.cpp:425] conv2 <- pool1
I0715 11:18:24.143836 18508 net.cpp:399] conv2 -> conv2
I0715 11:18:24.144049 18508 net.cpp:141] Setting up conv2
I0715 11:18:24.144067 18508 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0715 11:18:24.144069 18508 net.cpp:156] Memory required for data: 8295600
I0715 11:18:24.144075 18508 layer_factory.hpp:77] Creating layer pool2
I0715 11:18:24.144079 18508 net.cpp:91] Creating Layer pool2
I0715 11:18:24.144083 18508 net.cpp:425] pool2 <- conv2
I0715 11:18:24.144086 18508 net.cpp:399] pool2 -> pool2
I0715 11:18:24.144093 18508 net.cpp:141] Setting up pool2
I0715 11:18:24.144096 18508 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0715 11:18:24.144099 18508 net.cpp:156] Memory required for data: 8615600
I0715 11:18:24.144101 18508 layer_factory.hpp:77] Creating layer ip1
I0715 11:18:24.144109 18508 net.cpp:91] Creating Layer ip1
I0715 11:18:24.144111 18508 net.cpp:425] ip1 <- pool2
I0715 11:18:24.144115 18508 net.cpp:399] ip1 -> ip1
I0715 11:18:24.147056 18508 net.cpp:141] Setting up ip1
I0715 11:18:24.147107 18508 net.cpp:148] Top shape: 100 500 (50000)
I0715 11:18:24.147112 18508 net.cpp:156] Memory required for data: 8815600
I0715 11:18:24.147197 18508 layer_factory.hpp:77] Creating layer relu1
I0715 11:18:24.147213 18508 net.cpp:91] Creating Layer relu1
I0715 11:18:24.147218 18508 net.cpp:425] relu1 <- ip1
I0715 11:18:24.147224 18508 net.cpp:386] relu1 -> ip1 (in-place)
I0715 11:18:24.147233 18508 net.cpp:141] Setting up relu1
I0715 11:18:24.147238 18508 net.cpp:148] Top shape: 100 500 (50000)
I0715 11:18:24.147306 18508 net.cpp:156] Memory required for data: 9015600
I0715 11:18:24.147325 18508 layer_factory.hpp:77] Creating layer ip2
I0715 11:18:24.147333 18508 net.cpp:91] Creating Layer ip2
I0715 11:18:24.147337 18508 net.cpp:425] ip2 <- ip1
I0715 11:18:24.147342 18508 net.cpp:399] ip2 -> ip2
I0715 11:18:24.147397 18508 net.cpp:141] Setting up ip2
I0715 11:18:24.147403 18508 net.cpp:148] Top shape: 100 10 (1000)
I0715 11:18:24.147405 18508 net.cpp:156] Memory required for data: 9019600
I0715 11:18:24.147410 18508 layer_factory.hpp:77] Creating layer feat
I0715 11:18:24.147416 18508 net.cpp:91] Creating Layer feat
I0715 11:18:24.147419 18508 net.cpp:425] feat <- ip2
I0715 11:18:24.147424 18508 net.cpp:399] feat -> feat
I0715 11:18:24.147433 18508 net.cpp:141] Setting up feat
I0715 11:18:24.147438 18508 net.cpp:148] Top shape: 100 2 (200)
I0715 11:18:24.147439 18508 net.cpp:156] Memory required for data: 9020400
I0715 11:18:24.147445 18508 layer_factory.hpp:77] Creating layer conv1_p
I0715 11:18:24.147454 18508 net.cpp:91] Creating Layer conv1_p
I0715 11:18:24.147457 18508 net.cpp:425] conv1_p <- data_p
I0715 11:18:24.147462 18508 net.cpp:399] conv1_p -> conv1_p
I0715 11:18:24.147486 18508 net.cpp:141] Setting up conv1_p
I0715 11:18:24.147491 18508 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0715 11:18:24.147495 18508 net.cpp:156] Memory required for data: 13628400
I0715 11:18:24.147497 18508 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0715 11:18:24.147500 18508 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0715 11:18:24.147503 18508 layer_factory.hpp:77] Creating layer pool1_p
I0715 11:18:24.147510 18508 net.cpp:91] Creating Layer pool1_p
I0715 11:18:24.147511 18508 net.cpp:425] pool1_p <- conv1_p
I0715 11:18:24.147517 18508 net.cpp:399] pool1_p -> pool1_p
I0715 11:18:24.147537 18508 net.cpp:141] Setting up pool1_p
I0715 11:18:24.147541 18508 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0715 11:18:24.147543 18508 net.cpp:156] Memory required for data: 14780400
I0715 11:18:24.147547 18508 layer_factory.hpp:77] Creating layer conv2_p
I0715 11:18:24.147554 18508 net.cpp:91] Creating Layer conv2_p
I0715 11:18:24.147557 18508 net.cpp:425] conv2_p <- pool1_p
I0715 11:18:24.147562 18508 net.cpp:399] conv2_p -> conv2_p
I0715 11:18:24.147766 18508 net.cpp:141] Setting up conv2_p
I0715 11:18:24.147771 18508 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0715 11:18:24.147773 18508 net.cpp:156] Memory required for data: 16060400
I0715 11:18:24.147778 18508 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0715 11:18:24.147781 18508 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0715 11:18:24.147784 18508 layer_factory.hpp:77] Creating layer pool2_p
I0715 11:18:24.147790 18508 net.cpp:91] Creating Layer pool2_p
I0715 11:18:24.147794 18508 net.cpp:425] pool2_p <- conv2_p
I0715 11:18:24.147797 18508 net.cpp:399] pool2_p -> pool2_p
I0715 11:18:24.147806 18508 net.cpp:141] Setting up pool2_p
I0715 11:18:24.147810 18508 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0715 11:18:24.147812 18508 net.cpp:156] Memory required for data: 16380400
I0715 11:18:24.147815 18508 layer_factory.hpp:77] Creating layer ip1_p
I0715 11:18:24.147821 18508 net.cpp:91] Creating Layer ip1_p
I0715 11:18:24.147824 18508 net.cpp:425] ip1_p <- pool2_p
I0715 11:18:24.147830 18508 net.cpp:399] ip1_p -> ip1_p
I0715 11:18:24.155381 18508 net.cpp:141] Setting up ip1_p
I0715 11:18:24.155450 18508 net.cpp:148] Top shape: 100 500 (50000)
I0715 11:18:24.155462 18508 net.cpp:156] Memory required for data: 16580400
I0715 11:18:24.155474 18508 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0715 11:18:24.155483 18508 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0715 11:18:24.155490 18508 layer_factory.hpp:77] Creating layer relu1_p
I0715 11:18:24.155509 18508 net.cpp:91] Creating Layer relu1_p
I0715 11:18:24.155540 18508 net.cpp:425] relu1_p <- ip1_p
I0715 11:18:24.155589 18508 net.cpp:386] relu1_p -> ip1_p (in-place)
I0715 11:18:24.155627 18508 net.cpp:141] Setting up relu1_p
I0715 11:18:24.155634 18508 net.cpp:148] Top shape: 100 500 (50000)
I0715 11:18:24.155638 18508 net.cpp:156] Memory required for data: 16780400
I0715 11:18:24.155643 18508 layer_factory.hpp:77] Creating layer ip2_p
I0715 11:18:24.155655 18508 net.cpp:91] Creating Layer ip2_p
I0715 11:18:24.155676 18508 net.cpp:425] ip2_p <- ip1_p
I0715 11:18:24.157517 18508 net.cpp:399] ip2_p -> ip2_p
I0715 11:18:24.157608 18508 net.cpp:141] Setting up ip2_p
I0715 11:18:24.157636 18508 net.cpp:148] Top shape: 100 10 (1000)
I0715 11:18:24.157641 18508 net.cpp:156] Memory required for data: 16784400
I0715 11:18:24.157652 18508 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0715 11:18:24.157658 18508 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0715 11:18:24.157662 18508 layer_factory.hpp:77] Creating layer feat_p
I0715 11:18:24.157673 18508 net.cpp:91] Creating Layer feat_p
I0715 11:18:24.157678 18508 net.cpp:425] feat_p <- ip2_p
I0715 11:18:24.157687 18508 net.cpp:399] feat_p -> feat_p
I0715 11:18:24.157709 18508 net.cpp:141] Setting up feat_p
I0715 11:18:24.157717 18508 net.cpp:148] Top shape: 100 2 (200)
I0715 11:18:24.158226 18508 net.cpp:156] Memory required for data: 16785200
I0715 11:18:24.158257 18508 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0715 11:18:24.158264 18508 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0715 11:18:24.158267 18508 layer_factory.hpp:77] Creating layer concat
I0715 11:18:24.158290 18508 net.cpp:91] Creating Layer concat
I0715 11:18:24.158296 18508 net.cpp:425] concat <- feat
I0715 11:18:24.158303 18508 net.cpp:425] concat <- feat_p
I0715 11:18:24.158308 18508 net.cpp:399] concat -> comb
I0715 11:18:24.158323 18508 net.cpp:141] Setting up concat
I0715 11:18:24.158329 18508 net.cpp:148] Top shape: 100 4 (400)
I0715 11:18:24.158332 18508 net.cpp:156] Memory required for data: 16786800
I0715 11:18:24.158335 18508 layer_factory.hpp:77] Creating layer fc1
I0715 11:18:24.158354 18508 net.cpp:91] Creating Layer fc1
I0715 11:18:24.158356 18508 net.cpp:425] fc1 <- comb
I0715 11:18:24.158362 18508 net.cpp:399] fc1 -> fc1
I0715 11:18:24.158385 18508 net.cpp:141] Setting up fc1
I0715 11:18:24.158390 18508 net.cpp:148] Top shape: 100 100 (10000)
I0715 11:18:24.158391 18508 net.cpp:156] Memory required for data: 16826800
I0715 11:18:24.158396 18508 layer_factory.hpp:77] Creating layer relu1_fc1
I0715 11:18:24.158402 18508 net.cpp:91] Creating Layer relu1_fc1
I0715 11:18:24.158406 18508 net.cpp:425] relu1_fc1 <- fc1
I0715 11:18:24.158409 18508 net.cpp:386] relu1_fc1 -> fc1 (in-place)
I0715 11:18:24.158414 18508 net.cpp:141] Setting up relu1_fc1
I0715 11:18:24.158418 18508 net.cpp:148] Top shape: 100 100 (10000)
I0715 11:18:24.158421 18508 net.cpp:156] Memory required for data: 16866800
I0715 11:18:24.158423 18508 layer_factory.hpp:77] Creating layer fc2
I0715 11:18:24.158429 18508 net.cpp:91] Creating Layer fc2
I0715 11:18:24.158432 18508 net.cpp:425] fc2 <- fc1
I0715 11:18:24.158437 18508 net.cpp:399] fc2 -> fc2
I0715 11:18:24.158480 18508 net.cpp:141] Setting up fc2
I0715 11:18:24.158484 18508 net.cpp:148] Top shape: 100 50 (5000)
I0715 11:18:24.158486 18508 net.cpp:156] Memory required for data: 16886800
I0715 11:18:24.158490 18508 layer_factory.hpp:77] Creating layer relu2_fc2
I0715 11:18:24.158494 18508 net.cpp:91] Creating Layer relu2_fc2
I0715 11:18:24.158498 18508 net.cpp:425] relu2_fc2 <- fc2
I0715 11:18:24.158500 18508 net.cpp:386] relu2_fc2 -> fc2 (in-place)
I0715 11:18:24.158504 18508 net.cpp:141] Setting up relu2_fc2
I0715 11:18:24.158507 18508 net.cpp:148] Top shape: 100 50 (5000)
I0715 11:18:24.158510 18508 net.cpp:156] Memory required for data: 16906800
I0715 11:18:24.158512 18508 layer_factory.hpp:77] Creating layer fc3
I0715 11:18:24.158519 18508 net.cpp:91] Creating Layer fc3
I0715 11:18:24.158520 18508 net.cpp:425] fc3 <- fc2
I0715 11:18:24.158550 18508 net.cpp:399] fc3 -> fc3
I0715 11:18:24.158560 18508 net.cpp:141] Setting up fc3
I0715 11:18:24.158563 18508 net.cpp:148] Top shape: 100 2 (200)
I0715 11:18:24.158566 18508 net.cpp:156] Memory required for data: 16907600
I0715 11:18:24.158571 18508 layer_factory.hpp:77] Creating layer fc3_fc3_0_split
I0715 11:18:24.158576 18508 net.cpp:91] Creating Layer fc3_fc3_0_split
I0715 11:18:24.158577 18508 net.cpp:425] fc3_fc3_0_split <- fc3
I0715 11:18:24.158581 18508 net.cpp:399] fc3_fc3_0_split -> fc3_fc3_0_split_0
I0715 11:18:24.158593 18508 net.cpp:399] fc3_fc3_0_split -> fc3_fc3_0_split_1
I0715 11:18:24.158601 18508 net.cpp:141] Setting up fc3_fc3_0_split
I0715 11:18:24.158603 18508 net.cpp:148] Top shape: 100 2 (200)
I0715 11:18:24.158607 18508 net.cpp:148] Top shape: 100 2 (200)
I0715 11:18:24.158608 18508 net.cpp:156] Memory required for data: 16909200
I0715 11:18:24.158612 18508 layer_factory.hpp:77] Creating layer loss
I0715 11:18:24.158617 18508 net.cpp:91] Creating Layer loss
I0715 11:18:24.158619 18508 net.cpp:425] loss <- fc3_fc3_0_split_0
I0715 11:18:24.158622 18508 net.cpp:425] loss <- label_pair_data_1_split_0
I0715 11:18:24.158628 18508 net.cpp:399] loss -> loss
I0715 11:18:24.158635 18508 layer_factory.hpp:77] Creating layer loss
I0715 11:18:24.158649 18508 net.cpp:141] Setting up loss
I0715 11:18:24.158654 18508 net.cpp:148] Top shape: (1)
I0715 11:18:24.158658 18508 net.cpp:151]     with loss weight 1
I0715 11:18:24.158673 18508 net.cpp:156] Memory required for data: 16909204
I0715 11:18:24.158675 18508 layer_factory.hpp:77] Creating layer accuracy
I0715 11:18:24.158682 18508 net.cpp:91] Creating Layer accuracy
I0715 11:18:24.158685 18508 net.cpp:425] accuracy <- fc3_fc3_0_split_1
I0715 11:18:24.158689 18508 net.cpp:425] accuracy <- label_pair_data_1_split_1
I0715 11:18:24.158691 18508 net.cpp:399] accuracy -> accuracy
I0715 11:18:24.158699 18508 net.cpp:141] Setting up accuracy
I0715 11:18:24.158702 18508 net.cpp:148] Top shape: (1)
I0715 11:18:24.158704 18508 net.cpp:156] Memory required for data: 16909208
I0715 11:18:24.158707 18508 net.cpp:219] accuracy does not need backward computation.
I0715 11:18:24.158710 18508 net.cpp:217] loss needs backward computation.
I0715 11:18:24.158713 18508 net.cpp:217] fc3_fc3_0_split needs backward computation.
I0715 11:18:24.158715 18508 net.cpp:217] fc3 needs backward computation.
I0715 11:18:24.158718 18508 net.cpp:217] relu2_fc2 needs backward computation.
I0715 11:18:24.158720 18508 net.cpp:217] fc2 needs backward computation.
I0715 11:18:24.158725 18508 net.cpp:217] relu1_fc1 needs backward computation.
I0715 11:18:24.158727 18508 net.cpp:217] fc1 needs backward computation.
I0715 11:18:24.158730 18508 net.cpp:217] concat needs backward computation.
I0715 11:18:24.158733 18508 net.cpp:217] feat_p needs backward computation.
I0715 11:18:24.158736 18508 net.cpp:217] ip2_p needs backward computation.
I0715 11:18:24.158740 18508 net.cpp:217] relu1_p needs backward computation.
I0715 11:18:24.158742 18508 net.cpp:217] ip1_p needs backward computation.
I0715 11:18:24.158746 18508 net.cpp:217] pool2_p needs backward computation.
I0715 11:18:24.158749 18508 net.cpp:217] conv2_p needs backward computation.
I0715 11:18:24.158753 18508 net.cpp:217] pool1_p needs backward computation.
I0715 11:18:24.158759 18508 net.cpp:217] conv1_p needs backward computation.
I0715 11:18:24.158763 18508 net.cpp:217] feat needs backward computation.
I0715 11:18:24.158766 18508 net.cpp:217] ip2 needs backward computation.
I0715 11:18:24.158769 18508 net.cpp:217] relu1 needs backward computation.
I0715 11:18:24.158772 18508 net.cpp:217] ip1 needs backward computation.
I0715 11:18:24.158776 18508 net.cpp:217] pool2 needs backward computation.
I0715 11:18:24.158778 18508 net.cpp:217] conv2 needs backward computation.
I0715 11:18:24.158782 18508 net.cpp:217] pool1 needs backward computation.
I0715 11:18:24.158784 18508 net.cpp:217] conv1 needs backward computation.
I0715 11:18:24.158788 18508 net.cpp:219] slice_pair does not need backward computation.
I0715 11:18:24.158792 18508 net.cpp:219] label_pair_data_1_split does not need backward computation.
I0715 11:18:24.158804 18508 net.cpp:219] pair_data does not need backward computation.
I0715 11:18:24.158807 18508 net.cpp:261] This network produces output accuracy
I0715 11:18:24.158809 18508 net.cpp:261] This network produces output loss
I0715 11:18:24.158845 18508 net.cpp:274] Network initialization done.
I0715 11:18:24.159049 18508 solver.cpp:60] Solver scaffolding done.
I0715 11:18:24.159152 18508 caffe.cpp:219] Starting Optimization
I0715 11:18:24.159159 18508 solver.cpp:279] Solving mnist_siamese_train_test_sim
I0715 11:18:24.159162 18508 solver.cpp:280] Learning Rate Policy: inv
I0715 11:18:24.159848 18508 solver.cpp:337] Iteration 0, Testing net (#0)
I0715 11:18:35.063240 18508 solver.cpp:404]     Test net output #0: accuracy = 0.6658
I0715 11:18:35.063302 18508 solver.cpp:404]     Test net output #1: loss = 0.659828 (* 1 = 0.659828 loss)
I0715 11:18:35.270525 18508 solver.cpp:228] Iteration 0, loss = 0.692311
I0715 11:18:35.270584 18508 solver.cpp:244]     Train net output #0: loss = 0.692311 (* 1 = 0.692311 loss)
I0715 11:18:35.270704 18508 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0715 11:18:43.376323 18508 solver.cpp:337] Iteration 50, Testing net (#0)
I0715 11:18:53.390940 18508 solver.cpp:404]     Test net output #0: accuracy = 0.4849
I0715 11:18:53.391001 18508 solver.cpp:404]     Test net output #1: loss = 1.95786 (* 1 = 1.95786 loss)
I0715 11:19:02.249737 18508 solver.cpp:337] Iteration 100, Testing net (#0)
I0715 11:19:12.902768 18508 solver.cpp:404]     Test net output #0: accuracy = 0.491
I0715 11:19:12.902925 18508 solver.cpp:404]     Test net output #1: loss = 1.605 (* 1 = 1.605 loss)
I0715 11:19:13.098611 18508 solver.cpp:228] Iteration 100, loss = 0.000602001
I0715 11:19:13.098824 18508 solver.cpp:244]     Train net output #0: loss = 0.000602002 (* 1 = 0.000602002 loss)
I0715 11:19:13.098880 18508 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0715 11:19:21.170476 18508 solver.cpp:337] Iteration 150, Testing net (#0)
I0715 11:19:31.270764 18508 solver.cpp:404]     Test net output #0: accuracy = 0.4991
I0715 11:19:31.270820 18508 solver.cpp:404]     Test net output #1: loss = 1.58114 (* 1 = 1.58114 loss)
I0715 11:19:39.682679 18508 solver.cpp:337] Iteration 200, Testing net (#0)
I0715 11:19:50.141137 18508 solver.cpp:404]     Test net output #0: accuracy = 0.4962
I0715 11:19:50.141197 18508 solver.cpp:404]     Test net output #1: loss = 1.67887 (* 1 = 1.67887 loss)
I0715 11:19:50.343219 18508 solver.cpp:228] Iteration 200, loss = 0.00077489
I0715 11:19:50.343274 18508 solver.cpp:244]     Train net output #0: loss = 0.000774885 (* 1 = 0.000774885 loss)
I0715 11:19:50.343284 18508 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0715 11:19:58.581311 18508 solver.cpp:337] Iteration 250, Testing net (#0)
I0715 11:20:09.174715 18508 solver.cpp:404]     Test net output #0: accuracy = 0.4655
I0715 11:20:09.174800 18508 solver.cpp:404]     Test net output #1: loss = 2.20206 (* 1 = 2.20206 loss)
I0715 11:20:18.089615 18508 solver.cpp:337] Iteration 300, Testing net (#0)
I0715 11:20:29.005846 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5049
I0715 11:20:29.006057 18508 solver.cpp:404]     Test net output #1: loss = 1.71751 (* 1 = 1.71751 loss)
I0715 11:20:29.225508 18508 solver.cpp:228] Iteration 300, loss = 3.11391e-05
I0715 11:20:29.225709 18508 solver.cpp:244]     Train net output #0: loss = 3.11268e-05 (* 1 = 3.11268e-05 loss)
I0715 11:20:29.225776 18508 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0715 11:20:38.143731 18508 solver.cpp:337] Iteration 350, Testing net (#0)
I0715 11:20:49.830734 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5174
I0715 11:20:49.830842 18508 solver.cpp:404]     Test net output #1: loss = 1.46238 (* 1 = 1.46238 loss)
I0715 11:20:58.792176 18508 solver.cpp:337] Iteration 400, Testing net (#0)
I0715 11:21:09.591943 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5081
I0715 11:21:09.592000 18508 solver.cpp:404]     Test net output #1: loss = 1.65458 (* 1 = 1.65458 loss)
I0715 11:21:09.783308 18508 solver.cpp:228] Iteration 400, loss = 6.1342e-05
I0715 11:21:09.783371 18508 solver.cpp:244]     Train net output #0: loss = 6.132e-05 (* 1 = 6.132e-05 loss)
I0715 11:21:09.783383 18508 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0715 11:21:17.584265 18508 solver.cpp:337] Iteration 450, Testing net (#0)
I0715 11:21:27.553903 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5074
I0715 11:21:27.554052 18508 solver.cpp:404]     Test net output #1: loss = 1.60176 (* 1 = 1.60176 loss)
I0715 11:21:35.501754 18508 solver.cpp:337] Iteration 500, Testing net (#0)
I0715 11:21:45.432111 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5116
I0715 11:21:45.432170 18508 solver.cpp:404]     Test net output #1: loss = 1.42108 (* 1 = 1.42108 loss)
I0715 11:21:45.625054 18508 solver.cpp:228] Iteration 500, loss = 0.000601475
I0715 11:21:45.625113 18508 solver.cpp:244]     Train net output #0: loss = 0.000601451 (* 1 = 0.000601451 loss)
I0715 11:21:45.625125 18508 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0715 11:21:55.035064 18508 solver.cpp:337] Iteration 550, Testing net (#0)
I0715 11:22:06.241776 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5125
I0715 11:22:06.242004 18508 solver.cpp:404]     Test net output #1: loss = 1.55048 (* 1 = 1.55048 loss)
I0715 11:22:14.219034 18508 solver.cpp:337] Iteration 600, Testing net (#0)
I0715 11:22:24.116550 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5135
I0715 11:22:24.116611 18508 solver.cpp:404]     Test net output #1: loss = 1.62366 (* 1 = 1.62366 loss)
I0715 11:22:24.313369 18508 solver.cpp:228] Iteration 600, loss = 0.00130684
I0715 11:22:24.313580 18508 solver.cpp:244]     Train net output #0: loss = 0.00130681 (* 1 = 0.00130681 loss)
I0715 11:22:24.313623 18508 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0715 11:22:32.747982 18508 solver.cpp:337] Iteration 650, Testing net (#0)
I0715 11:22:43.902281 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5172
I0715 11:22:43.902626 18508 solver.cpp:404]     Test net output #1: loss = 1.65114 (* 1 = 1.65114 loss)
I0715 11:22:51.923359 18508 solver.cpp:337] Iteration 700, Testing net (#0)
I0715 11:23:01.913506 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5145
I0715 11:23:01.913568 18508 solver.cpp:404]     Test net output #1: loss = 1.71495 (* 1 = 1.71495 loss)
I0715 11:23:02.115283 18508 solver.cpp:228] Iteration 700, loss = 9.84236e-05
I0715 11:23:02.115344 18508 solver.cpp:244]     Train net output #0: loss = 9.83999e-05 (* 1 = 9.83999e-05 loss)
I0715 11:23:02.115355 18508 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0715 11:23:09.917227 18508 solver.cpp:337] Iteration 750, Testing net (#0)
I0715 11:23:19.859359 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5045
I0715 11:23:19.859665 18508 solver.cpp:404]     Test net output #1: loss = 1.80703 (* 1 = 1.80703 loss)
I0715 11:23:27.824426 18508 solver.cpp:337] Iteration 800, Testing net (#0)
I0715 11:23:37.773665 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5254
I0715 11:23:37.773725 18508 solver.cpp:404]     Test net output #1: loss = 1.67406 (* 1 = 1.67406 loss)
I0715 11:23:37.966725 18508 solver.cpp:228] Iteration 800, loss = 9.95336e-05
I0715 11:23:37.967046 18508 solver.cpp:244]     Train net output #0: loss = 9.95094e-05 (* 1 = 9.95094e-05 loss)
I0715 11:23:37.967133 18508 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0715 11:23:45.750552 18508 solver.cpp:337] Iteration 850, Testing net (#0)
I0715 11:23:55.672843 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5157
I0715 11:23:55.673142 18508 solver.cpp:404]     Test net output #1: loss = 1.67193 (* 1 = 1.67193 loss)
I0715 11:24:03.662715 18508 solver.cpp:337] Iteration 900, Testing net (#0)
I0715 11:24:15.079386 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5039
I0715 11:24:15.079445 18508 solver.cpp:404]     Test net output #1: loss = 1.81056 (* 1 = 1.81056 loss)
I0715 11:24:15.282850 18508 solver.cpp:228] Iteration 900, loss = 4.48332e-05
I0715 11:24:15.282909 18508 solver.cpp:244]     Train net output #0: loss = 4.48067e-05 (* 1 = 4.48067e-05 loss)
I0715 11:24:15.282920 18508 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0715 11:24:24.091287 18508 solver.cpp:337] Iteration 950, Testing net (#0)
I0715 11:24:34.132694 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5205
I0715 11:24:34.132881 18508 solver.cpp:404]     Test net output #1: loss = 1.76587 (* 1 = 1.76587 loss)
I0715 11:24:42.077899 18508 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to1_sim_iter_1000.caffemodel
I0715 11:24:42.091544 18508 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to1_sim_iter_1000.solverstate
I0715 11:24:42.101490 18508 solver.cpp:337] Iteration 1000, Testing net (#0)
I0715 11:24:52.077491 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5258
I0715 11:24:52.077728 18508 solver.cpp:404]     Test net output #1: loss = 1.75813 (* 1 = 1.75813 loss)
I0715 11:24:52.287807 18508 solver.cpp:228] Iteration 1000, loss = 0.000294929
I0715 11:24:52.288000 18508 solver.cpp:244]     Train net output #0: loss = 0.000294902 (* 1 = 0.000294902 loss)
I0715 11:24:52.288046 18508 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0715 11:25:00.095089 18508 solver.cpp:337] Iteration 1050, Testing net (#0)
I0715 11:25:10.109690 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5232
I0715 11:25:10.110038 18508 solver.cpp:404]     Test net output #1: loss = 1.80397 (* 1 = 1.80397 loss)
I0715 11:25:18.024003 18508 solver.cpp:337] Iteration 1100, Testing net (#0)
I0715 11:25:29.000495 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5243
I0715 11:25:29.000725 18508 solver.cpp:404]     Test net output #1: loss = 1.81267 (* 1 = 1.81267 loss)
I0715 11:25:29.282073 18508 solver.cpp:228] Iteration 1100, loss = 6.71095e-06
I0715 11:25:29.282192 18508 solver.cpp:244]     Train net output #0: loss = 6.68516e-06 (* 1 = 6.68516e-06 loss)
I0715 11:25:29.282218 18508 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0715 11:25:39.360975 18508 solver.cpp:337] Iteration 1150, Testing net (#0)
I0715 11:25:50.612198 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5259
I0715 11:25:50.612293 18508 solver.cpp:404]     Test net output #1: loss = 1.83588 (* 1 = 1.83588 loss)
I0715 11:25:58.636214 18508 solver.cpp:337] Iteration 1200, Testing net (#0)
I0715 11:26:08.633955 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5236
I0715 11:26:08.634014 18508 solver.cpp:404]     Test net output #1: loss = 1.86887 (* 1 = 1.86887 loss)
I0715 11:26:08.820907 18508 solver.cpp:228] Iteration 1200, loss = 7.09583e-05
I0715 11:26:08.820966 18508 solver.cpp:244]     Train net output #0: loss = 7.09326e-05 (* 1 = 7.09326e-05 loss)
I0715 11:26:08.820976 18508 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0715 11:26:16.646754 18508 solver.cpp:337] Iteration 1250, Testing net (#0)
I0715 11:26:26.584326 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5249
I0715 11:26:26.584550 18508 solver.cpp:404]     Test net output #1: loss = 1.88791 (* 1 = 1.88791 loss)
I0715 11:26:34.588088 18508 solver.cpp:337] Iteration 1300, Testing net (#0)
I0715 11:26:44.942167 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5256
I0715 11:26:44.942241 18508 solver.cpp:404]     Test net output #1: loss = 1.90864 (* 1 = 1.90864 loss)
I0715 11:26:45.135555 18508 solver.cpp:228] Iteration 1300, loss = 3.1103e-06
I0715 11:26:45.135614 18508 solver.cpp:244]     Train net output #0: loss = 3.08457e-06 (* 1 = 3.08457e-06 loss)
I0715 11:26:45.135627 18508 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0715 11:26:54.392413 18508 solver.cpp:337] Iteration 1350, Testing net (#0)
I0715 11:27:06.519493 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5173
I0715 11:27:06.519809 18508 solver.cpp:404]     Test net output #1: loss = 1.9417 (* 1 = 1.9417 loss)
I0715 11:27:14.834170 18508 solver.cpp:337] Iteration 1400, Testing net (#0)
I0715 11:27:24.897593 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5284
I0715 11:27:24.897658 18508 solver.cpp:404]     Test net output #1: loss = 1.84958 (* 1 = 1.84958 loss)
I0715 11:27:25.098192 18508 solver.cpp:228] Iteration 1400, loss = 1.49488e-05
I0715 11:27:25.098276 18508 solver.cpp:244]     Train net output #0: loss = 1.49222e-05 (* 1 = 1.49222e-05 loss)
I0715 11:27:25.098294 18508 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0715 11:27:32.922559 18508 solver.cpp:337] Iteration 1450, Testing net (#0)
I0715 11:27:42.859541 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5277
I0715 11:27:42.859737 18508 solver.cpp:404]     Test net output #1: loss = 1.88026 (* 1 = 1.88026 loss)
I0715 11:27:50.815798 18508 solver.cpp:337] Iteration 1500, Testing net (#0)
I0715 11:28:00.829984 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5262
I0715 11:28:00.830066 18508 solver.cpp:404]     Test net output #1: loss = 1.90415 (* 1 = 1.90415 loss)
I0715 11:28:01.033936 18508 solver.cpp:228] Iteration 1500, loss = 1.13514e-05
I0715 11:28:01.033999 18508 solver.cpp:244]     Train net output #0: loss = 1.13251e-05 (* 1 = 1.13251e-05 loss)
I0715 11:28:01.034013 18508 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0715 11:28:08.865649 18508 solver.cpp:337] Iteration 1550, Testing net (#0)
I0715 11:28:18.824350 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5263
I0715 11:28:18.824688 18508 solver.cpp:404]     Test net output #1: loss = 1.92491 (* 1 = 1.92491 loss)
I0715 11:28:26.818282 18508 solver.cpp:337] Iteration 1600, Testing net (#0)
I0715 11:28:36.792328 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5256
I0715 11:28:36.792392 18508 solver.cpp:404]     Test net output #1: loss = 1.94343 (* 1 = 1.94343 loss)
I0715 11:28:36.986538 18508 solver.cpp:228] Iteration 1600, loss = 0.000262588
I0715 11:28:36.986604 18508 solver.cpp:244]     Train net output #0: loss = 0.000262562 (* 1 = 0.000262562 loss)
I0715 11:28:36.986642 18508 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0715 11:28:44.811144 18508 solver.cpp:337] Iteration 1650, Testing net (#0)
I0715 11:28:54.784847 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5256
I0715 11:28:54.785199 18508 solver.cpp:404]     Test net output #1: loss = 1.97431 (* 1 = 1.97431 loss)
I0715 11:29:02.757284 18508 solver.cpp:337] Iteration 1700, Testing net (#0)
I0715 11:29:12.710018 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5253
I0715 11:29:12.710079 18508 solver.cpp:404]     Test net output #1: loss = 1.99846 (* 1 = 1.99846 loss)
I0715 11:29:12.904132 18508 solver.cpp:228] Iteration 1700, loss = 5.74057e-06
I0715 11:29:12.904191 18508 solver.cpp:244]     Train net output #0: loss = 5.71417e-06 (* 1 = 5.71417e-06 loss)
I0715 11:29:12.904202 18508 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0715 11:29:20.712558 18508 solver.cpp:337] Iteration 1750, Testing net (#0)
I0715 11:29:30.643128 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5253
I0715 11:29:30.643402 18508 solver.cpp:404]     Test net output #1: loss = 2.01076 (* 1 = 2.01076 loss)
I0715 11:29:38.668767 18508 solver.cpp:337] Iteration 1800, Testing net (#0)
I0715 11:29:48.601572 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5251
I0715 11:29:48.601639 18508 solver.cpp:404]     Test net output #1: loss = 2.02443 (* 1 = 2.02443 loss)
I0715 11:29:48.792707 18508 solver.cpp:228] Iteration 1800, loss = 4.44629e-05
I0715 11:29:48.792768 18508 solver.cpp:244]     Train net output #0: loss = 4.44365e-05 (* 1 = 4.44365e-05 loss)
I0715 11:29:48.792779 18508 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0715 11:29:56.691416 18508 solver.cpp:337] Iteration 1850, Testing net (#0)
I0715 11:30:06.682842 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5255
I0715 11:30:06.683045 18508 solver.cpp:404]     Test net output #1: loss = 2.0381 (* 1 = 2.0381 loss)
I0715 11:30:14.664521 18508 solver.cpp:337] Iteration 1900, Testing net (#0)
I0715 11:30:24.599810 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5255
I0715 11:30:24.599874 18508 solver.cpp:404]     Test net output #1: loss = 2.04995 (* 1 = 2.04995 loss)
I0715 11:30:24.791195 18508 solver.cpp:228] Iteration 1900, loss = 3.40185e-05
I0715 11:30:24.791260 18508 solver.cpp:244]     Train net output #0: loss = 3.3992e-05 (* 1 = 3.3992e-05 loss)
I0715 11:30:24.791321 18508 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0715 11:30:32.641991 18508 solver.cpp:337] Iteration 1950, Testing net (#0)
I0715 11:30:42.583667 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5257
I0715 11:30:42.584048 18508 solver.cpp:404]     Test net output #1: loss = 2.04958 (* 1 = 2.04958 loss)
I0715 11:30:50.567864 18508 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to1_sim_iter_2000.caffemodel
I0715 11:30:50.581418 18508 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to1_sim_iter_2000.solverstate
I0715 11:30:50.588713 18508 solver.cpp:337] Iteration 2000, Testing net (#0)
I0715 11:31:00.615792 18508 solver.cpp:404]     Test net output #0: accuracy = 0.525
I0715 11:31:00.615854 18508 solver.cpp:404]     Test net output #1: loss = 2.07302 (* 1 = 2.07302 loss)
I0715 11:31:00.809406 18508 solver.cpp:228] Iteration 2000, loss = 1.88376e-05
I0715 11:31:00.809465 18508 solver.cpp:244]     Train net output #0: loss = 1.88111e-05 (* 1 = 1.88111e-05 loss)
I0715 11:31:00.809478 18508 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0715 11:31:08.660655 18508 solver.cpp:337] Iteration 2050, Testing net (#0)
I0715 11:31:18.610877 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5259
I0715 11:31:18.611217 18508 solver.cpp:404]     Test net output #1: loss = 2.07202 (* 1 = 2.07202 loss)
I0715 11:31:26.578279 18508 solver.cpp:337] Iteration 2100, Testing net (#0)
I0715 11:31:36.561784 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5258
I0715 11:31:36.561841 18508 solver.cpp:404]     Test net output #1: loss = 2.08337 (* 1 = 2.08337 loss)
I0715 11:31:36.751441 18508 solver.cpp:228] Iteration 2100, loss = 2.70522e-05
I0715 11:31:36.751498 18508 solver.cpp:244]     Train net output #0: loss = 2.70258e-05 (* 1 = 2.70258e-05 loss)
I0715 11:31:36.751509 18508 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0715 11:31:44.566249 18508 solver.cpp:337] Iteration 2150, Testing net (#0)
I0715 11:31:54.470232 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5259
I0715 11:31:54.470587 18508 solver.cpp:404]     Test net output #1: loss = 2.09942 (* 1 = 2.09942 loss)
I0715 11:32:02.458559 18508 solver.cpp:337] Iteration 2200, Testing net (#0)
I0715 11:32:14.493181 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5253
I0715 11:32:14.493245 18508 solver.cpp:404]     Test net output #1: loss = 2.10771 (* 1 = 2.10771 loss)
I0715 11:32:14.687518 18508 solver.cpp:228] Iteration 2200, loss = 2.20574e-06
I0715 11:32:14.687640 18508 solver.cpp:244]     Train net output #0: loss = 2.17933e-06 (* 1 = 2.17933e-06 loss)
I0715 11:32:14.687747 18508 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0715 11:32:23.896363 18508 solver.cpp:337] Iteration 2250, Testing net (#0)
I0715 11:32:35.371959 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5242
I0715 11:32:35.372517 18508 solver.cpp:404]     Test net output #1: loss = 2.11496 (* 1 = 2.11496 loss)
I0715 11:32:44.518138 18508 solver.cpp:337] Iteration 2300, Testing net (#0)
I0715 11:32:54.636997 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5257
I0715 11:32:54.637090 18508 solver.cpp:404]     Test net output #1: loss = 2.12084 (* 1 = 2.12084 loss)
I0715 11:32:54.826783 18508 solver.cpp:228] Iteration 2300, loss = 0.000147892
I0715 11:32:54.826866 18508 solver.cpp:244]     Train net output #0: loss = 0.000147865 (* 1 = 0.000147865 loss)
I0715 11:32:54.826900 18508 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0715 11:33:02.723480 18508 solver.cpp:337] Iteration 2350, Testing net (#0)
I0715 11:33:14.279690 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5251
I0715 11:33:14.280089 18508 solver.cpp:404]     Test net output #1: loss = 2.12262 (* 1 = 2.12262 loss)
I0715 11:33:22.508540 18508 solver.cpp:337] Iteration 2400, Testing net (#0)
I0715 11:33:32.897238 18508 solver.cpp:404]     Test net output #0: accuracy = 0.526
I0715 11:33:32.897369 18508 solver.cpp:404]     Test net output #1: loss = 2.12688 (* 1 = 2.12688 loss)
I0715 11:33:33.103965 18508 solver.cpp:228] Iteration 2400, loss = 7.65835e-07
I0715 11:33:33.104075 18508 solver.cpp:244]     Train net output #0: loss = 7.39473e-07 (* 1 = 7.39473e-07 loss)
I0715 11:33:33.104101 18508 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0715 11:33:41.063943 18508 solver.cpp:337] Iteration 2450, Testing net (#0)
I0715 11:33:51.348302 18508 solver.cpp:404]     Test net output #0: accuracy = 0.525
I0715 11:33:51.348444 18508 solver.cpp:404]     Test net output #1: loss = 2.13607 (* 1 = 2.13607 loss)
I0715 11:33:59.654676 18508 solver.cpp:337] Iteration 2500, Testing net (#0)
I0715 11:34:10.137914 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5252
I0715 11:34:10.137972 18508 solver.cpp:404]     Test net output #1: loss = 2.14519 (* 1 = 2.14519 loss)
I0715 11:34:10.331650 18508 solver.cpp:228] Iteration 2500, loss = 1.79402e-06
I0715 11:34:10.331715 18508 solver.cpp:244]     Train net output #0: loss = 1.76767e-06 (* 1 = 1.76767e-06 loss)
I0715 11:34:10.331728 18508 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0715 11:34:18.552801 18508 solver.cpp:337] Iteration 2550, Testing net (#0)
I0715 11:34:28.961424 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5248
I0715 11:34:28.961622 18508 solver.cpp:404]     Test net output #1: loss = 2.15412 (* 1 = 2.15412 loss)
I0715 11:34:37.241044 18508 solver.cpp:337] Iteration 2600, Testing net (#0)
I0715 11:34:47.189921 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5267
I0715 11:34:47.189973 18508 solver.cpp:404]     Test net output #1: loss = 2.17041 (* 1 = 2.17041 loss)
I0715 11:34:47.383491 18508 solver.cpp:228] Iteration 2600, loss = 2.99363e-06
I0715 11:34:47.383549 18508 solver.cpp:244]     Train net output #0: loss = 2.96727e-06 (* 1 = 2.96727e-06 loss)
I0715 11:34:47.383560 18508 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0715 11:34:55.148640 18508 solver.cpp:337] Iteration 2650, Testing net (#0)
I0715 11:35:06.064157 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5256
I0715 11:35:06.064268 18508 solver.cpp:404]     Test net output #1: loss = 2.18266 (* 1 = 2.18266 loss)
I0715 11:35:14.056030 18508 solver.cpp:337] Iteration 2700, Testing net (#0)
I0715 11:35:23.946933 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5258
I0715 11:35:23.946993 18508 solver.cpp:404]     Test net output #1: loss = 2.18709 (* 1 = 2.18709 loss)
I0715 11:35:24.149950 18508 solver.cpp:228] Iteration 2700, loss = 1.37087e-05
I0715 11:35:24.150013 18508 solver.cpp:244]     Train net output #0: loss = 1.36824e-05 (* 1 = 1.36824e-05 loss)
I0715 11:35:24.150025 18508 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0715 11:35:32.056504 18508 solver.cpp:337] Iteration 2750, Testing net (#0)
I0715 11:35:44.141760 18508 solver.cpp:404]     Test net output #0: accuracy = 0.5256
I0715 11:35:44.141841 18508 solver.cpp:404]     Test net output #1: loss = 2.19544 (* 1 = 2.19544 loss)
I0715 11:35:52.811723 18508 solver.cpp:337] Iteration 2800, Testing net (#0)
