I0711 14:01:28.443143  5319 caffe.cpp:178] Use CPU.
I0711 14:01:28.443487  5319 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 5000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0
snapshot: 5000
snapshot_prefix: "examples/siamese/My_mnist_siamese_0to8"
solver_mode: CPU
net: "examples/siamese/mnist_siamese_train_test.prototxt"
I0711 14:01:28.443598  5319 solver.cpp:91] Creating training net from net file: examples/siamese/mnist_siamese_train_test.prototxt
I0711 14:01:28.444102  5319 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer pair_data
I0711 14:01:28.444252  5319 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TRAIN
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_train_leveldb_0to8"
    batch_size: 64
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0711 14:01:28.444409  5319 layer_factory.hpp:77] Creating layer pair_data
I0711 14:01:28.444958  5319 net.cpp:91] Creating Layer pair_data
I0711 14:01:28.444979  5319 net.cpp:399] pair_data -> pair_data
I0711 14:01:28.445005  5319 net.cpp:399] pair_data -> sim
I0711 14:01:28.511580  5323 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_train_leveldb_0to8
I0711 14:01:28.511863  5319 data_layer.cpp:41] output data size: 64,2,28,28
I0711 14:01:28.512239  5319 net.cpp:141] Setting up pair_data
I0711 14:01:28.512269  5319 net.cpp:148] Top shape: 64 2 28 28 (100352)
I0711 14:01:28.512275  5319 net.cpp:148] Top shape: 64 (64)
I0711 14:01:28.512279  5319 net.cpp:156] Memory required for data: 401664
I0711 14:01:28.512287  5319 layer_factory.hpp:77] Creating layer slice_pair
I0711 14:01:28.512302  5319 net.cpp:91] Creating Layer slice_pair
I0711 14:01:28.512307  5319 net.cpp:425] slice_pair <- pair_data
I0711 14:01:28.512316  5319 net.cpp:399] slice_pair -> data
I0711 14:01:28.512326  5319 net.cpp:399] slice_pair -> data_p
I0711 14:01:28.512334  5319 net.cpp:141] Setting up slice_pair
I0711 14:01:28.512338  5319 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0711 14:01:28.512342  5319 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0711 14:01:28.512344  5319 net.cpp:156] Memory required for data: 803072
I0711 14:01:28.512347  5319 layer_factory.hpp:77] Creating layer conv1
I0711 14:01:28.512362  5319 net.cpp:91] Creating Layer conv1
I0711 14:01:28.512364  5319 net.cpp:425] conv1 <- data
I0711 14:01:28.512368  5319 net.cpp:399] conv1 -> conv1
I0711 14:01:28.512413  5319 net.cpp:141] Setting up conv1
I0711 14:01:28.512418  5319 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0711 14:01:28.512419  5319 net.cpp:156] Memory required for data: 3752192
I0711 14:01:28.512428  5319 layer_factory.hpp:77] Creating layer pool1
I0711 14:01:28.512434  5319 net.cpp:91] Creating Layer pool1
I0711 14:01:28.512436  5319 net.cpp:425] pool1 <- conv1
I0711 14:01:28.512440  5319 net.cpp:399] pool1 -> pool1
I0711 14:01:28.512459  5319 net.cpp:141] Setting up pool1
I0711 14:01:28.512462  5319 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0711 14:01:28.512465  5319 net.cpp:156] Memory required for data: 4489472
I0711 14:01:28.512467  5319 layer_factory.hpp:77] Creating layer conv2
I0711 14:01:28.512476  5319 net.cpp:91] Creating Layer conv2
I0711 14:01:28.512480  5319 net.cpp:425] conv2 <- pool1
I0711 14:01:28.512485  5319 net.cpp:399] conv2 -> conv2
I0711 14:01:28.512902  5319 net.cpp:141] Setting up conv2
I0711 14:01:28.512922  5319 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0711 14:01:28.512926  5319 net.cpp:156] Memory required for data: 5308672
I0711 14:01:28.512938  5319 layer_factory.hpp:77] Creating layer pool2
I0711 14:01:28.512966  5319 net.cpp:91] Creating Layer pool2
I0711 14:01:28.513000  5319 net.cpp:425] pool2 <- conv2
I0711 14:01:28.513012  5319 net.cpp:399] pool2 -> pool2
I0711 14:01:28.513025  5319 net.cpp:141] Setting up pool2
I0711 14:01:28.513031  5319 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0711 14:01:28.513034  5319 net.cpp:156] Memory required for data: 5513472
I0711 14:01:28.513038  5319 layer_factory.hpp:77] Creating layer ip1
I0711 14:01:28.513053  5319 net.cpp:91] Creating Layer ip1
I0711 14:01:28.513058  5319 net.cpp:425] ip1 <- pool2
I0711 14:01:28.513067  5319 net.cpp:399] ip1 -> ip1
I0711 14:01:28.517791  5319 net.cpp:141] Setting up ip1
I0711 14:01:28.517868  5319 net.cpp:148] Top shape: 64 500 (32000)
I0711 14:01:28.517877  5319 net.cpp:156] Memory required for data: 5641472
I0711 14:01:28.517897  5319 layer_factory.hpp:77] Creating layer relu1
I0711 14:01:28.517913  5319 net.cpp:91] Creating Layer relu1
I0711 14:01:28.517920  5319 net.cpp:425] relu1 <- ip1
I0711 14:01:28.517930  5319 net.cpp:386] relu1 -> ip1 (in-place)
I0711 14:01:28.517946  5319 net.cpp:141] Setting up relu1
I0711 14:01:28.517953  5319 net.cpp:148] Top shape: 64 500 (32000)
I0711 14:01:28.517957  5319 net.cpp:156] Memory required for data: 5769472
I0711 14:01:28.517962  5319 layer_factory.hpp:77] Creating layer ip2
I0711 14:01:28.517976  5319 net.cpp:91] Creating Layer ip2
I0711 14:01:28.517979  5319 net.cpp:425] ip2 <- ip1
I0711 14:01:28.517988  5319 net.cpp:399] ip2 -> ip2
I0711 14:01:28.518061  5319 net.cpp:141] Setting up ip2
I0711 14:01:28.518069  5319 net.cpp:148] Top shape: 64 10 (640)
I0711 14:01:28.518072  5319 net.cpp:156] Memory required for data: 5772032
I0711 14:01:28.518079  5319 layer_factory.hpp:77] Creating layer feat
I0711 14:01:28.518087  5319 net.cpp:91] Creating Layer feat
I0711 14:01:28.518091  5319 net.cpp:425] feat <- ip2
I0711 14:01:28.518100  5319 net.cpp:399] feat -> feat
I0711 14:01:28.518113  5319 net.cpp:141] Setting up feat
I0711 14:01:28.518120  5319 net.cpp:148] Top shape: 64 2 (128)
I0711 14:01:28.518123  5319 net.cpp:156] Memory required for data: 5772544
I0711 14:01:28.518131  5319 layer_factory.hpp:77] Creating layer conv1_p
I0711 14:01:28.518149  5319 net.cpp:91] Creating Layer conv1_p
I0711 14:01:28.518154  5319 net.cpp:425] conv1_p <- data_p
I0711 14:01:28.518162  5319 net.cpp:399] conv1_p -> conv1_p
I0711 14:01:28.518198  5319 net.cpp:141] Setting up conv1_p
I0711 14:01:28.518204  5319 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0711 14:01:28.518208  5319 net.cpp:156] Memory required for data: 8721664
I0711 14:01:28.518213  5319 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0711 14:01:28.518219  5319 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0711 14:01:28.518224  5319 layer_factory.hpp:77] Creating layer pool1_p
I0711 14:01:28.518234  5319 net.cpp:91] Creating Layer pool1_p
I0711 14:01:28.518239  5319 net.cpp:425] pool1_p <- conv1_p
I0711 14:01:28.518245  5319 net.cpp:399] pool1_p -> pool1_p
I0711 14:01:28.518256  5319 net.cpp:141] Setting up pool1_p
I0711 14:01:28.518263  5319 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0711 14:01:28.518267  5319 net.cpp:156] Memory required for data: 9458944
I0711 14:01:28.518271  5319 layer_factory.hpp:77] Creating layer conv2_p
I0711 14:01:28.518283  5319 net.cpp:91] Creating Layer conv2_p
I0711 14:01:28.518288  5319 net.cpp:425] conv2_p <- pool1_p
I0711 14:01:28.518296  5319 net.cpp:399] conv2_p -> conv2_p
I0711 14:01:28.518590  5319 net.cpp:141] Setting up conv2_p
I0711 14:01:28.518599  5319 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0711 14:01:28.518604  5319 net.cpp:156] Memory required for data: 10278144
I0711 14:01:28.518609  5319 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0711 14:01:28.518613  5319 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0711 14:01:28.518618  5319 layer_factory.hpp:77] Creating layer pool2_p
I0711 14:01:28.518627  5319 net.cpp:91] Creating Layer pool2_p
I0711 14:01:28.518631  5319 net.cpp:425] pool2_p <- conv2_p
I0711 14:01:28.518638  5319 net.cpp:399] pool2_p -> pool2_p
I0711 14:01:28.518682  5319 net.cpp:141] Setting up pool2_p
I0711 14:01:28.518690  5319 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0711 14:01:28.518694  5319 net.cpp:156] Memory required for data: 10482944
I0711 14:01:28.518698  5319 layer_factory.hpp:77] Creating layer ip1_p
I0711 14:01:28.518707  5319 net.cpp:91] Creating Layer ip1_p
I0711 14:01:28.518712  5319 net.cpp:425] ip1_p <- pool2_p
I0711 14:01:28.518720  5319 net.cpp:399] ip1_p -> ip1_p
I0711 14:01:28.524271  5319 net.cpp:141] Setting up ip1_p
I0711 14:01:28.524369  5319 net.cpp:148] Top shape: 64 500 (32000)
I0711 14:01:28.524377  5319 net.cpp:156] Memory required for data: 10610944
I0711 14:01:28.524389  5319 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0711 14:01:28.524397  5319 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0711 14:01:28.524404  5319 layer_factory.hpp:77] Creating layer relu1_p
I0711 14:01:28.524466  5319 net.cpp:91] Creating Layer relu1_p
I0711 14:01:28.524478  5319 net.cpp:425] relu1_p <- ip1_p
I0711 14:01:28.524488  5319 net.cpp:386] relu1_p -> ip1_p (in-place)
I0711 14:01:28.524502  5319 net.cpp:141] Setting up relu1_p
I0711 14:01:28.524509  5319 net.cpp:148] Top shape: 64 500 (32000)
I0711 14:01:28.524514  5319 net.cpp:156] Memory required for data: 10738944
I0711 14:01:28.524518  5319 layer_factory.hpp:77] Creating layer ip2_p
I0711 14:01:28.524540  5319 net.cpp:91] Creating Layer ip2_p
I0711 14:01:28.524545  5319 net.cpp:425] ip2_p <- ip1_p
I0711 14:01:28.524554  5319 net.cpp:399] ip2_p -> ip2_p
I0711 14:01:28.524633  5319 net.cpp:141] Setting up ip2_p
I0711 14:01:28.524641  5319 net.cpp:148] Top shape: 64 10 (640)
I0711 14:01:28.524644  5319 net.cpp:156] Memory required for data: 10741504
I0711 14:01:28.524653  5319 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0711 14:01:28.524658  5319 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0711 14:01:28.524662  5319 layer_factory.hpp:77] Creating layer feat_p
I0711 14:01:28.524672  5319 net.cpp:91] Creating Layer feat_p
I0711 14:01:28.524677  5319 net.cpp:425] feat_p <- ip2_p
I0711 14:01:28.524683  5319 net.cpp:399] feat_p -> feat_p
I0711 14:01:28.524698  5319 net.cpp:141] Setting up feat_p
I0711 14:01:28.524703  5319 net.cpp:148] Top shape: 64 2 (128)
I0711 14:01:28.524706  5319 net.cpp:156] Memory required for data: 10742016
I0711 14:01:28.524710  5319 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0711 14:01:28.524714  5319 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0711 14:01:28.524718  5319 layer_factory.hpp:77] Creating layer loss
I0711 14:01:28.524742  5319 net.cpp:91] Creating Layer loss
I0711 14:01:28.524747  5319 net.cpp:425] loss <- feat
I0711 14:01:28.524754  5319 net.cpp:425] loss <- feat_p
I0711 14:01:28.524758  5319 net.cpp:425] loss <- sim
I0711 14:01:28.524768  5319 net.cpp:399] loss -> loss
I0711 14:01:28.524788  5319 net.cpp:141] Setting up loss
I0711 14:01:28.524794  5319 net.cpp:148] Top shape: (1)
I0711 14:01:28.524798  5319 net.cpp:151]     with loss weight 1
I0711 14:01:28.524833  5319 net.cpp:156] Memory required for data: 10742020
I0711 14:01:28.524838  5319 net.cpp:217] loss needs backward computation.
I0711 14:01:28.524843  5319 net.cpp:217] feat_p needs backward computation.
I0711 14:01:28.524847  5319 net.cpp:217] ip2_p needs backward computation.
I0711 14:01:28.524852  5319 net.cpp:217] relu1_p needs backward computation.
I0711 14:01:28.524855  5319 net.cpp:217] ip1_p needs backward computation.
I0711 14:01:28.524859  5319 net.cpp:217] pool2_p needs backward computation.
I0711 14:01:28.524863  5319 net.cpp:217] conv2_p needs backward computation.
I0711 14:01:28.524868  5319 net.cpp:217] pool1_p needs backward computation.
I0711 14:01:28.524873  5319 net.cpp:217] conv1_p needs backward computation.
I0711 14:01:28.524876  5319 net.cpp:217] feat needs backward computation.
I0711 14:01:28.524880  5319 net.cpp:217] ip2 needs backward computation.
I0711 14:01:28.524912  5319 net.cpp:217] relu1 needs backward computation.
I0711 14:01:28.524917  5319 net.cpp:217] ip1 needs backward computation.
I0711 14:01:28.524922  5319 net.cpp:217] pool2 needs backward computation.
I0711 14:01:28.524927  5319 net.cpp:217] conv2 needs backward computation.
I0711 14:01:28.524932  5319 net.cpp:217] pool1 needs backward computation.
I0711 14:01:28.524936  5319 net.cpp:217] conv1 needs backward computation.
I0711 14:01:28.524941  5319 net.cpp:219] slice_pair does not need backward computation.
I0711 14:01:28.524948  5319 net.cpp:219] pair_data does not need backward computation.
I0711 14:01:28.524952  5319 net.cpp:261] This network produces output loss
I0711 14:01:28.525122  5319 net.cpp:274] Network initialization done.
I0711 14:01:28.531571  5319 solver.cpp:181] Creating test net (#0) specified by net file: examples/siamese/mnist_siamese_train_test.prototxt
I0711 14:01:28.531805  5319 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0711 14:01:28.532024  5319 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb_0to8"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0711 14:01:28.532148  5319 layer_factory.hpp:77] Creating layer pair_data
I0711 14:01:28.532263  5319 net.cpp:91] Creating Layer pair_data
I0711 14:01:28.532271  5319 net.cpp:399] pair_data -> pair_data
I0711 14:01:28.532284  5319 net.cpp:399] pair_data -> sim
I0711 14:01:28.591189  5325 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb_0to8
I0711 14:01:28.591487  5319 data_layer.cpp:41] output data size: 100,2,28,28
I0711 14:01:28.592396  5319 net.cpp:141] Setting up pair_data
I0711 14:01:28.592447  5319 net.cpp:148] Top shape: 100 2 28 28 (156800)
I0711 14:01:28.592458  5319 net.cpp:148] Top shape: 100 (100)
I0711 14:01:28.592463  5319 net.cpp:156] Memory required for data: 627600
I0711 14:01:28.592473  5319 layer_factory.hpp:77] Creating layer slice_pair
I0711 14:01:28.592489  5319 net.cpp:91] Creating Layer slice_pair
I0711 14:01:28.592506  5319 net.cpp:425] slice_pair <- pair_data
I0711 14:01:28.592514  5319 net.cpp:399] slice_pair -> data
I0711 14:01:28.592524  5319 net.cpp:399] slice_pair -> data_p
I0711 14:01:28.592533  5319 net.cpp:141] Setting up slice_pair
I0711 14:01:28.592540  5319 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0711 14:01:28.592555  5319 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0711 14:01:28.592558  5319 net.cpp:156] Memory required for data: 1254800
I0711 14:01:28.592561  5319 layer_factory.hpp:77] Creating layer conv1
I0711 14:01:28.592573  5319 net.cpp:91] Creating Layer conv1
I0711 14:01:28.592576  5319 net.cpp:425] conv1 <- data
I0711 14:01:28.592582  5319 net.cpp:399] conv1 -> conv1
I0711 14:01:28.592623  5319 net.cpp:141] Setting up conv1
I0711 14:01:28.592641  5319 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0711 14:01:28.592644  5319 net.cpp:156] Memory required for data: 5862800
I0711 14:01:28.592653  5319 layer_factory.hpp:77] Creating layer pool1
I0711 14:01:28.592659  5319 net.cpp:91] Creating Layer pool1
I0711 14:01:28.592633  5326 blocking_queue.cpp:50] Waiting for data
I0711 14:01:28.592746  5319 net.cpp:425] pool1 <- conv1
I0711 14:01:28.592768  5319 net.cpp:399] pool1 -> pool1
I0711 14:01:28.592780  5319 net.cpp:141] Setting up pool1
I0711 14:01:28.592785  5319 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0711 14:01:28.592788  5319 net.cpp:156] Memory required for data: 7014800
I0711 14:01:28.592792  5319 layer_factory.hpp:77] Creating layer conv2
I0711 14:01:28.592824  5319 net.cpp:91] Creating Layer conv2
I0711 14:01:28.592828  5319 net.cpp:425] conv2 <- pool1
I0711 14:01:28.592833  5319 net.cpp:399] conv2 -> conv2
I0711 14:01:28.593050  5319 net.cpp:141] Setting up conv2
I0711 14:01:28.593096  5319 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0711 14:01:28.593101  5319 net.cpp:156] Memory required for data: 8294800
I0711 14:01:28.593116  5319 layer_factory.hpp:77] Creating layer pool2
I0711 14:01:28.593122  5319 net.cpp:91] Creating Layer pool2
I0711 14:01:28.593124  5319 net.cpp:425] pool2 <- conv2
I0711 14:01:28.593129  5319 net.cpp:399] pool2 -> pool2
I0711 14:01:28.593137  5319 net.cpp:141] Setting up pool2
I0711 14:01:28.593140  5319 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0711 14:01:28.593142  5319 net.cpp:156] Memory required for data: 8614800
I0711 14:01:28.593145  5319 layer_factory.hpp:77] Creating layer ip1
I0711 14:01:28.593152  5319 net.cpp:91] Creating Layer ip1
I0711 14:01:28.593154  5319 net.cpp:425] ip1 <- pool2
I0711 14:01:28.593158  5319 net.cpp:399] ip1 -> ip1
I0711 14:01:28.596077  5319 net.cpp:141] Setting up ip1
I0711 14:01:28.596148  5319 net.cpp:148] Top shape: 100 500 (50000)
I0711 14:01:28.596153  5319 net.cpp:156] Memory required for data: 8814800
I0711 14:01:28.596169  5319 layer_factory.hpp:77] Creating layer relu1
I0711 14:01:28.596182  5319 net.cpp:91] Creating Layer relu1
I0711 14:01:28.596186  5319 net.cpp:425] relu1 <- ip1
I0711 14:01:28.596191  5319 net.cpp:386] relu1 -> ip1 (in-place)
I0711 14:01:28.596201  5319 net.cpp:141] Setting up relu1
I0711 14:01:28.596205  5319 net.cpp:148] Top shape: 100 500 (50000)
I0711 14:01:28.596209  5319 net.cpp:156] Memory required for data: 9014800
I0711 14:01:28.596210  5319 layer_factory.hpp:77] Creating layer ip2
I0711 14:01:28.596220  5319 net.cpp:91] Creating Layer ip2
I0711 14:01:28.596222  5319 net.cpp:425] ip2 <- ip1
I0711 14:01:28.596227  5319 net.cpp:399] ip2 -> ip2
I0711 14:01:28.596285  5319 net.cpp:141] Setting up ip2
I0711 14:01:28.596290  5319 net.cpp:148] Top shape: 100 10 (1000)
I0711 14:01:28.596292  5319 net.cpp:156] Memory required for data: 9018800
I0711 14:01:28.596297  5319 layer_factory.hpp:77] Creating layer feat
I0711 14:01:28.596304  5319 net.cpp:91] Creating Layer feat
I0711 14:01:28.596307  5319 net.cpp:425] feat <- ip2
I0711 14:01:28.596312  5319 net.cpp:399] feat -> feat
I0711 14:01:28.596319  5319 net.cpp:141] Setting up feat
I0711 14:01:28.596323  5319 net.cpp:148] Top shape: 100 2 (200)
I0711 14:01:28.596325  5319 net.cpp:156] Memory required for data: 9019600
I0711 14:01:28.596330  5319 layer_factory.hpp:77] Creating layer conv1_p
I0711 14:01:28.596340  5319 net.cpp:91] Creating Layer conv1_p
I0711 14:01:28.596343  5319 net.cpp:425] conv1_p <- data_p
I0711 14:01:28.596348  5319 net.cpp:399] conv1_p -> conv1_p
I0711 14:01:28.596372  5319 net.cpp:141] Setting up conv1_p
I0711 14:01:28.596379  5319 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0711 14:01:28.596380  5319 net.cpp:156] Memory required for data: 13627600
I0711 14:01:28.596384  5319 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0711 14:01:28.596386  5319 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0711 14:01:28.596390  5319 layer_factory.hpp:77] Creating layer pool1_p
I0711 14:01:28.596395  5319 net.cpp:91] Creating Layer pool1_p
I0711 14:01:28.596396  5319 net.cpp:425] pool1_p <- conv1_p
I0711 14:01:28.596401  5319 net.cpp:399] pool1_p -> pool1_p
I0711 14:01:28.596410  5319 net.cpp:141] Setting up pool1_p
I0711 14:01:28.596412  5319 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0711 14:01:28.596415  5319 net.cpp:156] Memory required for data: 14779600
I0711 14:01:28.596417  5319 layer_factory.hpp:77] Creating layer conv2_p
I0711 14:01:28.596427  5319 net.cpp:91] Creating Layer conv2_p
I0711 14:01:28.596431  5319 net.cpp:425] conv2_p <- pool1_p
I0711 14:01:28.596434  5319 net.cpp:399] conv2_p -> conv2_p
I0711 14:01:28.596633  5319 net.cpp:141] Setting up conv2_p
I0711 14:01:28.596658  5319 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0711 14:01:28.596745  5319 net.cpp:156] Memory required for data: 16059600
I0711 14:01:28.596751  5319 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0711 14:01:28.596756  5319 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0711 14:01:28.596760  5319 layer_factory.hpp:77] Creating layer pool2_p
I0711 14:01:28.596765  5319 net.cpp:91] Creating Layer pool2_p
I0711 14:01:28.596768  5319 net.cpp:425] pool2_p <- conv2_p
I0711 14:01:28.596776  5319 net.cpp:399] pool2_p -> pool2_p
I0711 14:01:28.596786  5319 net.cpp:141] Setting up pool2_p
I0711 14:01:28.596804  5319 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0711 14:01:28.596807  5319 net.cpp:156] Memory required for data: 16379600
I0711 14:01:28.596810  5319 layer_factory.hpp:77] Creating layer ip1_p
I0711 14:01:28.596817  5319 net.cpp:91] Creating Layer ip1_p
I0711 14:01:28.596819  5319 net.cpp:425] ip1_p <- pool2_p
I0711 14:01:28.596825  5319 net.cpp:399] ip1_p -> ip1_p
I0711 14:01:28.600260  5319 net.cpp:141] Setting up ip1_p
I0711 14:01:28.600363  5319 net.cpp:148] Top shape: 100 500 (50000)
I0711 14:01:28.600702  5319 net.cpp:156] Memory required for data: 16579600
I0711 14:01:28.600714  5319 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0711 14:01:28.600723  5319 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0711 14:01:28.600728  5319 layer_factory.hpp:77] Creating layer relu1_p
I0711 14:01:28.600741  5319 net.cpp:91] Creating Layer relu1_p
I0711 14:01:28.600746  5319 net.cpp:425] relu1_p <- ip1_p
I0711 14:01:28.600762  5319 net.cpp:386] relu1_p -> ip1_p (in-place)
I0711 14:01:28.600775  5319 net.cpp:141] Setting up relu1_p
I0711 14:01:28.600782  5319 net.cpp:148] Top shape: 100 500 (50000)
I0711 14:01:28.600785  5319 net.cpp:156] Memory required for data: 16779600
I0711 14:01:28.600790  5319 layer_factory.hpp:77] Creating layer ip2_p
I0711 14:01:28.600802  5319 net.cpp:91] Creating Layer ip2_p
I0711 14:01:28.600806  5319 net.cpp:425] ip2_p <- ip1_p
I0711 14:01:28.600813  5319 net.cpp:399] ip2_p -> ip2_p
I0711 14:01:28.600884  5319 net.cpp:141] Setting up ip2_p
I0711 14:01:28.600894  5319 net.cpp:148] Top shape: 100 10 (1000)
I0711 14:01:28.600899  5319 net.cpp:156] Memory required for data: 16783600
I0711 14:01:28.600908  5319 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0711 14:01:28.600914  5319 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0711 14:01:28.600919  5319 layer_factory.hpp:77] Creating layer feat_p
I0711 14:01:28.600929  5319 net.cpp:91] Creating Layer feat_p
I0711 14:01:28.600934  5319 net.cpp:425] feat_p <- ip2_p
I0711 14:01:28.600940  5319 net.cpp:399] feat_p -> feat_p
I0711 14:01:28.600956  5319 net.cpp:141] Setting up feat_p
I0711 14:01:28.600963  5319 net.cpp:148] Top shape: 100 2 (200)
I0711 14:01:28.600968  5319 net.cpp:156] Memory required for data: 16784400
I0711 14:01:28.600972  5319 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0711 14:01:28.600978  5319 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0711 14:01:28.600982  5319 layer_factory.hpp:77] Creating layer loss
I0711 14:01:28.600991  5319 net.cpp:91] Creating Layer loss
I0711 14:01:28.600996  5319 net.cpp:425] loss <- feat
I0711 14:01:28.601001  5319 net.cpp:425] loss <- feat_p
I0711 14:01:28.601007  5319 net.cpp:425] loss <- sim
I0711 14:01:28.601013  5319 net.cpp:399] loss -> loss
I0711 14:01:28.601029  5319 net.cpp:141] Setting up loss
I0711 14:01:28.601037  5319 net.cpp:148] Top shape: (1)
I0711 14:01:28.601040  5319 net.cpp:151]     with loss weight 1
I0711 14:01:28.601057  5319 net.cpp:156] Memory required for data: 16784404
I0711 14:01:28.601061  5319 net.cpp:217] loss needs backward computation.
I0711 14:01:28.601066  5319 net.cpp:217] feat_p needs backward computation.
I0711 14:01:28.601071  5319 net.cpp:217] ip2_p needs backward computation.
I0711 14:01:28.601075  5319 net.cpp:217] relu1_p needs backward computation.
I0711 14:01:28.601197  5319 net.cpp:217] ip1_p needs backward computation.
I0711 14:01:28.601204  5319 net.cpp:217] pool2_p needs backward computation.
I0711 14:01:28.601209  5319 net.cpp:217] conv2_p needs backward computation.
I0711 14:01:28.601213  5319 net.cpp:217] pool1_p needs backward computation.
I0711 14:01:28.601217  5319 net.cpp:217] conv1_p needs backward computation.
I0711 14:01:28.601222  5319 net.cpp:217] feat needs backward computation.
I0711 14:01:28.601227  5319 net.cpp:217] ip2 needs backward computation.
I0711 14:01:28.601232  5319 net.cpp:217] relu1 needs backward computation.
I0711 14:01:28.601236  5319 net.cpp:217] ip1 needs backward computation.
I0711 14:01:28.601240  5319 net.cpp:217] pool2 needs backward computation.
I0711 14:01:28.601244  5319 net.cpp:217] conv2 needs backward computation.
I0711 14:01:28.601248  5319 net.cpp:217] pool1 needs backward computation.
I0711 14:01:28.601253  5319 net.cpp:217] conv1 needs backward computation.
I0711 14:01:28.601258  5319 net.cpp:219] slice_pair does not need backward computation.
I0711 14:01:28.601264  5319 net.cpp:219] pair_data does not need backward computation.
I0711 14:01:28.601269  5319 net.cpp:261] This network produces output loss
I0711 14:01:28.601297  5319 net.cpp:274] Network initialization done.
I0711 14:01:28.601439  5319 solver.cpp:60] Solver scaffolding done.
I0711 14:01:28.601505  5319 caffe.cpp:219] Starting Optimization
I0711 14:01:28.601516  5319 solver.cpp:279] Solving mnist_siamese_train_test
I0711 14:01:28.601521  5319 solver.cpp:280] Learning Rate Policy: inv
I0711 14:01:28.602191  5319 solver.cpp:337] Iteration 0, Testing net (#0)
I0711 14:01:38.672132  5319 solver.cpp:404]     Test net output #0: loss = 0.2542 (* 1 = 0.2542 loss)
I0711 14:01:38.845149  5319 solver.cpp:228] Iteration 0, loss = 0.26046
I0711 14:01:38.845214  5319 solver.cpp:244]     Train net output #0: loss = 0.26046 (* 1 = 0.26046 loss)
I0711 14:01:38.845234  5319 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0711 14:01:54.484844  5319 solver.cpp:228] Iteration 100, loss = 0.0185609
I0711 14:01:54.485119  5319 solver.cpp:244]     Train net output #0: loss = 0.0185609 (* 1 = 0.0185609 loss)
I0711 14:01:54.486711  5319 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0711 14:02:10.541502  5319 solver.cpp:228] Iteration 200, loss = 0.0267344
I0711 14:02:10.541607  5319 solver.cpp:244]     Train net output #0: loss = 0.0267344 (* 1 = 0.0267344 loss)
I0711 14:02:10.541651  5319 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0711 14:02:26.543653  5319 solver.cpp:228] Iteration 300, loss = 0.0440846
I0711 14:02:26.543720  5319 solver.cpp:244]     Train net output #0: loss = 0.0440846 (* 1 = 0.0440846 loss)
I0711 14:02:26.543732  5319 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0711 14:02:42.568212  5319 solver.cpp:228] Iteration 400, loss = 0.0349594
I0711 14:02:42.568567  5319 solver.cpp:244]     Train net output #0: loss = 0.0349594 (* 1 = 0.0349594 loss)
I0711 14:02:42.568711  5319 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0711 14:02:58.403087  5319 solver.cpp:337] Iteration 500, Testing net (#0)
I0711 14:03:08.556929  5319 solver.cpp:404]     Test net output #0: loss = 0.0336022 (* 1 = 0.0336022 loss)
I0711 14:03:08.751750  5319 solver.cpp:228] Iteration 500, loss = 0.046617
I0711 14:03:08.751813  5319 solver.cpp:244]     Train net output #0: loss = 0.046617 (* 1 = 0.046617 loss)
I0711 14:03:08.751824  5319 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0711 14:03:24.382210  5319 solver.cpp:228] Iteration 600, loss = 0.0239194
I0711 14:03:24.382408  5319 solver.cpp:244]     Train net output #0: loss = 0.0239194 (* 1 = 0.0239194 loss)
I0711 14:03:24.382423  5319 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0711 14:03:40.286705  5319 solver.cpp:228] Iteration 700, loss = 0.0370117
I0711 14:03:40.286895  5319 solver.cpp:244]     Train net output #0: loss = 0.0370117 (* 1 = 0.0370117 loss)
I0711 14:03:40.286939  5319 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0711 14:03:56.316720  5319 solver.cpp:228] Iteration 800, loss = 0.0158143
I0711 14:03:56.317127  5319 solver.cpp:244]     Train net output #0: loss = 0.0158143 (* 1 = 0.0158143 loss)
I0711 14:03:56.317198  5319 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0711 14:04:12.276167  5319 solver.cpp:228] Iteration 900, loss = 0.0424235
I0711 14:04:12.276357  5319 solver.cpp:244]     Train net output #0: loss = 0.0424235 (* 1 = 0.0424235 loss)
I0711 14:04:12.276397  5319 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0711 14:04:27.985782  5319 solver.cpp:337] Iteration 1000, Testing net (#0)
I0711 14:04:37.963482  5319 solver.cpp:404]     Test net output #0: loss = 0.0243396 (* 1 = 0.0243396 loss)
I0711 14:04:38.147639  5319 solver.cpp:228] Iteration 1000, loss = 0.0220269
I0711 14:04:38.147809  5319 solver.cpp:244]     Train net output #0: loss = 0.0220269 (* 1 = 0.0220269 loss)
I0711 14:04:38.147847  5319 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0711 14:04:54.008698  5319 solver.cpp:228] Iteration 1100, loss = 0.0124622
I0711 14:04:54.008898  5319 solver.cpp:244]     Train net output #0: loss = 0.0124622 (* 1 = 0.0124622 loss)
I0711 14:04:54.008944  5319 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0711 14:05:10.015161  5319 solver.cpp:228] Iteration 1200, loss = 0.0194647
I0711 14:05:10.015440  5319 solver.cpp:244]     Train net output #0: loss = 0.0194647 (* 1 = 0.0194647 loss)
I0711 14:05:10.015470  5319 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0711 14:05:25.953877  5319 solver.cpp:228] Iteration 1300, loss = 0.0106573
I0711 14:05:25.954051  5319 solver.cpp:244]     Train net output #0: loss = 0.0106573 (* 1 = 0.0106573 loss)
I0711 14:05:25.954092  5319 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0711 14:05:41.772418  5319 solver.cpp:228] Iteration 1400, loss = 0.0236597
I0711 14:05:41.772635  5319 solver.cpp:244]     Train net output #0: loss = 0.0236597 (* 1 = 0.0236597 loss)
I0711 14:05:41.772678  5319 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0711 14:05:57.515069  5319 solver.cpp:337] Iteration 1500, Testing net (#0)
I0711 14:06:07.523011  5319 solver.cpp:404]     Test net output #0: loss = 0.0202777 (* 1 = 0.0202777 loss)
I0711 14:06:07.712829  5319 solver.cpp:228] Iteration 1500, loss = 0.0155526
I0711 14:06:07.713006  5319 solver.cpp:244]     Train net output #0: loss = 0.0155526 (* 1 = 0.0155526 loss)
I0711 14:06:07.713034  5319 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0711 14:06:23.319984  5319 solver.cpp:228] Iteration 1600, loss = 0.00721608
I0711 14:06:23.320268  5319 solver.cpp:244]     Train net output #0: loss = 0.00721608 (* 1 = 0.00721608 loss)
I0711 14:06:23.320345  5319 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0711 14:06:40.949416  5319 solver.cpp:228] Iteration 1700, loss = 0.0108056
I0711 14:06:40.949575  5319 solver.cpp:244]     Train net output #0: loss = 0.0108056 (* 1 = 0.0108056 loss)
I0711 14:06:40.949620  5319 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0711 14:06:58.535719  5319 solver.cpp:228] Iteration 1800, loss = 0.0400681
I0711 14:06:58.536360  5319 solver.cpp:244]     Train net output #0: loss = 0.0400681 (* 1 = 0.0400681 loss)
I0711 14:06:58.536384  5319 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0711 14:07:14.396972  5319 solver.cpp:228] Iteration 1900, loss = 0.0168013
I0711 14:07:14.397034  5319 solver.cpp:244]     Train net output #0: loss = 0.0168013 (* 1 = 0.0168013 loss)
I0711 14:07:14.397045  5319 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0711 14:07:29.900416  5319 solver.cpp:337] Iteration 2000, Testing net (#0)
I0711 14:07:39.650023  5319 solver.cpp:404]     Test net output #0: loss = 0.0182633 (* 1 = 0.0182633 loss)
I0711 14:07:39.845460  5319 solver.cpp:228] Iteration 2000, loss = 0.0110439
I0711 14:07:39.845634  5319 solver.cpp:244]     Train net output #0: loss = 0.0110439 (* 1 = 0.0110439 loss)
I0711 14:07:39.845666  5319 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0711 14:07:55.598958  5319 solver.cpp:228] Iteration 2100, loss = 0.0246405
I0711 14:07:55.599020  5319 solver.cpp:244]     Train net output #0: loss = 0.0246405 (* 1 = 0.0246405 loss)
I0711 14:07:55.599031  5319 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0711 14:08:11.519251  5319 solver.cpp:228] Iteration 2200, loss = 0.0273239
I0711 14:08:11.519528  5319 solver.cpp:244]     Train net output #0: loss = 0.0273239 (* 1 = 0.0273239 loss)
I0711 14:08:11.519551  5319 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0711 14:08:27.237743  5319 solver.cpp:228] Iteration 2300, loss = 0.0131428
I0711 14:08:27.237916  5319 solver.cpp:244]     Train net output #0: loss = 0.0131428 (* 1 = 0.0131428 loss)
I0711 14:08:27.237956  5319 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0711 14:08:42.823959  5319 solver.cpp:228] Iteration 2400, loss = 0.0183532
I0711 14:08:42.824131  5319 solver.cpp:244]     Train net output #0: loss = 0.0183532 (* 1 = 0.0183532 loss)
I0711 14:08:42.824168  5319 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0711 14:08:58.257514  5319 solver.cpp:337] Iteration 2500, Testing net (#0)
I0711 14:09:08.082901  5319 solver.cpp:404]     Test net output #0: loss = 0.0160827 (* 1 = 0.0160827 loss)
I0711 14:09:08.293032  5319 solver.cpp:228] Iteration 2500, loss = 0.0190995
I0711 14:09:08.293210  5319 solver.cpp:244]     Train net output #0: loss = 0.0190995 (* 1 = 0.0190995 loss)
I0711 14:09:08.293239  5319 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0711 14:09:23.909924  5319 solver.cpp:228] Iteration 2600, loss = 0.0122964
I0711 14:09:23.910015  5319 solver.cpp:244]     Train net output #0: loss = 0.0122964 (* 1 = 0.0122964 loss)
I0711 14:09:23.910027  5319 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0711 14:09:39.638845  5319 solver.cpp:228] Iteration 2700, loss = 0.00842135
I0711 14:09:39.638900  5319 solver.cpp:244]     Train net output #0: loss = 0.00842137 (* 1 = 0.00842137 loss)
I0711 14:09:39.638911  5319 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0711 14:09:55.351819  5319 solver.cpp:228] Iteration 2800, loss = 0.00842348
I0711 14:09:55.352232  5319 solver.cpp:244]     Train net output #0: loss = 0.0084235 (* 1 = 0.0084235 loss)
I0711 14:09:55.352320  5319 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0711 14:10:11.013090  5319 solver.cpp:228] Iteration 2900, loss = 0.00648068
I0711 14:10:11.013273  5319 solver.cpp:244]     Train net output #0: loss = 0.00648069 (* 1 = 0.00648069 loss)
I0711 14:10:11.013310  5319 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0711 14:10:26.514647  5319 solver.cpp:337] Iteration 3000, Testing net (#0)
I0711 14:10:36.384598  5319 solver.cpp:404]     Test net output #0: loss = 0.0159838 (* 1 = 0.0159838 loss)
I0711 14:10:36.580639  5319 solver.cpp:228] Iteration 3000, loss = 0.00718506
I0711 14:10:36.580724  5319 solver.cpp:244]     Train net output #0: loss = 0.00718508 (* 1 = 0.00718508 loss)
I0711 14:10:36.580734  5319 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0711 14:10:52.276965  5319 solver.cpp:228] Iteration 3100, loss = 0.00405828
I0711 14:10:52.277030  5319 solver.cpp:244]     Train net output #0: loss = 0.0040583 (* 1 = 0.0040583 loss)
I0711 14:10:52.277042  5319 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0711 14:11:08.006449  5319 solver.cpp:228] Iteration 3200, loss = 0.00439959
I0711 14:11:08.006539  5319 solver.cpp:244]     Train net output #0: loss = 0.0043996 (* 1 = 0.0043996 loss)
I0711 14:11:08.006552  5319 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0711 14:11:23.734419  5319 solver.cpp:228] Iteration 3300, loss = 0.0178905
I0711 14:11:23.734479  5319 solver.cpp:244]     Train net output #0: loss = 0.0178905 (* 1 = 0.0178905 loss)
I0711 14:11:23.734490  5319 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0711 14:11:39.309288  5319 solver.cpp:228] Iteration 3400, loss = 0.00879809
I0711 14:11:39.309466  5319 solver.cpp:244]     Train net output #0: loss = 0.00879811 (* 1 = 0.00879811 loss)
I0711 14:11:39.309504  5319 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0711 14:11:54.739503  5319 solver.cpp:337] Iteration 3500, Testing net (#0)
I0711 14:12:04.531137  5319 solver.cpp:404]     Test net output #0: loss = 0.0150879 (* 1 = 0.0150879 loss)
I0711 14:12:04.714893  5319 solver.cpp:228] Iteration 3500, loss = 0.00646342
I0711 14:12:04.715032  5319 solver.cpp:244]     Train net output #0: loss = 0.00646344 (* 1 = 0.00646344 loss)
I0711 14:12:04.715060  5319 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0711 14:12:20.315529  5319 solver.cpp:228] Iteration 3600, loss = 0.0078583
I0711 14:12:20.315804  5319 solver.cpp:244]     Train net output #0: loss = 0.00785832 (* 1 = 0.00785832 loss)
I0711 14:12:20.315819  5319 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0711 14:12:36.039155  5319 solver.cpp:228] Iteration 3700, loss = 0.022641
I0711 14:12:36.039387  5319 solver.cpp:244]     Train net output #0: loss = 0.022641 (* 1 = 0.022641 loss)
I0711 14:12:36.039469  5319 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0711 14:12:51.682327  5319 solver.cpp:228] Iteration 3800, loss = 0.0122949
I0711 14:12:51.682579  5319 solver.cpp:244]     Train net output #0: loss = 0.0122949 (* 1 = 0.0122949 loss)
I0711 14:12:51.682631  5319 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0711 14:13:07.248360  5319 solver.cpp:228] Iteration 3900, loss = 0.0035432
I0711 14:13:07.248625  5319 solver.cpp:244]     Train net output #0: loss = 0.00354321 (* 1 = 0.00354321 loss)
I0711 14:13:07.248672  5319 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0711 14:13:22.635808  5319 solver.cpp:337] Iteration 4000, Testing net (#0)
I0711 14:13:32.378439  5319 solver.cpp:404]     Test net output #0: loss = 0.0149038 (* 1 = 0.0149038 loss)
I0711 14:13:32.578975  5319 solver.cpp:228] Iteration 4000, loss = 0.0218936
I0711 14:13:32.579146  5319 solver.cpp:244]     Train net output #0: loss = 0.0218936 (* 1 = 0.0218936 loss)
I0711 14:13:32.579186  5319 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0711 14:13:48.116606  5319 solver.cpp:228] Iteration 4100, loss = 0.0133079
I0711 14:13:48.116803  5319 solver.cpp:244]     Train net output #0: loss = 0.0133079 (* 1 = 0.0133079 loss)
I0711 14:13:48.116845  5319 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0711 14:14:03.798141  5319 solver.cpp:228] Iteration 4200, loss = 0.0147831
I0711 14:14:03.798377  5319 solver.cpp:244]     Train net output #0: loss = 0.0147831 (* 1 = 0.0147831 loss)
I0711 14:14:03.798424  5319 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0711 14:14:19.383682  5319 solver.cpp:228] Iteration 4300, loss = 0.0192576
I0711 14:14:19.383743  5319 solver.cpp:244]     Train net output #0: loss = 0.0192576 (* 1 = 0.0192576 loss)
I0711 14:14:19.383754  5319 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0711 14:14:34.994921  5319 solver.cpp:228] Iteration 4400, loss = 0.00922815
I0711 14:14:34.995108  5319 solver.cpp:244]     Train net output #0: loss = 0.00922817 (* 1 = 0.00922817 loss)
I0711 14:14:34.995122  5319 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0711 14:14:50.726186  5319 solver.cpp:337] Iteration 4500, Testing net (#0)
I0711 14:15:00.826490  5319 solver.cpp:404]     Test net output #0: loss = 0.0142458 (* 1 = 0.0142458 loss)
I0711 14:15:01.026439  5319 solver.cpp:228] Iteration 4500, loss = 0.00693109
I0711 14:15:01.026736  5319 solver.cpp:244]     Train net output #0: loss = 0.00693111 (* 1 = 0.00693111 loss)
I0711 14:15:01.026804  5319 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0711 14:15:16.741467  5319 solver.cpp:228] Iteration 4600, loss = 0.0116614
I0711 14:15:16.741749  5319 solver.cpp:244]     Train net output #0: loss = 0.0116614 (* 1 = 0.0116614 loss)
I0711 14:15:16.741787  5319 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0711 14:15:32.602187  5319 solver.cpp:228] Iteration 4700, loss = 0.00785404
I0711 14:15:32.602406  5319 solver.cpp:244]     Train net output #0: loss = 0.00785405 (* 1 = 0.00785405 loss)
I0711 14:15:32.602455  5319 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0711 14:15:48.453011  5319 solver.cpp:228] Iteration 4800, loss = 0.0141181
I0711 14:15:48.453553  5319 solver.cpp:244]     Train net output #0: loss = 0.0141181 (* 1 = 0.0141181 loss)
I0711 14:15:48.453609  5319 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0711 14:16:04.361577  5319 solver.cpp:228] Iteration 4900, loss = 0.00365242
I0711 14:16:04.361745  5319 solver.cpp:244]     Train net output #0: loss = 0.00365243 (* 1 = 0.00365243 loss)
I0711 14:16:04.361784  5319 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0711 14:16:20.068111  5319 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to8_iter_5000.caffemodel
I0711 14:16:20.077422  5319 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to8_iter_5000.solverstate
I0711 14:16:20.169584  5319 solver.cpp:317] Iteration 5000, loss = 0.0141703
I0711 14:16:20.169782  5319 solver.cpp:337] Iteration 5000, Testing net (#0)
I0711 14:16:30.174132  5319 solver.cpp:404]     Test net output #0: loss = 0.0143945 (* 1 = 0.0143945 loss)
I0711 14:16:30.174237  5319 solver.cpp:322] Optimization Done.
I0711 14:16:30.174257  5319 caffe.cpp:222] Optimization Done.
