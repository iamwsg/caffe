I0727 16:25:52.645480 11073 caffe.cpp:185] Using GPUs 0
I0727 16:25:52.770848 11073 caffe.cpp:190] GPU 0: GeForce GTX TITAN X
I0727 16:25:53.745435 11073 solver.cpp:48] Initializing solver from parameters: 
train_net: "examples/scene/matchNetTrainHingeSimple.prototxt"
test_net: "examples/scene/matchNetTestHingeSimple.prototxt"
test_iter: 10
test_interval: 10
base_lr: 0.0001
display: 10
max_iter: 5000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 2e-05
snapshot: 1000
snapshot_prefix: "examples/scene"
solver_mode: GPU
device_id: 0
I0727 16:25:53.745610 11073 solver.cpp:81] Creating training net from train_net file: examples/scene/matchNetTrainHingeSimple.prototxt
I0727 16:25:53.783809 11073 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.0039215689
    mirror: true
    mean_file: "examples/scene/scene_mean.binaryproto"
  }
  data_param {
    source: "examples/scene/train_pairs.lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "i1"
  type: "Slice"
  bottom: "data"
  top: "i1"
  top: "i2"
  slice_param {
    slice_dim: 1
    slice_point: 3
  }
}
layer {
  name: "p1"
  type: "Pooling"
  bottom: "i1"
  top: "p1"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "p2"
  type: "Pooling"
  bottom: "i2"
  top: "p2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "p1"
  top: "Convolution1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling2"
  top: "InnerProduct1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "p2"
  top: "Convolution3"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Pooling3"
  top: "Convolution4"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Pooling4"
  type: "Pooling"
  bottom: "Convolution4"
  top: "Pooling4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct3"
  type: "InnerProduct"
  bottom: "Pooling4"
  top: "InnerProduct3"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "InnerProduct3"
  top: "InnerProduct3"
}
layer {
  name: "InnerProduct4"
  type: "InnerProduct"
  bottom: "InnerProduct3"
  top: "InnerProduct4"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "InnerProduct2"
  bottom: "InnerProduct4"
  top: "Concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "InnerProduct5"
  type: "InnerProduct"
  bottom: "Concat1"
  top: "InnerProduct5"
  param {
    name: "fc1_w"
    lr_mult: 1
  }
  param {
    name: "fc1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "InnerProduct5"
  top: "InnerProduct5"
}
layer {
  name: "InnerProduct6"
  type: "InnerProduct"
  bottom: "InnerProduct5"
  top: "InnerProduct6"
  param {
    name: "fc2_w"
    lr_mult: 1
  }
  param {
    name: "fc2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "InnerProduct6"
  top: "InnerProduct6"
}
layer {
  name: "dt"
  type: "InnerProduct"
  bottom: "InnerProduct6"
  top: "dt"
  param {
    name: "fc3_w"
    lr_mult: 1
  }
  param {
    name: "fc3_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "dt"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TRAIN
  }
}
layer {
  name: "loss"
  type: "HingeLoss"
  bottom: "dt"
  bottom: "label"
  top: "loss"
}
I0727 16:25:53.784622 11073 layer_factory.hpp:77] Creating layer data
I0727 16:25:53.822691 11073 net.cpp:91] Creating Layer data
I0727 16:25:53.822724 11073 net.cpp:399] data -> data
I0727 16:25:53.822764 11073 net.cpp:399] data -> label
I0727 16:25:53.822784 11073 data_transformer.cpp:25] Loading mean file from: examples/scene/scene_mean.binaryproto
I0727 16:25:53.867630 11079 db_lmdb.cpp:35] Opened lmdb examples/scene/train_pairs.lmdb
I0727 16:25:54.183758 11073 data_layer.cpp:41] output data size: 128,6,128,128
I0727 16:25:54.316059 11073 net.cpp:141] Setting up data
I0727 16:25:54.316117 11073 net.cpp:148] Top shape: 128 6 128 128 (12582912)
I0727 16:25:54.316128 11073 net.cpp:148] Top shape: 128 (128)
I0727 16:25:54.316135 11073 net.cpp:156] Memory required for data: 50332160
I0727 16:25:54.316153 11073 layer_factory.hpp:77] Creating layer label_data_1_split
I0727 16:25:54.316176 11073 net.cpp:91] Creating Layer label_data_1_split
I0727 16:25:54.316186 11073 net.cpp:425] label_data_1_split <- label
I0727 16:25:54.316205 11073 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0727 16:25:54.316226 11073 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0727 16:25:54.316335 11073 net.cpp:141] Setting up label_data_1_split
I0727 16:25:54.316346 11073 net.cpp:148] Top shape: 128 (128)
I0727 16:25:54.316355 11073 net.cpp:148] Top shape: 128 (128)
I0727 16:25:54.316362 11073 net.cpp:156] Memory required for data: 50333184
I0727 16:25:54.316370 11073 layer_factory.hpp:77] Creating layer i1
I0727 16:25:54.319959 11073 net.cpp:91] Creating Layer i1
I0727 16:25:54.320036 11073 net.cpp:425] i1 <- data
I0727 16:25:54.320058 11073 net.cpp:399] i1 -> i1
I0727 16:25:54.320083 11073 net.cpp:399] i1 -> i2
I0727 16:25:54.325251 11073 net.cpp:141] Setting up i1
I0727 16:25:54.325289 11073 net.cpp:148] Top shape: 128 3 128 128 (6291456)
I0727 16:25:54.325299 11073 net.cpp:148] Top shape: 128 3 128 128 (6291456)
I0727 16:25:54.325305 11073 net.cpp:156] Memory required for data: 100664832
I0727 16:25:54.325315 11073 layer_factory.hpp:77] Creating layer p1
I0727 16:25:54.325389 11073 net.cpp:91] Creating Layer p1
I0727 16:25:54.325399 11073 net.cpp:425] p1 <- i1
I0727 16:25:54.325412 11073 net.cpp:399] p1 -> p1
I0727 16:25:54.325469 11073 net.cpp:141] Setting up p1
I0727 16:25:54.325479 11073 net.cpp:148] Top shape: 128 3 64 64 (1572864)
I0727 16:25:54.325486 11073 net.cpp:156] Memory required for data: 106956288
I0727 16:25:54.325494 11073 layer_factory.hpp:77] Creating layer p2
I0727 16:25:54.325506 11073 net.cpp:91] Creating Layer p2
I0727 16:25:54.325512 11073 net.cpp:425] p2 <- i2
I0727 16:25:54.325523 11073 net.cpp:399] p2 -> p2
I0727 16:25:54.325552 11073 net.cpp:141] Setting up p2
I0727 16:25:54.325562 11073 net.cpp:148] Top shape: 128 3 64 64 (1572864)
I0727 16:25:54.325569 11073 net.cpp:156] Memory required for data: 113247744
I0727 16:25:54.325577 11073 layer_factory.hpp:77] Creating layer Convolution1
I0727 16:25:54.325599 11073 net.cpp:91] Creating Layer Convolution1
I0727 16:25:54.325606 11073 net.cpp:425] Convolution1 <- p1
I0727 16:25:54.325618 11073 net.cpp:399] Convolution1 -> Convolution1
I0727 16:25:54.338102 11073 net.cpp:141] Setting up Convolution1
I0727 16:25:54.338141 11073 net.cpp:148] Top shape: 128 20 60 60 (9216000)
I0727 16:25:54.338150 11073 net.cpp:156] Memory required for data: 150111744
I0727 16:25:54.338173 11073 layer_factory.hpp:77] Creating layer Pooling1
I0727 16:25:54.338218 11073 net.cpp:91] Creating Layer Pooling1
I0727 16:25:54.338229 11073 net.cpp:425] Pooling1 <- Convolution1
I0727 16:25:54.338240 11073 net.cpp:399] Pooling1 -> Pooling1
I0727 16:25:54.338299 11073 net.cpp:141] Setting up Pooling1
I0727 16:25:54.338311 11073 net.cpp:148] Top shape: 128 20 30 30 (2304000)
I0727 16:25:54.338317 11073 net.cpp:156] Memory required for data: 159327744
I0727 16:25:54.338325 11073 layer_factory.hpp:77] Creating layer Convolution2
I0727 16:25:54.338340 11073 net.cpp:91] Creating Layer Convolution2
I0727 16:25:54.338346 11073 net.cpp:425] Convolution2 <- Pooling1
I0727 16:25:54.338359 11073 net.cpp:399] Convolution2 -> Convolution2
I0727 16:25:54.340322 11073 net.cpp:141] Setting up Convolution2
I0727 16:25:54.340345 11073 net.cpp:148] Top shape: 128 50 26 26 (4326400)
I0727 16:25:54.340353 11073 net.cpp:156] Memory required for data: 176633344
I0727 16:25:54.340368 11073 layer_factory.hpp:77] Creating layer Pooling2
I0727 16:25:54.340382 11073 net.cpp:91] Creating Layer Pooling2
I0727 16:25:54.340389 11073 net.cpp:425] Pooling2 <- Convolution2
I0727 16:25:54.340399 11073 net.cpp:399] Pooling2 -> Pooling2
I0727 16:25:54.340453 11073 net.cpp:141] Setting up Pooling2
I0727 16:25:54.340466 11073 net.cpp:148] Top shape: 128 50 13 13 (1081600)
I0727 16:25:54.340471 11073 net.cpp:156] Memory required for data: 180959744
I0727 16:25:54.340478 11073 layer_factory.hpp:77] Creating layer InnerProduct1
I0727 16:25:54.340490 11073 net.cpp:91] Creating Layer InnerProduct1
I0727 16:25:54.340497 11073 net.cpp:425] InnerProduct1 <- Pooling2
I0727 16:25:54.340507 11073 net.cpp:399] InnerProduct1 -> InnerProduct1
I0727 16:25:54.402698 11073 net.cpp:141] Setting up InnerProduct1
I0727 16:25:54.402737 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.402746 11073 net.cpp:156] Memory required for data: 181215744
I0727 16:25:54.402775 11073 layer_factory.hpp:77] Creating layer ReLU1
I0727 16:25:54.402792 11073 net.cpp:91] Creating Layer ReLU1
I0727 16:25:54.402802 11073 net.cpp:425] ReLU1 <- InnerProduct1
I0727 16:25:54.402817 11073 net.cpp:386] ReLU1 -> InnerProduct1 (in-place)
I0727 16:25:54.402833 11073 net.cpp:141] Setting up ReLU1
I0727 16:25:54.402881 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.402889 11073 net.cpp:156] Memory required for data: 181471744
I0727 16:25:54.402896 11073 layer_factory.hpp:77] Creating layer InnerProduct2
I0727 16:25:54.402961 11073 net.cpp:91] Creating Layer InnerProduct2
I0727 16:25:54.402971 11073 net.cpp:425] InnerProduct2 <- InnerProduct1
I0727 16:25:54.402984 11073 net.cpp:399] InnerProduct2 -> InnerProduct2
I0727 16:25:54.406047 11073 net.cpp:141] Setting up InnerProduct2
I0727 16:25:54.406075 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.406085 11073 net.cpp:156] Memory required for data: 181727744
I0727 16:25:54.406100 11073 layer_factory.hpp:77] Creating layer Convolution3
I0727 16:25:54.406121 11073 net.cpp:91] Creating Layer Convolution3
I0727 16:25:54.406129 11073 net.cpp:425] Convolution3 <- p2
I0727 16:25:54.406144 11073 net.cpp:399] Convolution3 -> Convolution3
I0727 16:25:54.406522 11073 net.cpp:141] Setting up Convolution3
I0727 16:25:54.406533 11073 net.cpp:148] Top shape: 128 20 60 60 (9216000)
I0727 16:25:54.406540 11073 net.cpp:156] Memory required for data: 218591744
I0727 16:25:54.406553 11073 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'Convolution1', param index 0
I0727 16:25:54.406561 11073 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'Convolution1', param index 1
I0727 16:25:54.406569 11073 layer_factory.hpp:77] Creating layer Pooling3
I0727 16:25:54.406579 11073 net.cpp:91] Creating Layer Pooling3
I0727 16:25:54.406586 11073 net.cpp:425] Pooling3 <- Convolution3
I0727 16:25:54.406596 11073 net.cpp:399] Pooling3 -> Pooling3
I0727 16:25:54.406646 11073 net.cpp:141] Setting up Pooling3
I0727 16:25:54.406658 11073 net.cpp:148] Top shape: 128 20 30 30 (2304000)
I0727 16:25:54.406666 11073 net.cpp:156] Memory required for data: 227807744
I0727 16:25:54.406672 11073 layer_factory.hpp:77] Creating layer Convolution4
I0727 16:25:54.406685 11073 net.cpp:91] Creating Layer Convolution4
I0727 16:25:54.406692 11073 net.cpp:425] Convolution4 <- Pooling3
I0727 16:25:54.406703 11073 net.cpp:399] Convolution4 -> Convolution4
I0727 16:25:54.407270 11073 net.cpp:141] Setting up Convolution4
I0727 16:25:54.407286 11073 net.cpp:148] Top shape: 128 50 26 26 (4326400)
I0727 16:25:54.407294 11073 net.cpp:156] Memory required for data: 245113344
I0727 16:25:54.407302 11073 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'Convolution2', param index 0
I0727 16:25:54.407312 11073 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'Convolution2', param index 1
I0727 16:25:54.407321 11073 layer_factory.hpp:77] Creating layer Pooling4
I0727 16:25:54.407332 11073 net.cpp:91] Creating Layer Pooling4
I0727 16:25:54.407341 11073 net.cpp:425] Pooling4 <- Convolution4
I0727 16:25:54.407352 11073 net.cpp:399] Pooling4 -> Pooling4
I0727 16:25:54.407407 11073 net.cpp:141] Setting up Pooling4
I0727 16:25:54.407419 11073 net.cpp:148] Top shape: 128 50 13 13 (1081600)
I0727 16:25:54.407428 11073 net.cpp:156] Memory required for data: 249439744
I0727 16:25:54.407435 11073 layer_factory.hpp:77] Creating layer InnerProduct3
I0727 16:25:54.407454 11073 net.cpp:91] Creating Layer InnerProduct3
I0727 16:25:54.407462 11073 net.cpp:425] InnerProduct3 <- Pooling4
I0727 16:25:54.407474 11073 net.cpp:399] InnerProduct3 -> InnerProduct3
I0727 16:25:54.454177 11073 net.cpp:141] Setting up InnerProduct3
I0727 16:25:54.454228 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.454237 11073 net.cpp:156] Memory required for data: 249695744
I0727 16:25:54.454252 11073 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'InnerProduct1', param index 0
I0727 16:25:54.454318 11073 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'InnerProduct1', param index 1
I0727 16:25:54.454330 11073 layer_factory.hpp:77] Creating layer ReLU2
I0727 16:25:54.454351 11073 net.cpp:91] Creating Layer ReLU2
I0727 16:25:54.454365 11073 net.cpp:425] ReLU2 <- InnerProduct3
I0727 16:25:54.454380 11073 net.cpp:386] ReLU2 -> InnerProduct3 (in-place)
I0727 16:25:54.454401 11073 net.cpp:141] Setting up ReLU2
I0727 16:25:54.454449 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.454457 11073 net.cpp:156] Memory required for data: 249951744
I0727 16:25:54.454466 11073 layer_factory.hpp:77] Creating layer InnerProduct4
I0727 16:25:54.454483 11073 net.cpp:91] Creating Layer InnerProduct4
I0727 16:25:54.454489 11073 net.cpp:425] InnerProduct4 <- InnerProduct3
I0727 16:25:54.454502 11073 net.cpp:399] InnerProduct4 -> InnerProduct4
I0727 16:25:54.457614 11073 net.cpp:141] Setting up InnerProduct4
I0727 16:25:54.457645 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.457653 11073 net.cpp:156] Memory required for data: 250207744
I0727 16:25:54.457666 11073 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'InnerProduct2', param index 0
I0727 16:25:54.457681 11073 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'InnerProduct2', param index 1
I0727 16:25:54.457697 11073 layer_factory.hpp:77] Creating layer Concat1
I0727 16:25:54.458402 11073 net.cpp:91] Creating Layer Concat1
I0727 16:25:54.458415 11073 net.cpp:425] Concat1 <- InnerProduct2
I0727 16:25:54.458425 11073 net.cpp:425] Concat1 <- InnerProduct4
I0727 16:25:54.458436 11073 net.cpp:399] Concat1 -> Concat1
I0727 16:25:54.458473 11073 net.cpp:141] Setting up Concat1
I0727 16:25:54.458482 11073 net.cpp:148] Top shape: 128 1000 (128000)
I0727 16:25:54.458489 11073 net.cpp:156] Memory required for data: 250719744
I0727 16:25:54.458497 11073 layer_factory.hpp:77] Creating layer InnerProduct5
I0727 16:25:54.458508 11073 net.cpp:91] Creating Layer InnerProduct5
I0727 16:25:54.458518 11073 net.cpp:425] InnerProduct5 <- Concat1
I0727 16:25:54.458528 11073 net.cpp:399] InnerProduct5 -> InnerProduct5
I0727 16:25:54.464499 11073 net.cpp:141] Setting up InnerProduct5
I0727 16:25:54.464536 11073 net.cpp:148] Top shape: 128 512 (65536)
I0727 16:25:54.464545 11073 net.cpp:156] Memory required for data: 250981888
I0727 16:25:54.464608 11073 layer_factory.hpp:77] Creating layer ReLU3
I0727 16:25:54.464637 11073 net.cpp:91] Creating Layer ReLU3
I0727 16:25:54.464646 11073 net.cpp:425] ReLU3 <- InnerProduct5
I0727 16:25:54.464658 11073 net.cpp:386] ReLU3 -> InnerProduct5 (in-place)
I0727 16:25:54.464673 11073 net.cpp:141] Setting up ReLU3
I0727 16:25:54.464681 11073 net.cpp:148] Top shape: 128 512 (65536)
I0727 16:25:54.464687 11073 net.cpp:156] Memory required for data: 251244032
I0727 16:25:54.464694 11073 layer_factory.hpp:77] Creating layer InnerProduct6
I0727 16:25:54.464707 11073 net.cpp:91] Creating Layer InnerProduct6
I0727 16:25:54.464715 11073 net.cpp:425] InnerProduct6 <- InnerProduct5
I0727 16:25:54.464725 11073 net.cpp:399] InnerProduct6 -> InnerProduct6
I0727 16:25:54.483708 11073 net.cpp:141] Setting up InnerProduct6
I0727 16:25:54.483750 11073 net.cpp:148] Top shape: 128 512 (65536)
I0727 16:25:54.483759 11073 net.cpp:156] Memory required for data: 251506176
I0727 16:25:54.483777 11073 layer_factory.hpp:77] Creating layer ReLU4
I0727 16:25:54.483793 11073 net.cpp:91] Creating Layer ReLU4
I0727 16:25:54.483800 11073 net.cpp:425] ReLU4 <- InnerProduct6
I0727 16:25:54.483811 11073 net.cpp:386] ReLU4 -> InnerProduct6 (in-place)
I0727 16:25:54.483825 11073 net.cpp:141] Setting up ReLU4
I0727 16:25:54.483834 11073 net.cpp:148] Top shape: 128 512 (65536)
I0727 16:25:54.483840 11073 net.cpp:156] Memory required for data: 251768320
I0727 16:25:54.483847 11073 layer_factory.hpp:77] Creating layer dt
I0727 16:25:54.483860 11073 net.cpp:91] Creating Layer dt
I0727 16:25:54.483866 11073 net.cpp:425] dt <- InnerProduct6
I0727 16:25:54.483878 11073 net.cpp:399] dt -> dt
I0727 16:25:54.484009 11073 net.cpp:141] Setting up dt
I0727 16:25:54.484019 11073 net.cpp:148] Top shape: 128 1 (128)
I0727 16:25:54.484025 11073 net.cpp:156] Memory required for data: 251768832
I0727 16:25:54.484076 11073 layer_factory.hpp:77] Creating layer dt_dt_0_split
I0727 16:25:54.484143 11073 net.cpp:91] Creating Layer dt_dt_0_split
I0727 16:25:54.484179 11073 net.cpp:425] dt_dt_0_split <- dt
I0727 16:25:54.484225 11073 net.cpp:399] dt_dt_0_split -> dt_dt_0_split_0
I0727 16:25:54.484279 11073 net.cpp:399] dt_dt_0_split -> dt_dt_0_split_1
I0727 16:25:54.484355 11073 net.cpp:141] Setting up dt_dt_0_split
I0727 16:25:54.484391 11073 net.cpp:148] Top shape: 128 1 (128)
I0727 16:25:54.484423 11073 net.cpp:148] Top shape: 128 1 (128)
I0727 16:25:54.484450 11073 net.cpp:156] Memory required for data: 251769856
I0727 16:25:54.484478 11073 layer_factory.hpp:77] Creating layer accuracy
I0727 16:25:54.484524 11073 net.cpp:91] Creating Layer accuracy
I0727 16:25:54.484555 11073 net.cpp:425] accuracy <- dt_dt_0_split_0
I0727 16:25:54.484586 11073 net.cpp:425] accuracy <- label_data_1_split_0
I0727 16:25:54.484618 11073 net.cpp:399] accuracy -> accuracy
I0727 16:25:54.484661 11073 net.cpp:141] Setting up accuracy
I0727 16:25:54.484694 11073 net.cpp:148] Top shape: (1)
I0727 16:25:54.484722 11073 net.cpp:156] Memory required for data: 251769860
I0727 16:25:54.484750 11073 layer_factory.hpp:77] Creating layer loss
I0727 16:25:54.484791 11073 net.cpp:91] Creating Layer loss
I0727 16:25:54.484824 11073 net.cpp:425] loss <- dt_dt_0_split_1
I0727 16:25:54.484853 11073 net.cpp:425] loss <- label_data_1_split_1
I0727 16:25:54.484886 11073 net.cpp:399] loss -> loss
I0727 16:25:54.484953 11073 net.cpp:141] Setting up loss
I0727 16:25:54.484987 11073 net.cpp:148] Top shape: (1)
I0727 16:25:54.485015 11073 net.cpp:151]     with loss weight 1
I0727 16:25:54.485065 11073 net.cpp:156] Memory required for data: 251769864
I0727 16:25:54.485093 11073 net.cpp:217] loss needs backward computation.
I0727 16:25:54.485126 11073 net.cpp:219] accuracy does not need backward computation.
I0727 16:25:54.485155 11073 net.cpp:217] dt_dt_0_split needs backward computation.
I0727 16:25:54.485185 11073 net.cpp:217] dt needs backward computation.
I0727 16:25:54.485213 11073 net.cpp:217] ReLU4 needs backward computation.
I0727 16:25:54.485246 11073 net.cpp:217] InnerProduct6 needs backward computation.
I0727 16:25:54.485275 11073 net.cpp:217] ReLU3 needs backward computation.
I0727 16:25:54.485303 11073 net.cpp:217] InnerProduct5 needs backward computation.
I0727 16:25:54.485335 11073 net.cpp:217] Concat1 needs backward computation.
I0727 16:25:54.485368 11073 net.cpp:217] InnerProduct4 needs backward computation.
I0727 16:25:54.485399 11073 net.cpp:217] ReLU2 needs backward computation.
I0727 16:25:54.485427 11073 net.cpp:217] InnerProduct3 needs backward computation.
I0727 16:25:54.485456 11073 net.cpp:217] Pooling4 needs backward computation.
I0727 16:25:54.485491 11073 net.cpp:217] Convolution4 needs backward computation.
I0727 16:25:54.485519 11073 net.cpp:217] Pooling3 needs backward computation.
I0727 16:25:54.485548 11073 net.cpp:217] Convolution3 needs backward computation.
I0727 16:25:54.485579 11073 net.cpp:217] InnerProduct2 needs backward computation.
I0727 16:25:54.485607 11073 net.cpp:217] ReLU1 needs backward computation.
I0727 16:25:54.485635 11073 net.cpp:217] InnerProduct1 needs backward computation.
I0727 16:25:54.485666 11073 net.cpp:217] Pooling2 needs backward computation.
I0727 16:25:54.485695 11073 net.cpp:217] Convolution2 needs backward computation.
I0727 16:25:54.485726 11073 net.cpp:217] Pooling1 needs backward computation.
I0727 16:25:54.485754 11073 net.cpp:217] Convolution1 needs backward computation.
I0727 16:25:54.485783 11073 net.cpp:219] p2 does not need backward computation.
I0727 16:25:54.485812 11073 net.cpp:219] p1 does not need backward computation.
I0727 16:25:54.485842 11073 net.cpp:219] i1 does not need backward computation.
I0727 16:25:54.485870 11073 net.cpp:219] label_data_1_split does not need backward computation.
I0727 16:25:54.485900 11073 net.cpp:219] data does not need backward computation.
I0727 16:25:54.485927 11073 net.cpp:261] This network produces output accuracy
I0727 16:25:54.485954 11073 net.cpp:261] This network produces output loss
I0727 16:25:54.490902 11073 net.cpp:274] Network initialization done.
I0727 16:25:54.492887 11073 solver.cpp:181] Creating test net (#0) specified by test_net file: examples/scene/matchNetTestHingeSimple.prototxt
I0727 16:25:54.493595 11073 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "data"
  type: "Data"
  top: "data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.0039215689
    mirror: true
    mean_file: "examples/scene/scene_mean.binaryproto"
  }
  data_param {
    source: "examples/scene/test_pairs.lmdb"
    batch_size: 128
    backend: LMDB
  }
}
layer {
  name: "i1"
  type: "Slice"
  bottom: "data"
  top: "i1"
  top: "i2"
  slice_param {
    slice_dim: 1
    slice_point: 3
  }
}
layer {
  name: "p1"
  type: "Pooling"
  bottom: "i1"
  top: "p1"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "p2"
  type: "Pooling"
  bottom: "i2"
  top: "p2"
  pooling_param {
    pool: AVE
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "p1"
  top: "Convolution1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Pooling1"
  type: "Pooling"
  bottom: "Convolution1"
  top: "Pooling1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "Pooling1"
  top: "Convolution2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Pooling2"
  type: "Pooling"
  bottom: "Convolution2"
  top: "Pooling2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling2"
  top: "InnerProduct1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "InnerProduct1"
  top: "InnerProduct1"
}
layer {
  name: "InnerProduct2"
  type: "InnerProduct"
  bottom: "InnerProduct1"
  top: "InnerProduct2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "p2"
  top: "Convolution3"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "Convolution3"
  top: "Pooling3"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "Pooling3"
  top: "Convolution4"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Pooling4"
  type: "Pooling"
  bottom: "Convolution4"
  top: "Pooling4"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "InnerProduct3"
  type: "InnerProduct"
  bottom: "Pooling4"
  top: "InnerProduct3"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "InnerProduct3"
  top: "InnerProduct3"
}
layer {
  name: "InnerProduct4"
  type: "InnerProduct"
  bottom: "InnerProduct3"
  top: "InnerProduct4"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "InnerProduct2"
  bottom: "InnerProduct4"
  top: "Concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "InnerProduct5"
  type: "InnerProduct"
  bottom: "Concat1"
  top: "InnerProduct5"
  param {
    name: "fc1_w"
    lr_mult: 1
  }
  param {
    name: "fc1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "InnerProduct5"
  top: "InnerProduct5"
}
layer {
  name: "InnerProduct6"
  type: "InnerProduct"
  bottom: "InnerProduct5"
  top: "InnerProduct6"
  param {
    name: "fc2_w"
    lr_mult: 1
  }
  param {
    name: "fc2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 512
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "InnerProduct6"
  top: "InnerProduct6"
}
layer {
  name: "dt"
  type: "InnerProduct"
  bottom: "InnerProduct6"
  top: "dt"
  param {
    name: "fc3_w"
    lr_mult: 1
  }
  param {
    name: "fc3_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "dt"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
layer {
  name: "loss"
  type: "HingeLoss"
  bottom: "dt"
  bottom: "label"
  top: "loss"
}
I0727 16:25:54.496104 11073 layer_factory.hpp:77] Creating layer data
I0727 16:25:54.496373 11073 net.cpp:91] Creating Layer data
I0727 16:25:54.496551 11073 net.cpp:399] data -> data
I0727 16:25:54.496592 11073 net.cpp:399] data -> label
I0727 16:25:54.496628 11073 data_transformer.cpp:25] Loading mean file from: examples/scene/scene_mean.binaryproto
I0727 16:25:54.542742 11080 blocking_queue.cpp:50] Waiting for data
I0727 16:25:54.551657 11081 db_lmdb.cpp:35] Opened lmdb examples/scene/test_pairs.lmdb
I0727 16:25:54.553200 11073 data_layer.cpp:41] output data size: 128,6,128,128
I0727 16:25:54.710942 11073 net.cpp:141] Setting up data
I0727 16:25:54.711105 11073 net.cpp:148] Top shape: 128 6 128 128 (12582912)
I0727 16:25:54.711144 11073 net.cpp:148] Top shape: 128 (128)
I0727 16:25:54.711174 11073 net.cpp:156] Memory required for data: 50332160
I0727 16:25:54.711206 11073 layer_factory.hpp:77] Creating layer label_data_1_split
I0727 16:25:54.711246 11073 net.cpp:91] Creating Layer label_data_1_split
I0727 16:25:54.711275 11073 net.cpp:425] label_data_1_split <- label
I0727 16:25:54.711308 11073 net.cpp:399] label_data_1_split -> label_data_1_split_0
I0727 16:25:54.711347 11073 net.cpp:399] label_data_1_split -> label_data_1_split_1
I0727 16:25:54.711443 11073 net.cpp:141] Setting up label_data_1_split
I0727 16:25:54.711482 11073 net.cpp:148] Top shape: 128 (128)
I0727 16:25:54.711513 11073 net.cpp:148] Top shape: 128 (128)
I0727 16:25:54.711539 11073 net.cpp:156] Memory required for data: 50333184
I0727 16:25:54.711566 11073 layer_factory.hpp:77] Creating layer i1
I0727 16:25:54.711601 11073 net.cpp:91] Creating Layer i1
I0727 16:25:54.711632 11073 net.cpp:425] i1 <- data
I0727 16:25:54.711664 11073 net.cpp:399] i1 -> i1
I0727 16:25:54.711700 11073 net.cpp:399] i1 -> i2
I0727 16:25:54.711791 11073 net.cpp:141] Setting up i1
I0727 16:25:54.711828 11073 net.cpp:148] Top shape: 128 3 128 128 (6291456)
I0727 16:25:54.711859 11073 net.cpp:148] Top shape: 128 3 128 128 (6291456)
I0727 16:25:54.711887 11073 net.cpp:156] Memory required for data: 100664832
I0727 16:25:54.711915 11073 layer_factory.hpp:77] Creating layer p1
I0727 16:25:54.711961 11073 net.cpp:91] Creating Layer p1
I0727 16:25:54.712013 11073 net.cpp:425] p1 <- i1
I0727 16:25:54.712050 11073 net.cpp:399] p1 -> p1
I0727 16:25:54.719563 11073 net.cpp:141] Setting up p1
I0727 16:25:54.719655 11073 net.cpp:148] Top shape: 128 3 64 64 (1572864)
I0727 16:25:54.719686 11073 net.cpp:156] Memory required for data: 106956288
I0727 16:25:54.719717 11073 layer_factory.hpp:77] Creating layer p2
I0727 16:25:54.719756 11073 net.cpp:91] Creating Layer p2
I0727 16:25:54.719790 11073 net.cpp:425] p2 <- i2
I0727 16:25:54.719825 11073 net.cpp:399] p2 -> p2
I0727 16:25:54.719902 11073 net.cpp:141] Setting up p2
I0727 16:25:54.719938 11073 net.cpp:148] Top shape: 128 3 64 64 (1572864)
I0727 16:25:54.719964 11073 net.cpp:156] Memory required for data: 113247744
I0727 16:25:54.719995 11073 layer_factory.hpp:77] Creating layer Convolution1
I0727 16:25:54.720036 11073 net.cpp:91] Creating Layer Convolution1
I0727 16:25:54.720067 11073 net.cpp:425] Convolution1 <- p1
I0727 16:25:54.720103 11073 net.cpp:399] Convolution1 -> Convolution1
I0727 16:25:54.720564 11073 net.cpp:141] Setting up Convolution1
I0727 16:25:54.720603 11073 net.cpp:148] Top shape: 128 20 60 60 (9216000)
I0727 16:25:54.720633 11073 net.cpp:156] Memory required for data: 150111744
I0727 16:25:54.720669 11073 layer_factory.hpp:77] Creating layer Pooling1
I0727 16:25:54.720705 11073 net.cpp:91] Creating Layer Pooling1
I0727 16:25:54.720733 11073 net.cpp:425] Pooling1 <- Convolution1
I0727 16:25:54.720765 11073 net.cpp:399] Pooling1 -> Pooling1
I0727 16:25:54.720850 11073 net.cpp:141] Setting up Pooling1
I0727 16:25:54.720885 11073 net.cpp:148] Top shape: 128 20 30 30 (2304000)
I0727 16:25:54.720912 11073 net.cpp:156] Memory required for data: 159327744
I0727 16:25:54.720939 11073 layer_factory.hpp:77] Creating layer Convolution2
I0727 16:25:54.720978 11073 net.cpp:91] Creating Layer Convolution2
I0727 16:25:54.721005 11073 net.cpp:425] Convolution2 <- Pooling1
I0727 16:25:54.721040 11073 net.cpp:399] Convolution2 -> Convolution2
I0727 16:25:54.721873 11073 net.cpp:141] Setting up Convolution2
I0727 16:25:54.722041 11073 net.cpp:148] Top shape: 128 50 26 26 (4326400)
I0727 16:25:54.722070 11073 net.cpp:156] Memory required for data: 176633344
I0727 16:25:54.722108 11073 layer_factory.hpp:77] Creating layer Pooling2
I0727 16:25:54.722143 11073 net.cpp:91] Creating Layer Pooling2
I0727 16:25:54.722172 11073 net.cpp:425] Pooling2 <- Convolution2
I0727 16:25:54.722205 11073 net.cpp:399] Pooling2 -> Pooling2
I0727 16:25:54.722292 11073 net.cpp:141] Setting up Pooling2
I0727 16:25:54.722324 11073 net.cpp:148] Top shape: 128 50 13 13 (1081600)
I0727 16:25:54.722352 11073 net.cpp:156] Memory required for data: 180959744
I0727 16:25:54.722378 11073 layer_factory.hpp:77] Creating layer InnerProduct1
I0727 16:25:54.722411 11073 net.cpp:91] Creating Layer InnerProduct1
I0727 16:25:54.722527 11073 net.cpp:425] InnerProduct1 <- Pooling2
I0727 16:25:54.722558 11073 net.cpp:399] InnerProduct1 -> InnerProduct1
I0727 16:25:54.780292 11073 net.cpp:141] Setting up InnerProduct1
I0727 16:25:54.780336 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.780344 11073 net.cpp:156] Memory required for data: 181215744
I0727 16:25:54.780374 11073 layer_factory.hpp:77] Creating layer ReLU1
I0727 16:25:54.780391 11073 net.cpp:91] Creating Layer ReLU1
I0727 16:25:54.780401 11073 net.cpp:425] ReLU1 <- InnerProduct1
I0727 16:25:54.780416 11073 net.cpp:386] ReLU1 -> InnerProduct1 (in-place)
I0727 16:25:54.780431 11073 net.cpp:141] Setting up ReLU1
I0727 16:25:54.780441 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.780449 11073 net.cpp:156] Memory required for data: 181471744
I0727 16:25:54.780457 11073 layer_factory.hpp:77] Creating layer InnerProduct2
I0727 16:25:54.780472 11073 net.cpp:91] Creating Layer InnerProduct2
I0727 16:25:54.780480 11073 net.cpp:425] InnerProduct2 <- InnerProduct1
I0727 16:25:54.780493 11073 net.cpp:399] InnerProduct2 -> InnerProduct2
I0727 16:25:54.784622 11073 net.cpp:141] Setting up InnerProduct2
I0727 16:25:54.784657 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.784693 11073 net.cpp:156] Memory required for data: 181727744
I0727 16:25:54.784709 11073 layer_factory.hpp:77] Creating layer Convolution3
I0727 16:25:54.784730 11073 net.cpp:91] Creating Layer Convolution3
I0727 16:25:54.784739 11073 net.cpp:425] Convolution3 <- p2
I0727 16:25:54.784751 11073 net.cpp:399] Convolution3 -> Convolution3
I0727 16:25:54.785235 11073 net.cpp:141] Setting up Convolution3
I0727 16:25:54.785246 11073 net.cpp:148] Top shape: 128 20 60 60 (9216000)
I0727 16:25:54.785253 11073 net.cpp:156] Memory required for data: 218591744
I0727 16:25:54.785264 11073 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'Convolution1', param index 0
I0727 16:25:54.785382 11073 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'Convolution1', param index 1
I0727 16:25:54.785390 11073 layer_factory.hpp:77] Creating layer Pooling3
I0727 16:25:54.785403 11073 net.cpp:91] Creating Layer Pooling3
I0727 16:25:54.785410 11073 net.cpp:425] Pooling3 <- Convolution3
I0727 16:25:54.785420 11073 net.cpp:399] Pooling3 -> Pooling3
I0727 16:25:54.785472 11073 net.cpp:141] Setting up Pooling3
I0727 16:25:54.785481 11073 net.cpp:148] Top shape: 128 20 30 30 (2304000)
I0727 16:25:54.785487 11073 net.cpp:156] Memory required for data: 227807744
I0727 16:25:54.785493 11073 layer_factory.hpp:77] Creating layer Convolution4
I0727 16:25:54.785507 11073 net.cpp:91] Creating Layer Convolution4
I0727 16:25:54.785513 11073 net.cpp:425] Convolution4 <- Pooling3
I0727 16:25:54.785524 11073 net.cpp:399] Convolution4 -> Convolution4
I0727 16:25:54.786260 11073 net.cpp:141] Setting up Convolution4
I0727 16:25:54.786276 11073 net.cpp:148] Top shape: 128 50 26 26 (4326400)
I0727 16:25:54.786283 11073 net.cpp:156] Memory required for data: 245113344
I0727 16:25:54.786294 11073 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'Convolution2', param index 0
I0727 16:25:54.786304 11073 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'Convolution2', param index 1
I0727 16:25:54.786432 11073 layer_factory.hpp:77] Creating layer Pooling4
I0727 16:25:54.786448 11073 net.cpp:91] Creating Layer Pooling4
I0727 16:25:54.786456 11073 net.cpp:425] Pooling4 <- Convolution4
I0727 16:25:54.786468 11073 net.cpp:399] Pooling4 -> Pooling4
I0727 16:25:54.786530 11073 net.cpp:141] Setting up Pooling4
I0727 16:25:54.786540 11073 net.cpp:148] Top shape: 128 50 13 13 (1081600)
I0727 16:25:54.786547 11073 net.cpp:156] Memory required for data: 249439744
I0727 16:25:54.786555 11073 layer_factory.hpp:77] Creating layer InnerProduct3
I0727 16:25:54.786581 11073 net.cpp:91] Creating Layer InnerProduct3
I0727 16:25:54.786588 11073 net.cpp:425] InnerProduct3 <- Pooling4
I0727 16:25:54.786602 11073 net.cpp:399] InnerProduct3 -> InnerProduct3
I0727 16:25:54.836822 11073 net.cpp:141] Setting up InnerProduct3
I0727 16:25:54.836875 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.836882 11073 net.cpp:156] Memory required for data: 249695744
I0727 16:25:54.836895 11073 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'InnerProduct1', param index 0
I0727 16:25:54.836905 11073 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'InnerProduct1', param index 1
I0727 16:25:54.836912 11073 layer_factory.hpp:77] Creating layer ReLU2
I0727 16:25:54.836928 11073 net.cpp:91] Creating Layer ReLU2
I0727 16:25:54.836936 11073 net.cpp:425] ReLU2 <- InnerProduct3
I0727 16:25:54.836948 11073 net.cpp:386] ReLU2 -> InnerProduct3 (in-place)
I0727 16:25:54.836963 11073 net.cpp:141] Setting up ReLU2
I0727 16:25:54.836971 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.836978 11073 net.cpp:156] Memory required for data: 249951744
I0727 16:25:54.836984 11073 layer_factory.hpp:77] Creating layer InnerProduct4
I0727 16:25:54.836997 11073 net.cpp:91] Creating Layer InnerProduct4
I0727 16:25:54.837003 11073 net.cpp:425] InnerProduct4 <- InnerProduct3
I0727 16:25:54.837014 11073 net.cpp:399] InnerProduct4 -> InnerProduct4
I0727 16:25:54.840337 11073 net.cpp:141] Setting up InnerProduct4
I0727 16:25:54.840366 11073 net.cpp:148] Top shape: 128 500 (64000)
I0727 16:25:54.840404 11073 net.cpp:156] Memory required for data: 250207744
I0727 16:25:54.840416 11073 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'InnerProduct2', param index 0
I0727 16:25:54.840425 11073 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'InnerProduct2', param index 1
I0727 16:25:54.840432 11073 layer_factory.hpp:77] Creating layer Concat1
I0727 16:25:54.840447 11073 net.cpp:91] Creating Layer Concat1
I0727 16:25:54.840456 11073 net.cpp:425] Concat1 <- InnerProduct2
I0727 16:25:54.840466 11073 net.cpp:425] Concat1 <- InnerProduct4
I0727 16:25:54.840476 11073 net.cpp:399] Concat1 -> Concat1
I0727 16:25:54.840507 11073 net.cpp:141] Setting up Concat1
I0727 16:25:54.840517 11073 net.cpp:148] Top shape: 128 1000 (128000)
I0727 16:25:54.840523 11073 net.cpp:156] Memory required for data: 250719744
I0727 16:25:54.840533 11073 layer_factory.hpp:77] Creating layer InnerProduct5
I0727 16:25:54.840548 11073 net.cpp:91] Creating Layer InnerProduct5
I0727 16:25:54.840554 11073 net.cpp:425] InnerProduct5 <- Concat1
I0727 16:25:54.840564 11073 net.cpp:399] InnerProduct5 -> InnerProduct5
I0727 16:25:54.846596 11073 net.cpp:141] Setting up InnerProduct5
I0727 16:25:54.846629 11073 net.cpp:148] Top shape: 128 512 (65536)
I0727 16:25:54.846637 11073 net.cpp:156] Memory required for data: 250981888
I0727 16:25:54.846664 11073 layer_factory.hpp:77] Creating layer ReLU3
I0727 16:25:54.846678 11073 net.cpp:91] Creating Layer ReLU3
I0727 16:25:54.846686 11073 net.cpp:425] ReLU3 <- InnerProduct5
I0727 16:25:54.846698 11073 net.cpp:386] ReLU3 -> InnerProduct5 (in-place)
I0727 16:25:54.846710 11073 net.cpp:141] Setting up ReLU3
I0727 16:25:54.846719 11073 net.cpp:148] Top shape: 128 512 (65536)
I0727 16:25:54.846724 11073 net.cpp:156] Memory required for data: 251244032
I0727 16:25:54.846731 11073 layer_factory.hpp:77] Creating layer InnerProduct6
I0727 16:25:54.846743 11073 net.cpp:91] Creating Layer InnerProduct6
I0727 16:25:54.846750 11073 net.cpp:425] InnerProduct6 <- InnerProduct5
I0727 16:25:54.846781 11073 net.cpp:399] InnerProduct6 -> InnerProduct6
I0727 16:25:54.852843 11073 net.cpp:141] Setting up InnerProduct6
I0727 16:25:54.852949 11073 net.cpp:148] Top shape: 128 512 (65536)
I0727 16:25:54.852980 11073 net.cpp:156] Memory required for data: 251506176
I0727 16:25:54.853025 11073 layer_factory.hpp:77] Creating layer ReLU4
I0727 16:25:54.853070 11073 net.cpp:91] Creating Layer ReLU4
I0727 16:25:54.853108 11073 net.cpp:425] ReLU4 <- InnerProduct6
I0727 16:25:54.853148 11073 net.cpp:386] ReLU4 -> InnerProduct6 (in-place)
I0727 16:25:54.853196 11073 net.cpp:141] Setting up ReLU4
I0727 16:25:54.853231 11073 net.cpp:148] Top shape: 128 512 (65536)
I0727 16:25:54.853456 11073 net.cpp:156] Memory required for data: 251768320
I0727 16:25:54.853492 11073 layer_factory.hpp:77] Creating layer dt
I0727 16:25:54.853530 11073 net.cpp:91] Creating Layer dt
I0727 16:25:54.853562 11073 net.cpp:425] dt <- InnerProduct6
I0727 16:25:54.853602 11073 net.cpp:399] dt -> dt
I0727 16:25:54.854105 11073 net.cpp:141] Setting up dt
I0727 16:25:54.854163 11073 net.cpp:148] Top shape: 128 1 (128)
I0727 16:25:54.854197 11073 net.cpp:156] Memory required for data: 251768832
I0727 16:25:54.854238 11073 layer_factory.hpp:77] Creating layer dt_dt_0_split
I0727 16:25:54.854280 11073 net.cpp:91] Creating Layer dt_dt_0_split
I0727 16:25:54.854312 11073 net.cpp:425] dt_dt_0_split <- dt
I0727 16:25:54.854348 11073 net.cpp:399] dt_dt_0_split -> dt_dt_0_split_0
I0727 16:25:54.854393 11073 net.cpp:399] dt_dt_0_split -> dt_dt_0_split_1
I0727 16:25:54.854485 11073 net.cpp:141] Setting up dt_dt_0_split
I0727 16:25:54.854521 11073 net.cpp:148] Top shape: 128 1 (128)
I0727 16:25:54.854549 11073 net.cpp:148] Top shape: 128 1 (128)
I0727 16:25:54.854573 11073 net.cpp:156] Memory required for data: 251769856
I0727 16:25:54.854598 11073 layer_factory.hpp:77] Creating layer accuracy
I0727 16:25:54.854626 11073 net.cpp:91] Creating Layer accuracy
I0727 16:25:54.854651 11073 net.cpp:425] accuracy <- dt_dt_0_split_0
I0727 16:25:54.854686 11073 net.cpp:425] accuracy <- label_data_1_split_0
I0727 16:25:54.854732 11073 net.cpp:399] accuracy -> accuracy
I0727 16:25:54.854768 11073 net.cpp:141] Setting up accuracy
I0727 16:25:54.854795 11073 net.cpp:148] Top shape: (1)
I0727 16:25:54.854820 11073 net.cpp:156] Memory required for data: 251769860
I0727 16:25:54.854845 11073 layer_factory.hpp:77] Creating layer loss
I0727 16:25:54.854874 11073 net.cpp:91] Creating Layer loss
I0727 16:25:54.854900 11073 net.cpp:425] loss <- dt_dt_0_split_1
I0727 16:25:54.854926 11073 net.cpp:425] loss <- label_data_1_split_1
I0727 16:25:54.854957 11073 net.cpp:399] loss -> loss
I0727 16:25:54.855013 11073 net.cpp:141] Setting up loss
I0727 16:25:54.855044 11073 net.cpp:148] Top shape: (1)
I0727 16:25:54.855078 11073 net.cpp:151]     with loss weight 1
I0727 16:25:54.855119 11073 net.cpp:156] Memory required for data: 251769864
I0727 16:25:54.855149 11073 net.cpp:217] loss needs backward computation.
I0727 16:25:54.855177 11073 net.cpp:219] accuracy does not need backward computation.
I0727 16:25:54.855206 11073 net.cpp:217] dt_dt_0_split needs backward computation.
I0727 16:25:54.855232 11073 net.cpp:217] dt needs backward computation.
I0727 16:25:54.855259 11073 net.cpp:217] ReLU4 needs backward computation.
I0727 16:25:54.855285 11073 net.cpp:217] InnerProduct6 needs backward computation.
I0727 16:25:54.855314 11073 net.cpp:217] ReLU3 needs backward computation.
I0727 16:25:54.855342 11073 net.cpp:217] InnerProduct5 needs backward computation.
I0727 16:25:54.855370 11073 net.cpp:217] Concat1 needs backward computation.
I0727 16:25:54.855396 11073 net.cpp:217] InnerProduct4 needs backward computation.
I0727 16:25:54.855423 11073 net.cpp:217] ReLU2 needs backward computation.
I0727 16:25:54.855449 11073 net.cpp:217] InnerProduct3 needs backward computation.
I0727 16:25:54.855475 11073 net.cpp:217] Pooling4 needs backward computation.
I0727 16:25:54.855501 11073 net.cpp:217] Convolution4 needs backward computation.
I0727 16:25:54.855528 11073 net.cpp:217] Pooling3 needs backward computation.
I0727 16:25:54.855553 11073 net.cpp:217] Convolution3 needs backward computation.
I0727 16:25:54.855579 11073 net.cpp:217] InnerProduct2 needs backward computation.
I0727 16:25:54.855607 11073 net.cpp:217] ReLU1 needs backward computation.
I0727 16:25:54.855633 11073 net.cpp:217] InnerProduct1 needs backward computation.
I0727 16:25:54.855660 11073 net.cpp:217] Pooling2 needs backward computation.
I0727 16:25:54.855685 11073 net.cpp:217] Convolution2 needs backward computation.
I0727 16:25:54.855711 11073 net.cpp:217] Pooling1 needs backward computation.
I0727 16:25:54.855738 11073 net.cpp:217] Convolution1 needs backward computation.
I0727 16:25:54.855765 11073 net.cpp:219] p2 does not need backward computation.
I0727 16:25:54.855792 11073 net.cpp:219] p1 does not need backward computation.
I0727 16:25:54.855824 11073 net.cpp:219] i1 does not need backward computation.
I0727 16:25:54.855850 11073 net.cpp:219] label_data_1_split does not need backward computation.
I0727 16:25:54.855877 11073 net.cpp:219] data does not need backward computation.
I0727 16:25:54.855901 11073 net.cpp:261] This network produces output accuracy
I0727 16:25:54.855924 11073 net.cpp:261] This network produces output loss
I0727 16:25:54.862900 11073 net.cpp:274] Network initialization done.
I0727 16:25:54.863435 11073 solver.cpp:60] Solver scaffolding done.
I0727 16:25:54.864722 11073 caffe.cpp:219] Starting Optimization
I0727 16:25:54.864894 11073 solver.cpp:279] Solving 
I0727 16:25:54.864923 11073 solver.cpp:280] Learning Rate Policy: inv
I0727 16:25:54.867234 11073 solver.cpp:337] Iteration 0, Testing net (#0)
I0727 16:25:55.806627 11073 solver.cpp:404]     Test net output #0: accuracy = 0.2
I0727 16:25:55.806674 11073 solver.cpp:404]     Test net output #1: loss = 0.97526 (* 1 = 0.97526 loss)
I0727 16:25:56.072069 11073 solver.cpp:228] Iteration 0, loss = 0.960479
I0727 16:25:56.072114 11073 solver.cpp:244]     Train net output #0: accuracy = 0.125
I0727 16:25:56.072134 11073 solver.cpp:244]     Train net output #1: loss = -0.960479 (* 1 = -0.960479 loss)
I0727 16:25:56.072183 11073 sgd_solver.cpp:106] Iteration 0, lr = 0.0001
I0727 16:25:57.512341 11073 solver.cpp:337] Iteration 10, Testing net (#0)
I0727 16:25:57.744647 11073 blocking_queue.cpp:50] Data layer prefetch queue empty
I0727 16:25:58.289839 11073 solver.cpp:404]     Test net output #0: accuracy = 0.202344
I0727 16:25:58.289997 11073 solver.cpp:404]     Test net output #1: loss = 0.96092 (* 1 = 0.96092 loss)
I0727 16:25:58.416050 11073 solver.cpp:228] Iteration 10, loss = 0.961233
I0727 16:25:58.416106 11073 solver.cpp:244]     Train net output #0: accuracy = 0.203125
I0727 16:25:58.416123 11073 solver.cpp:244]     Train net output #1: loss = -0.961233 (* 1 = -0.961233 loss)
I0727 16:25:58.416244 11073 sgd_solver.cpp:106] Iteration 10, lr = 9.99251e-05
I0727 16:25:59.808753 11073 solver.cpp:337] Iteration 20, Testing net (#0)
I0727 16:26:00.540225 11073 solver.cpp:404]     Test net output #0: accuracy = 0.203125
I0727 16:26:00.540371 11073 solver.cpp:404]     Test net output #1: loss = 0.944884 (* 1 = 0.944884 loss)
I0727 16:26:00.679009 11073 solver.cpp:228] Iteration 20, loss = 0.95141
I0727 16:26:00.679172 11073 solver.cpp:244]     Train net output #0: accuracy = 0.210938
I0727 16:26:00.679216 11073 solver.cpp:244]     Train net output #1: loss = 0.95141 (* 1 = 0.95141 loss)
I0727 16:26:00.679249 11073 sgd_solver.cpp:106] Iteration 20, lr = 9.98503e-05
I0727 16:26:02.066717 11073 solver.cpp:337] Iteration 30, Testing net (#0)
I0727 16:26:02.680528 11073 solver.cpp:404]     Test net output #0: accuracy = 0.196094
I0727 16:26:02.680585 11073 solver.cpp:404]     Test net output #1: loss = 0.921971 (* 1 = 0.921971 loss)
I0727 16:26:02.798960 11073 solver.cpp:228] Iteration 30, loss = 0.898111
I0727 16:26:02.799023 11073 solver.cpp:244]     Train net output #0: accuracy = 0.179688
I0727 16:26:02.799214 11073 solver.cpp:244]     Train net output #1: loss = 0.898111 (* 1 = 0.898111 loss)
I0727 16:26:02.799280 11073 sgd_solver.cpp:106] Iteration 30, lr = 9.97756e-05
I0727 16:26:04.156776 11073 solver.cpp:337] Iteration 40, Testing net (#0)
I0727 16:26:05.463984 11073 solver.cpp:404]     Test net output #0: accuracy = 0.194531
I0727 16:26:05.464239 11073 solver.cpp:404]     Test net output #1: loss = 0.895182 (* 1 = 0.895182 loss)
I0727 16:26:05.737193 11073 solver.cpp:228] Iteration 40, loss = 0.847425
I0727 16:26:05.737387 11073 solver.cpp:244]     Train net output #0: accuracy = 0.148438
I0727 16:26:05.737431 11073 solver.cpp:244]     Train net output #1: loss = -0.847425 (* 1 = -0.847425 loss)
I0727 16:26:05.737469 11073 sgd_solver.cpp:106] Iteration 40, lr = 9.97011e-05
I0727 16:26:07.057477 11073 solver.cpp:337] Iteration 50, Testing net (#0)
I0727 16:26:08.225113 11073 solver.cpp:404]     Test net output #0: accuracy = 0.20625
I0727 16:26:08.225242 11073 solver.cpp:404]     Test net output #1: loss = 0.875023 (* 1 = 0.875023 loss)
I0727 16:26:08.567665 11073 solver.cpp:228] Iteration 50, loss = 0.843468
I0727 16:26:08.567855 11073 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I0727 16:26:08.567929 11073 solver.cpp:244]     Train net output #1: loss = -0.843468 (* 1 = -0.843468 loss)
I0727 16:26:08.567993 11073 sgd_solver.cpp:106] Iteration 50, lr = 9.96266e-05
I0727 16:26:09.988713 11073 solver.cpp:337] Iteration 60, Testing net (#0)
I0727 16:26:11.121593 11073 solver.cpp:404]     Test net output #0: accuracy = 0.201562
I0727 16:26:11.121809 11073 solver.cpp:404]     Test net output #1: loss = 0.851101 (* 1 = 0.851101 loss)
I0727 16:26:11.475100 11073 solver.cpp:228] Iteration 60, loss = 0.848207
I0727 16:26:11.475165 11073 solver.cpp:244]     Train net output #0: accuracy = 0.1875
I0727 16:26:11.475186 11073 solver.cpp:244]     Train net output #1: loss = 0.848207 (* 1 = 0.848207 loss)
I0727 16:26:11.475204 11073 sgd_solver.cpp:106] Iteration 60, lr = 9.95523e-05
I0727 16:26:13.612601 11073 solver.cpp:337] Iteration 70, Testing net (#0)
I0727 16:26:14.460402 11073 solver.cpp:404]     Test net output #0: accuracy = 0.195312
I0727 16:26:14.460517 11073 solver.cpp:404]     Test net output #1: loss = 0.826127 (* 1 = 0.826127 loss)
I0727 16:26:14.602563 11073 solver.cpp:228] Iteration 70, loss = 0.761472
I0727 16:26:14.602648 11073 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I0727 16:26:14.602676 11073 solver.cpp:244]     Train net output #1: loss = -0.761472 (* 1 = -0.761472 loss)
I0727 16:26:14.602699 11073 sgd_solver.cpp:106] Iteration 70, lr = 9.94782e-05
I0727 16:26:16.006005 11073 solver.cpp:337] Iteration 80, Testing net (#0)
I0727 16:26:17.042860 11073 solver.cpp:404]     Test net output #0: accuracy = 0.203125
I0727 16:26:17.042948 11073 solver.cpp:404]     Test net output #1: loss = 0.779992 (* 1 = 0.779992 loss)
I0727 16:26:17.157569 11073 solver.cpp:228] Iteration 80, loss = 0.849605
I0727 16:26:17.157645 11073 solver.cpp:244]     Train net output #0: accuracy = 0.234375
I0727 16:26:17.157680 11073 solver.cpp:244]     Train net output #1: loss = 0.849605 (* 1 = 0.849605 loss)
I0727 16:26:17.157706 11073 sgd_solver.cpp:106] Iteration 80, lr = 9.94042e-05
I0727 16:26:18.511554 11073 solver.cpp:337] Iteration 90, Testing net (#0)
I0727 16:26:19.539402 11073 solver.cpp:404]     Test net output #0: accuracy = 0.20625
I0727 16:26:19.539636 11073 solver.cpp:404]     Test net output #1: loss = 0.766558 (* 1 = 0.766558 loss)
I0727 16:26:19.687786 11073 solver.cpp:228] Iteration 90, loss = 0.630457
I0727 16:26:19.688107 11073 solver.cpp:244]     Train net output #0: accuracy = 0.148438
I0727 16:26:19.688336 11073 solver.cpp:244]     Train net output #1: loss = -0.630457 (* 1 = -0.630457 loss)
I0727 16:26:19.688527 11073 sgd_solver.cpp:106] Iteration 90, lr = 9.93303e-05
I0727 16:26:20.993247 11073 solver.cpp:337] Iteration 100, Testing net (#0)
I0727 16:26:21.675496 11073 solver.cpp:404]     Test net output #0: accuracy = 0.191406
I0727 16:26:21.675631 11073 solver.cpp:404]     Test net output #1: loss = 0.704982 (* 1 = 0.704982 loss)
I0727 16:26:21.791849 11073 solver.cpp:228] Iteration 100, loss = 0.702254
I0727 16:26:21.791976 11073 solver.cpp:244]     Train net output #0: accuracy = 0.1875
I0727 16:26:21.792037 11073 solver.cpp:244]     Train net output #1: loss = 0.702254 (* 1 = 0.702254 loss)
I0727 16:26:21.792090 11073 sgd_solver.cpp:106] Iteration 100, lr = 9.92565e-05
I0727 16:26:23.195719 11073 solver.cpp:337] Iteration 110, Testing net (#0)
I0727 16:26:23.774047 11073 solver.cpp:404]     Test net output #0: accuracy = 0.197656
I0727 16:26:23.774102 11073 solver.cpp:404]     Test net output #1: loss = 0.689956 (* 1 = 0.689956 loss)
I0727 16:26:23.931066 11073 solver.cpp:228] Iteration 110, loss = 0.632258
I0727 16:26:23.931310 11073 solver.cpp:244]     Train net output #0: accuracy = 0.234375
I0727 16:26:23.931411 11073 solver.cpp:244]     Train net output #1: loss = 0.632258 (* 1 = 0.632258 loss)
I0727 16:26:23.931499 11073 sgd_solver.cpp:106] Iteration 110, lr = 9.91829e-05
I0727 16:26:25.482460 11073 solver.cpp:337] Iteration 120, Testing net (#0)
I0727 16:26:26.072402 11073 solver.cpp:404]     Test net output #0: accuracy = 0.207031
I0727 16:26:26.072466 11073 solver.cpp:404]     Test net output #1: loss = 0.696398 (* 1 = 0.696398 loss)
I0727 16:26:26.226562 11073 solver.cpp:228] Iteration 120, loss = 0.64766
I0727 16:26:26.226702 11073 solver.cpp:244]     Train net output #0: accuracy = 0.164062
I0727 16:26:26.226748 11073 solver.cpp:244]     Train net output #1: loss = -0.64766 (* 1 = -0.64766 loss)
I0727 16:26:26.226799 11073 sgd_solver.cpp:106] Iteration 120, lr = 9.91093e-05
I0727 16:26:27.625562 11073 solver.cpp:337] Iteration 130, Testing net (#0)
I0727 16:26:28.329475 11073 solver.cpp:404]     Test net output #0: accuracy = 0.2
I0727 16:26:28.329566 11073 solver.cpp:404]     Test net output #1: loss = 0.678674 (* 1 = 0.678674 loss)
I0727 16:26:28.497628 11073 solver.cpp:228] Iteration 130, loss = 0.680642
I0727 16:26:28.497704 11073 solver.cpp:244]     Train net output #0: accuracy = 0.242188
I0727 16:26:28.497743 11073 solver.cpp:244]     Train net output #1: loss = -0.680642 (* 1 = -0.680642 loss)
I0727 16:26:28.497781 11073 sgd_solver.cpp:106] Iteration 130, lr = 9.9036e-05
I0727 16:26:30.011013 11073 solver.cpp:337] Iteration 140, Testing net (#0)
I0727 16:26:31.269171 11073 solver.cpp:404]     Test net output #0: accuracy = 0.196094
I0727 16:26:31.269222 11073 solver.cpp:404]     Test net output #1: loss = 0.699743 (* 1 = 0.699743 loss)
I0727 16:26:31.550938 11073 solver.cpp:228] Iteration 140, loss = 0.578533
I0727 16:26:31.550981 11073 solver.cpp:244]     Train net output #0: accuracy = 0.164062
I0727 16:26:31.550998 11073 solver.cpp:244]     Train net output #1: loss = -0.578533 (* 1 = -0.578533 loss)
I0727 16:26:31.551013 11073 sgd_solver.cpp:106] Iteration 140, lr = 9.89627e-05
I0727 16:26:33.466454 11073 solver.cpp:337] Iteration 150, Testing net (#0)
I0727 16:26:34.488122 11073 solver.cpp:404]     Test net output #0: accuracy = 0.202344
I0727 16:26:34.488234 11073 solver.cpp:404]     Test net output #1: loss = 0.670618 (* 1 = 0.670618 loss)
I0727 16:26:34.618988 11073 solver.cpp:228] Iteration 150, loss = 0.622116
I0727 16:26:34.619048 11073 solver.cpp:244]     Train net output #0: accuracy = 0.1875
I0727 16:26:34.619079 11073 solver.cpp:244]     Train net output #1: loss = 0.622116 (* 1 = 0.622116 loss)
I0727 16:26:34.619096 11073 sgd_solver.cpp:106] Iteration 150, lr = 9.88896e-05
I0727 16:26:35.992930 11073 solver.cpp:337] Iteration 160, Testing net (#0)
I0727 16:26:36.895792 11073 solver.cpp:404]     Test net output #0: accuracy = 0.203125
I0727 16:26:36.895876 11073 solver.cpp:404]     Test net output #1: loss = 0.693833 (* 1 = 0.693833 loss)
I0727 16:26:37.060576 11073 solver.cpp:228] Iteration 160, loss = 0.591373
I0727 16:26:37.060653 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:26:37.060693 11073 solver.cpp:244]     Train net output #1: loss = -0.591373 (* 1 = -0.591373 loss)
I0727 16:26:37.060716 11073 sgd_solver.cpp:106] Iteration 160, lr = 9.88166e-05
I0727 16:26:38.466250 11073 solver.cpp:337] Iteration 170, Testing net (#0)
I0727 16:26:39.476610 11073 solver.cpp:404]     Test net output #0: accuracy = 0.194531
I0727 16:26:39.476694 11073 solver.cpp:404]     Test net output #1: loss = 0.662174 (* 1 = 0.662174 loss)
I0727 16:26:39.637964 11073 solver.cpp:228] Iteration 170, loss = 0.632916
I0727 16:26:39.638053 11073 solver.cpp:244]     Train net output #0: accuracy = 0.195312
I0727 16:26:39.638072 11073 solver.cpp:244]     Train net output #1: loss = -0.632916 (* 1 = -0.632916 loss)
I0727 16:26:39.638134 11073 sgd_solver.cpp:106] Iteration 170, lr = 9.87437e-05
I0727 16:26:40.984776 11073 solver.cpp:337] Iteration 180, Testing net (#0)
I0727 16:26:41.743326 11073 solver.cpp:404]     Test net output #0: accuracy = 0.198437
I0727 16:26:41.743386 11073 solver.cpp:404]     Test net output #1: loss = 0.698067 (* 1 = 0.698067 loss)
I0727 16:26:41.890286 11073 solver.cpp:228] Iteration 180, loss = 0.645694
I0727 16:26:41.890437 11073 solver.cpp:244]     Train net output #0: accuracy = 0.179688
I0727 16:26:41.890486 11073 solver.cpp:244]     Train net output #1: loss = -0.645694 (* 1 = -0.645694 loss)
I0727 16:26:41.890535 11073 sgd_solver.cpp:106] Iteration 180, lr = 9.86709e-05
I0727 16:26:43.248239 11073 solver.cpp:337] Iteration 190, Testing net (#0)
I0727 16:26:44.001135 11073 solver.cpp:404]     Test net output #0: accuracy = 0.204688
I0727 16:26:44.001253 11073 solver.cpp:404]     Test net output #1: loss = 0.685478 (* 1 = 0.685478 loss)
I0727 16:26:44.176167 11073 solver.cpp:228] Iteration 190, loss = 0.66569
I0727 16:26:44.176240 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:26:44.176275 11073 solver.cpp:244]     Train net output #1: loss = -0.66569 (* 1 = -0.66569 loss)
I0727 16:26:44.176414 11073 sgd_solver.cpp:106] Iteration 190, lr = 9.85983e-05
I0727 16:26:45.764746 11073 solver.cpp:337] Iteration 200, Testing net (#0)
I0727 16:26:46.841609 11073 solver.cpp:404]     Test net output #0: accuracy = 0.2
I0727 16:26:46.841773 11073 solver.cpp:404]     Test net output #1: loss = 0.675425 (* 1 = 0.675425 loss)
I0727 16:26:47.116544 11073 solver.cpp:228] Iteration 200, loss = 0.571215
I0727 16:26:47.116693 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:26:47.116793 11073 solver.cpp:244]     Train net output #1: loss = 0.571215 (* 1 = 0.571215 loss)
I0727 16:26:47.116894 11073 sgd_solver.cpp:106] Iteration 200, lr = 9.85258e-05
I0727 16:26:48.572825 11073 solver.cpp:337] Iteration 210, Testing net (#0)
I0727 16:26:49.164041 11073 solver.cpp:404]     Test net output #0: accuracy = 0.197656
I0727 16:26:49.164125 11073 solver.cpp:404]     Test net output #1: loss = 0.687729 (* 1 = 0.687729 loss)
I0727 16:26:49.287062 11073 solver.cpp:228] Iteration 210, loss = 0.592169
I0727 16:26:49.287142 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:26:49.287176 11073 solver.cpp:244]     Train net output #1: loss = 0.592169 (* 1 = 0.592169 loss)
I0727 16:26:49.287204 11073 sgd_solver.cpp:106] Iteration 210, lr = 9.84534e-05
I0727 16:26:53.440701 11073 solver.cpp:337] Iteration 220, Testing net (#0)
I0727 16:26:54.376016 11073 solver.cpp:404]     Test net output #0: accuracy = 0.197656
I0727 16:26:54.376215 11073 solver.cpp:404]     Test net output #1: loss = 0.665313 (* 1 = 0.665313 loss)
I0727 16:26:54.505055 11073 solver.cpp:228] Iteration 220, loss = 0.881196
I0727 16:26:54.505158 11073 solver.cpp:244]     Train net output #0: accuracy = 0.25
I0727 16:26:54.505179 11073 solver.cpp:244]     Train net output #1: loss = 0.881196 (* 1 = 0.881196 loss)
I0727 16:26:54.505198 11073 sgd_solver.cpp:106] Iteration 220, lr = 9.83811e-05
I0727 16:27:07.277304 11073 solver.cpp:337] Iteration 230, Testing net (#0)
I0727 16:27:08.248777 11073 solver.cpp:404]     Test net output #0: accuracy = 0.204688
I0727 16:27:08.248878 11073 solver.cpp:404]     Test net output #1: loss = 0.688512 (* 1 = 0.688512 loss)
I0727 16:27:08.407351 11073 solver.cpp:228] Iteration 230, loss = 0.676899
I0727 16:27:08.407439 11073 solver.cpp:244]     Train net output #0: accuracy = 0.1875
I0727 16:27:08.407480 11073 solver.cpp:244]     Train net output #1: loss = -0.676899 (* 1 = -0.676899 loss)
I0727 16:27:08.407512 11073 sgd_solver.cpp:106] Iteration 230, lr = 9.8309e-05
I0727 16:27:10.780834 11073 solver.cpp:337] Iteration 240, Testing net (#0)
I0727 16:27:11.377378 11073 solver.cpp:404]     Test net output #0: accuracy = 0.196094
I0727 16:27:11.377495 11073 solver.cpp:404]     Test net output #1: loss = 0.663569 (* 1 = 0.663569 loss)
I0727 16:27:11.500257 11073 solver.cpp:228] Iteration 240, loss = 0.541984
I0727 16:27:11.500342 11073 solver.cpp:244]     Train net output #0: accuracy = 0.164062
I0727 16:27:11.500377 11073 solver.cpp:244]     Train net output #1: loss = -0.541984 (* 1 = -0.541984 loss)
I0727 16:27:11.500406 11073 sgd_solver.cpp:106] Iteration 240, lr = 9.8237e-05
I0727 16:27:12.864521 11073 solver.cpp:337] Iteration 250, Testing net (#0)
I0727 16:27:13.812290 11073 solver.cpp:404]     Test net output #0: accuracy = 0.2
I0727 16:27:13.812525 11073 solver.cpp:404]     Test net output #1: loss = 0.701653 (* 1 = 0.701653 loss)
I0727 16:27:13.954514 11073 solver.cpp:228] Iteration 250, loss = 0.696669
I0727 16:27:13.954694 11073 solver.cpp:244]     Train net output #0: accuracy = 0.203125
I0727 16:27:13.954758 11073 solver.cpp:244]     Train net output #1: loss = -0.696669 (* 1 = -0.696669 loss)
I0727 16:27:13.954898 11073 sgd_solver.cpp:106] Iteration 250, lr = 9.81651e-05
I0727 16:27:15.302788 11073 solver.cpp:337] Iteration 260, Testing net (#0)
I0727 16:27:15.874266 11073 solver.cpp:404]     Test net output #0: accuracy = 0.202344
I0727 16:27:15.874331 11073 solver.cpp:404]     Test net output #1: loss = 0.671496 (* 1 = 0.671496 loss)
I0727 16:27:15.990414 11073 solver.cpp:228] Iteration 260, loss = 0.500426
I0727 16:27:15.990488 11073 solver.cpp:244]     Train net output #0: accuracy = 0.148438
I0727 16:27:15.990510 11073 solver.cpp:244]     Train net output #1: loss = -0.500426 (* 1 = -0.500426 loss)
I0727 16:27:15.990527 11073 sgd_solver.cpp:106] Iteration 260, lr = 9.80933e-05
I0727 16:27:17.337395 11073 solver.cpp:337] Iteration 270, Testing net (#0)
I0727 16:27:18.267992 11073 solver.cpp:404]     Test net output #0: accuracy = 0.203125
I0727 16:27:18.268113 11073 solver.cpp:404]     Test net output #1: loss = 0.689266 (* 1 = 0.689266 loss)
I0727 16:27:18.496211 11073 solver.cpp:228] Iteration 270, loss = 0.59608
I0727 16:27:18.496309 11073 solver.cpp:244]     Train net output #0: accuracy = 0.164062
I0727 16:27:18.496346 11073 solver.cpp:244]     Train net output #1: loss = -0.59608 (* 1 = -0.59608 loss)
I0727 16:27:18.496383 11073 sgd_solver.cpp:106] Iteration 270, lr = 9.80217e-05
I0727 16:27:20.268790 11073 solver.cpp:337] Iteration 280, Testing net (#0)
I0727 16:27:21.054103 11073 solver.cpp:404]     Test net output #0: accuracy = 0.196094
I0727 16:27:21.054486 11073 solver.cpp:404]     Test net output #1: loss = 0.676287 (* 1 = 0.676287 loss)
I0727 16:27:21.178093 11073 solver.cpp:228] Iteration 280, loss = 0.502383
I0727 16:27:21.178174 11073 solver.cpp:244]     Train net output #0: accuracy = 0.125
I0727 16:27:21.178208 11073 solver.cpp:244]     Train net output #1: loss = -0.502383 (* 1 = -0.502383 loss)
I0727 16:27:21.178297 11073 sgd_solver.cpp:106] Iteration 280, lr = 9.79502e-05
I0727 16:27:22.562013 11073 solver.cpp:337] Iteration 290, Testing net (#0)
I0727 16:27:23.539391 11073 solver.cpp:404]     Test net output #0: accuracy = 0.194531
I0727 16:27:23.539634 11073 solver.cpp:404]     Test net output #1: loss = 0.669507 (* 1 = 0.669507 loss)
I0727 16:27:23.718951 11073 solver.cpp:228] Iteration 290, loss = 0.606545
I0727 16:27:23.719240 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:27:23.719400 11073 solver.cpp:244]     Train net output #1: loss = -0.606545 (* 1 = -0.606545 loss)
I0727 16:27:23.719537 11073 sgd_solver.cpp:106] Iteration 290, lr = 9.78788e-05
I0727 16:27:25.155115 11073 solver.cpp:337] Iteration 300, Testing net (#0)
I0727 16:27:26.169064 11073 solver.cpp:404]     Test net output #0: accuracy = 0.20625
I0727 16:27:26.169360 11073 solver.cpp:404]     Test net output #1: loss = 0.683595 (* 1 = 0.683595 loss)
I0727 16:27:26.400389 11073 solver.cpp:228] Iteration 300, loss = 0.669383
I0727 16:27:26.400480 11073 solver.cpp:244]     Train net output #0: accuracy = 0.195312
I0727 16:27:26.400509 11073 solver.cpp:244]     Train net output #1: loss = -0.669383 (* 1 = -0.669383 loss)
I0727 16:27:26.400534 11073 sgd_solver.cpp:106] Iteration 300, lr = 9.78075e-05
I0727 16:27:28.623641 11073 solver.cpp:337] Iteration 310, Testing net (#0)
I0727 16:27:29.444417 11073 solver.cpp:404]     Test net output #0: accuracy = 0.201562
I0727 16:27:29.444532 11073 solver.cpp:404]     Test net output #1: loss = 0.682647 (* 1 = 0.682647 loss)
I0727 16:27:29.584640 11073 solver.cpp:228] Iteration 310, loss = 0.668827
I0727 16:27:29.584722 11073 solver.cpp:244]     Train net output #0: accuracy = 0.179688
I0727 16:27:29.584750 11073 solver.cpp:244]     Train net output #1: loss = -0.668827 (* 1 = -0.668827 loss)
I0727 16:27:29.584774 11073 sgd_solver.cpp:106] Iteration 310, lr = 9.77363e-05
I0727 16:27:30.955204 11073 solver.cpp:337] Iteration 320, Testing net (#0)
I0727 16:27:31.880461 11073 solver.cpp:404]     Test net output #0: accuracy = 0.195312
I0727 16:27:31.880547 11073 solver.cpp:404]     Test net output #1: loss = 0.684647 (* 1 = 0.684647 loss)
I0727 16:27:31.997488 11073 solver.cpp:228] Iteration 320, loss = 0.67028
I0727 16:27:31.997570 11073 solver.cpp:244]     Train net output #0: accuracy = 0.21875
I0727 16:27:31.997603 11073 solver.cpp:244]     Train net output #1: loss = -0.67028 (* 1 = -0.67028 loss)
I0727 16:27:31.997632 11073 sgd_solver.cpp:106] Iteration 320, lr = 9.76653e-05
I0727 16:27:33.391650 11073 solver.cpp:337] Iteration 330, Testing net (#0)
I0727 16:27:34.112107 11073 solver.cpp:404]     Test net output #0: accuracy = 0.203125
I0727 16:27:34.112205 11073 solver.cpp:404]     Test net output #1: loss = 0.665628 (* 1 = 0.665628 loss)
I0727 16:27:34.250710 11073 solver.cpp:228] Iteration 330, loss = 0.599404
I0727 16:27:34.250900 11073 solver.cpp:244]     Train net output #0: accuracy = 0.148438
I0727 16:27:34.250937 11073 solver.cpp:244]     Train net output #1: loss = 0.599404 (* 1 = 0.599404 loss)
I0727 16:27:34.250967 11073 sgd_solver.cpp:106] Iteration 330, lr = 9.75944e-05
I0727 16:27:35.605011 11073 solver.cpp:337] Iteration 340, Testing net (#0)
I0727 16:27:36.392349 11073 solver.cpp:404]     Test net output #0: accuracy = 0.20625
I0727 16:27:36.392444 11073 solver.cpp:404]     Test net output #1: loss = 0.693368 (* 1 = 0.693368 loss)
I0727 16:27:36.529384 11073 solver.cpp:228] Iteration 340, loss = 0.547206
I0727 16:27:36.529479 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:27:36.529517 11073 solver.cpp:244]     Train net output #1: loss = -0.547206 (* 1 = -0.547206 loss)
I0727 16:27:36.529547 11073 sgd_solver.cpp:106] Iteration 340, lr = 9.75236e-05
I0727 16:27:37.940078 11073 solver.cpp:337] Iteration 350, Testing net (#0)
I0727 16:27:38.839262 11073 solver.cpp:404]     Test net output #0: accuracy = 0.191406
I0727 16:27:38.839354 11073 solver.cpp:404]     Test net output #1: loss = 0.661185 (* 1 = 0.661185 loss)
I0727 16:27:39.014072 11073 solver.cpp:228] Iteration 350, loss = 0.6561
I0727 16:27:39.015473 11073 solver.cpp:244]     Train net output #0: accuracy = 0.1875
I0727 16:27:39.015702 11073 solver.cpp:244]     Train net output #1: loss = -0.6561 (* 1 = -0.6561 loss)
I0727 16:27:39.015964 11073 sgd_solver.cpp:106] Iteration 350, lr = 9.74529e-05
I0727 16:27:40.407457 11073 solver.cpp:337] Iteration 360, Testing net (#0)
I0727 16:27:41.433951 11073 solver.cpp:404]     Test net output #0: accuracy = 0.197656
I0727 16:27:41.434134 11073 solver.cpp:404]     Test net output #1: loss = 0.676794 (* 1 = 0.676794 loss)
I0727 16:27:41.578986 11073 solver.cpp:228] Iteration 360, loss = 0.669216
I0727 16:27:41.579097 11073 solver.cpp:244]     Train net output #0: accuracy = 0.21875
I0727 16:27:41.579144 11073 solver.cpp:244]     Train net output #1: loss = -0.669216 (* 1 = -0.669216 loss)
I0727 16:27:41.579180 11073 sgd_solver.cpp:106] Iteration 360, lr = 9.73823e-05
I0727 16:27:42.940034 11073 solver.cpp:337] Iteration 370, Testing net (#0)
I0727 16:27:43.641598 11073 solver.cpp:404]     Test net output #0: accuracy = 0.207031
I0727 16:27:43.641964 11073 solver.cpp:404]     Test net output #1: loss = 0.686108 (* 1 = 0.686108 loss)
I0727 16:27:43.771842 11073 solver.cpp:228] Iteration 370, loss = 0.751147
I0727 16:27:43.771924 11073 solver.cpp:244]     Train net output #0: accuracy = 0.210938
I0727 16:27:43.771960 11073 solver.cpp:244]     Train net output #1: loss = 0.751147 (* 1 = 0.751147 loss)
I0727 16:27:43.771989 11073 sgd_solver.cpp:106] Iteration 370, lr = 9.73119e-05
I0727 16:27:45.087918 11073 solver.cpp:337] Iteration 380, Testing net (#0)
I0727 16:27:45.668990 11073 solver.cpp:404]     Test net output #0: accuracy = 0.2
I0727 16:27:45.669054 11073 solver.cpp:404]     Test net output #1: loss = 0.671708 (* 1 = 0.671708 loss)
I0727 16:27:45.816247 11073 solver.cpp:228] Iteration 380, loss = 0.610059
I0727 16:27:45.816413 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:27:45.816462 11073 solver.cpp:244]     Train net output #1: loss = -0.610059 (* 1 = -0.610059 loss)
I0727 16:27:45.816501 11073 sgd_solver.cpp:106] Iteration 380, lr = 9.72416e-05
I0727 16:27:47.161460 11073 solver.cpp:337] Iteration 390, Testing net (#0)
I0727 16:27:48.138788 11073 solver.cpp:404]     Test net output #0: accuracy = 0.196094
I0727 16:27:48.138900 11073 solver.cpp:404]     Test net output #1: loss = 0.692114 (* 1 = 0.692114 loss)
I0727 16:27:48.349755 11073 solver.cpp:228] Iteration 390, loss = 0.886927
I0727 16:27:48.349848 11073 solver.cpp:244]     Train net output #0: accuracy = 0.234375
I0727 16:27:48.349884 11073 solver.cpp:244]     Train net output #1: loss = -0.886927 (* 1 = -0.886927 loss)
I0727 16:27:48.349907 11073 sgd_solver.cpp:106] Iteration 390, lr = 9.71714e-05
I0727 16:27:49.805244 11073 solver.cpp:337] Iteration 400, Testing net (#0)
I0727 16:27:50.430379 11073 solver.cpp:404]     Test net output #0: accuracy = 0.202344
I0727 16:27:50.430445 11073 solver.cpp:404]     Test net output #1: loss = 0.664965 (* 1 = 0.664965 loss)
I0727 16:27:50.590098 11073 solver.cpp:228] Iteration 400, loss = 0.638498
I0727 16:27:50.590164 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:27:50.590185 11073 solver.cpp:244]     Train net output #1: loss = -0.638498 (* 1 = -0.638498 loss)
I0727 16:27:50.590204 11073 sgd_solver.cpp:106] Iteration 400, lr = 9.71013e-05
I0727 16:27:52.005477 11073 solver.cpp:337] Iteration 410, Testing net (#0)
I0727 16:27:52.984473 11073 solver.cpp:404]     Test net output #0: accuracy = 0.203125
I0727 16:27:52.984665 11073 solver.cpp:404]     Test net output #1: loss = 0.686236 (* 1 = 0.686236 loss)
I0727 16:27:53.120952 11073 solver.cpp:228] Iteration 410, loss = 0.497252
I0727 16:27:53.121151 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:27:53.121214 11073 solver.cpp:244]     Train net output #1: loss = 0.497252 (* 1 = 0.497252 loss)
I0727 16:27:53.121265 11073 sgd_solver.cpp:106] Iteration 410, lr = 9.70313e-05
I0727 16:27:54.504597 11073 solver.cpp:337] Iteration 420, Testing net (#0)
I0727 16:27:55.505378 11073 solver.cpp:404]     Test net output #0: accuracy = 0.194531
I0727 16:27:55.505487 11073 solver.cpp:404]     Test net output #1: loss = 0.656614 (* 1 = 0.656614 loss)
I0727 16:27:55.643252 11073 solver.cpp:228] Iteration 420, loss = 0.677823
I0727 16:27:55.643671 11073 solver.cpp:244]     Train net output #0: accuracy = 0.210938
I0727 16:27:55.643743 11073 solver.cpp:244]     Train net output #1: loss = -0.677823 (* 1 = -0.677823 loss)
I0727 16:27:55.643801 11073 sgd_solver.cpp:106] Iteration 420, lr = 9.69615e-05
I0727 16:27:59.201439 11073 solver.cpp:337] Iteration 430, Testing net (#0)
I0727 16:28:00.752557 11073 solver.cpp:404]     Test net output #0: accuracy = 0.198437
I0727 16:28:00.752764 11073 solver.cpp:404]     Test net output #1: loss = 0.695161 (* 1 = 0.695161 loss)
I0727 16:28:01.123967 11073 solver.cpp:228] Iteration 430, loss = 0.602013
I0727 16:28:01.124053 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:28:01.124074 11073 solver.cpp:244]     Train net output #1: loss = -0.602013 (* 1 = -0.602013 loss)
I0727 16:28:01.124091 11073 sgd_solver.cpp:106] Iteration 430, lr = 9.68917e-05
I0727 16:28:02.987041 11073 solver.cpp:337] Iteration 440, Testing net (#0)
I0727 16:28:03.871691 11073 solver.cpp:404]     Test net output #0: accuracy = 0.204688
I0727 16:28:03.872071 11073 solver.cpp:404]     Test net output #1: loss = 0.681096 (* 1 = 0.681096 loss)
I0727 16:28:04.027532 11073 solver.cpp:228] Iteration 440, loss = 0.61662
I0727 16:28:04.027616 11073 solver.cpp:244]     Train net output #0: accuracy = 0.148438
I0727 16:28:04.027653 11073 solver.cpp:244]     Train net output #1: loss = -0.61662 (* 1 = -0.61662 loss)
I0727 16:28:04.027685 11073 sgd_solver.cpp:106] Iteration 440, lr = 9.68221e-05
I0727 16:28:05.437665 11073 solver.cpp:337] Iteration 450, Testing net (#0)
I0727 16:28:06.186331 11073 solver.cpp:404]     Test net output #0: accuracy = 0.2
I0727 16:28:06.186501 11073 solver.cpp:404]     Test net output #1: loss = 0.668616 (* 1 = 0.668616 loss)
I0727 16:28:06.318830 11073 solver.cpp:228] Iteration 450, loss = 0.693701
I0727 16:28:06.319160 11073 solver.cpp:244]     Train net output #0: accuracy = 0.210938
I0727 16:28:06.319245 11073 solver.cpp:244]     Train net output #1: loss = -0.693701 (* 1 = -0.693701 loss)
I0727 16:28:06.319311 11073 sgd_solver.cpp:106] Iteration 450, lr = 9.67526e-05
I0727 16:28:07.740710 11073 solver.cpp:337] Iteration 460, Testing net (#0)
I0727 16:28:08.712000 11073 solver.cpp:404]     Test net output #0: accuracy = 0.197656
I0727 16:28:08.712095 11073 solver.cpp:404]     Test net output #1: loss = 0.68618 (* 1 = 0.68618 loss)
I0727 16:28:08.856516 11073 solver.cpp:228] Iteration 460, loss = 0.529619
I0727 16:28:08.856688 11073 solver.cpp:244]     Train net output #0: accuracy = 0.132812
I0727 16:28:08.856762 11073 solver.cpp:244]     Train net output #1: loss = 0.529619 (* 1 = 0.529619 loss)
I0727 16:28:08.856838 11073 sgd_solver.cpp:106] Iteration 460, lr = 9.66832e-05
I0727 16:28:10.282853 11073 solver.cpp:337] Iteration 470, Testing net (#0)
I0727 16:28:11.293256 11073 solver.cpp:404]     Test net output #0: accuracy = 0.197656
I0727 16:28:11.293579 11073 solver.cpp:404]     Test net output #1: loss = 0.663673 (* 1 = 0.663673 loss)
I0727 16:28:11.420637 11073 solver.cpp:228] Iteration 470, loss = 0.732491
I0727 16:28:11.420717 11073 solver.cpp:244]     Train net output #0: accuracy = 0.242188
I0727 16:28:11.420753 11073 solver.cpp:244]     Train net output #1: loss = -0.732491 (* 1 = -0.732491 loss)
I0727 16:28:11.420783 11073 sgd_solver.cpp:106] Iteration 470, lr = 9.6614e-05
I0727 16:28:12.910367 11073 solver.cpp:337] Iteration 480, Testing net (#0)
I0727 16:28:13.857359 11073 solver.cpp:404]     Test net output #0: accuracy = 0.204688
I0727 16:28:13.857458 11073 solver.cpp:404]     Test net output #1: loss = 0.685297 (* 1 = 0.685297 loss)
I0727 16:28:14.003482 11073 solver.cpp:228] Iteration 480, loss = 0.736623
I0727 16:28:14.003942 11073 solver.cpp:244]     Train net output #0: accuracy = 0.21875
I0727 16:28:14.004212 11073 solver.cpp:244]     Train net output #1: loss = 0.736623 (* 1 = 0.736623 loss)
I0727 16:28:14.022008 11073 sgd_solver.cpp:106] Iteration 480, lr = 9.65448e-05
I0727 16:28:15.429734 11073 solver.cpp:337] Iteration 490, Testing net (#0)
I0727 16:28:16.418995 11073 solver.cpp:404]     Test net output #0: accuracy = 0.196094
I0727 16:28:16.419113 11073 solver.cpp:404]     Test net output #1: loss = 0.659249 (* 1 = 0.659249 loss)
I0727 16:28:16.607347 11073 solver.cpp:228] Iteration 490, loss = 0.722666
I0727 16:28:16.607429 11073 solver.cpp:244]     Train net output #0: accuracy = 0.210938
I0727 16:28:16.607689 11073 solver.cpp:244]     Train net output #1: loss = -0.722666 (* 1 = -0.722666 loss)
I0727 16:28:16.607791 11073 sgd_solver.cpp:106] Iteration 490, lr = 9.64758e-05
I0727 16:28:17.997197 11073 solver.cpp:337] Iteration 500, Testing net (#0)
I0727 16:28:19.001502 11073 solver.cpp:404]     Test net output #0: accuracy = 0.2
I0727 16:28:19.001612 11073 solver.cpp:404]     Test net output #1: loss = 0.699339 (* 1 = 0.699339 loss)
I0727 16:28:19.155740 11073 solver.cpp:228] Iteration 500, loss = 0.640001
I0727 16:28:19.155910 11073 solver.cpp:244]     Train net output #0: accuracy = 0.210938
I0727 16:28:19.156062 11073 solver.cpp:244]     Train net output #1: loss = -0.640001 (* 1 = -0.640001 loss)
I0727 16:28:19.156117 11073 sgd_solver.cpp:106] Iteration 500, lr = 9.64069e-05
I0727 16:28:20.593492 11073 solver.cpp:337] Iteration 510, Testing net (#0)
I0727 16:28:21.556737 11073 solver.cpp:404]     Test net output #0: accuracy = 0.202344
I0727 16:28:21.556845 11073 solver.cpp:404]     Test net output #1: loss = 0.670004 (* 1 = 0.670004 loss)
I0727 16:28:21.684381 11073 solver.cpp:228] Iteration 510, loss = 0.580118
I0727 16:28:21.685042 11073 solver.cpp:244]     Train net output #0: accuracy = 0.164062
I0727 16:28:21.685289 11073 solver.cpp:244]     Train net output #1: loss = -0.580118 (* 1 = -0.580118 loss)
I0727 16:28:21.685572 11073 sgd_solver.cpp:106] Iteration 510, lr = 9.63381e-05
I0727 16:28:23.091439 11073 solver.cpp:337] Iteration 520, Testing net (#0)
I0727 16:28:23.666493 11073 solver.cpp:404]     Test net output #0: accuracy = 0.203125
I0727 16:28:23.666546 11073 solver.cpp:404]     Test net output #1: loss = 0.685367 (* 1 = 0.685367 loss)
I0727 16:28:23.795354 11073 solver.cpp:228] Iteration 520, loss = 0.622124
I0727 16:28:23.795405 11073 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I0727 16:28:23.795423 11073 solver.cpp:244]     Train net output #1: loss = 0.622124 (* 1 = 0.622124 loss)
I0727 16:28:23.795436 11073 sgd_solver.cpp:106] Iteration 520, lr = 9.62694e-05
I0727 16:28:25.170929 11073 solver.cpp:337] Iteration 530, Testing net (#0)
I0727 16:28:25.725046 11073 solver.cpp:404]     Test net output #0: accuracy = 0.196094
I0727 16:28:25.725101 11073 solver.cpp:404]     Test net output #1: loss = 0.671759 (* 1 = 0.671759 loss)
I0727 16:28:25.867398 11073 solver.cpp:228] Iteration 530, loss = 0.702653
I0727 16:28:25.867455 11073 solver.cpp:244]     Train net output #0: accuracy = 0.203125
I0727 16:28:25.867475 11073 solver.cpp:244]     Train net output #1: loss = -0.702653 (* 1 = -0.702653 loss)
I0727 16:28:25.867491 11073 sgd_solver.cpp:106] Iteration 530, lr = 9.62008e-05
I0727 16:28:27.448276 11073 solver.cpp:337] Iteration 540, Testing net (#0)
I0727 16:28:28.252774 11073 solver.cpp:404]     Test net output #0: accuracy = 0.194531
I0727 16:28:28.252898 11073 solver.cpp:404]     Test net output #1: loss = 0.669079 (* 1 = 0.669079 loss)
I0727 16:28:28.488375 11073 solver.cpp:228] Iteration 540, loss = 0.704525
I0727 16:28:28.488467 11073 solver.cpp:244]     Train net output #0: accuracy = 0.1875
I0727 16:28:28.488507 11073 solver.cpp:244]     Train net output #1: loss = -0.704525 (* 1 = -0.704525 loss)
I0727 16:28:28.488528 11073 sgd_solver.cpp:106] Iteration 540, lr = 9.61323e-05
I0727 16:28:29.959914 11073 solver.cpp:337] Iteration 550, Testing net (#0)
I0727 16:28:30.982154 11073 solver.cpp:404]     Test net output #0: accuracy = 0.20625
I0727 16:28:30.982372 11073 solver.cpp:404]     Test net output #1: loss = 0.683812 (* 1 = 0.683812 loss)
I0727 16:28:31.107118 11073 solver.cpp:228] Iteration 550, loss = 0.490272
I0727 16:28:31.107239 11073 solver.cpp:244]     Train net output #0: accuracy = 0.125
I0727 16:28:31.107282 11073 solver.cpp:244]     Train net output #1: loss = -0.490272 (* 1 = -0.490272 loss)
I0727 16:28:31.107314 11073 sgd_solver.cpp:106] Iteration 550, lr = 9.6064e-05
I0727 16:28:32.467178 11073 solver.cpp:337] Iteration 560, Testing net (#0)
I0727 16:28:33.521735 11073 solver.cpp:404]     Test net output #0: accuracy = 0.201562
I0727 16:28:33.521822 11073 solver.cpp:404]     Test net output #1: loss = 0.681788 (* 1 = 0.681788 loss)
I0727 16:28:33.647547 11073 solver.cpp:228] Iteration 560, loss = 0.64486
I0727 16:28:33.647946 11073 solver.cpp:244]     Train net output #0: accuracy = 0.179688
I0727 16:28:33.648129 11073 solver.cpp:244]     Train net output #1: loss = -0.64486 (* 1 = -0.64486 loss)
I0727 16:28:33.648315 11073 sgd_solver.cpp:106] Iteration 560, lr = 9.59958e-05
I0727 16:28:35.027604 11073 solver.cpp:337] Iteration 570, Testing net (#0)
I0727 16:28:36.338217 11073 solver.cpp:404]     Test net output #0: accuracy = 0.195312
I0727 16:28:36.338362 11073 solver.cpp:404]     Test net output #1: loss = 0.684647 (* 1 = 0.684647 loss)
I0727 16:28:36.610754 11073 solver.cpp:228] Iteration 570, loss = 0.814623
I0727 16:28:36.610792 11073 solver.cpp:244]     Train net output #0: accuracy = 0.28125
I0727 16:28:36.610807 11073 solver.cpp:244]     Train net output #1: loss = -0.814623 (* 1 = -0.814623 loss)
I0727 16:28:36.610821 11073 sgd_solver.cpp:106] Iteration 570, lr = 9.59276e-05
I0727 16:28:38.499802 11073 solver.cpp:337] Iteration 580, Testing net (#0)
I0727 16:28:39.474907 11073 solver.cpp:404]     Test net output #0: accuracy = 0.203125
I0727 16:28:39.475105 11073 solver.cpp:404]     Test net output #1: loss = 0.665033 (* 1 = 0.665033 loss)
I0727 16:28:39.618345 11073 solver.cpp:228] Iteration 580, loss = 0.667625
I0727 16:28:39.618434 11073 solver.cpp:244]     Train net output #0: accuracy = 0.203125
I0727 16:28:39.618479 11073 solver.cpp:244]     Train net output #1: loss = -0.667625 (* 1 = -0.667625 loss)
I0727 16:28:39.618567 11073 sgd_solver.cpp:106] Iteration 580, lr = 9.58596e-05
I0727 16:28:41.012843 11073 solver.cpp:337] Iteration 590, Testing net (#0)
I0727 16:28:41.993583 11073 solver.cpp:404]     Test net output #0: accuracy = 0.20625
I0727 16:28:41.993733 11073 solver.cpp:404]     Test net output #1: loss = 0.690685 (* 1 = 0.690685 loss)
I0727 16:28:42.116127 11073 solver.cpp:228] Iteration 590, loss = 0.756133
I0727 16:28:42.116339 11073 solver.cpp:244]     Train net output #0: accuracy = 0.3125
I0727 16:28:42.116403 11073 solver.cpp:244]     Train net output #1: loss = -0.756133 (* 1 = -0.756133 loss)
I0727 16:28:42.116595 11073 sgd_solver.cpp:106] Iteration 590, lr = 9.57917e-05
I0727 16:28:43.482683 11073 solver.cpp:337] Iteration 600, Testing net (#0)
I0727 16:28:44.298399 11073 solver.cpp:404]     Test net output #0: accuracy = 0.191406
I0727 16:28:44.298552 11073 solver.cpp:404]     Test net output #1: loss = 0.661532 (* 1 = 0.661532 loss)
I0727 16:28:44.419726 11073 solver.cpp:228] Iteration 600, loss = 0.541721
I0727 16:28:44.419869 11073 solver.cpp:244]     Train net output #0: accuracy = 0.132812
I0727 16:28:44.419934 11073 solver.cpp:244]     Train net output #1: loss = -0.541721 (* 1 = -0.541721 loss)
I0727 16:28:44.420095 11073 sgd_solver.cpp:106] Iteration 600, lr = 9.57239e-05
I0727 16:28:45.805268 11073 solver.cpp:337] Iteration 610, Testing net (#0)
I0727 16:28:46.743875 11073 solver.cpp:404]     Test net output #0: accuracy = 0.197656
I0727 16:28:46.743984 11073 solver.cpp:404]     Test net output #1: loss = 0.676112 (* 1 = 0.676112 loss)
I0727 16:28:46.885694 11073 solver.cpp:228] Iteration 610, loss = 0.567892
I0727 16:28:46.885774 11073 solver.cpp:244]     Train net output #0: accuracy = 0.164062
I0727 16:28:46.885802 11073 solver.cpp:244]     Train net output #1: loss = -0.567892 (* 1 = -0.567892 loss)
I0727 16:28:46.885824 11073 sgd_solver.cpp:106] Iteration 610, lr = 9.56563e-05
I0727 16:28:48.241127 11073 solver.cpp:337] Iteration 620, Testing net (#0)
I0727 16:28:49.113207 11073 solver.cpp:404]     Test net output #0: accuracy = 0.207031
I0727 16:28:49.113379 11073 solver.cpp:404]     Test net output #1: loss = 0.685353 (* 1 = 0.685353 loss)
I0727 16:28:49.231119 11073 solver.cpp:228] Iteration 620, loss = 0.721739
I0727 16:28:49.231286 11073 solver.cpp:244]     Train net output #0: accuracy = 0.195312
I0727 16:28:49.231366 11073 solver.cpp:244]     Train net output #1: loss = -0.721739 (* 1 = -0.721739 loss)
I0727 16:28:49.231420 11073 sgd_solver.cpp:106] Iteration 620, lr = 9.55887e-05
I0727 16:28:50.643880 11073 solver.cpp:337] Iteration 630, Testing net (#0)
I0727 16:28:51.298707 11073 solver.cpp:404]     Test net output #0: accuracy = 0.2
I0727 16:28:51.298898 11073 solver.cpp:404]     Test net output #1: loss = 0.667114 (* 1 = 0.667114 loss)
I0727 16:28:51.429955 11073 solver.cpp:228] Iteration 630, loss = 0.812588
I0727 16:28:51.430063 11073 solver.cpp:244]     Train net output #0: accuracy = 0.234375
I0727 16:28:51.430080 11073 solver.cpp:244]     Train net output #1: loss = -0.812588 (* 1 = -0.812588 loss)
I0727 16:28:51.430096 11073 sgd_solver.cpp:106] Iteration 630, lr = 9.55213e-05
I0727 16:28:52.915678 11073 solver.cpp:337] Iteration 640, Testing net (#0)
I0727 16:28:53.693873 11073 solver.cpp:404]     Test net output #0: accuracy = 0.196094
I0727 16:28:53.693929 11073 solver.cpp:404]     Test net output #1: loss = 0.69106 (* 1 = 0.69106 loss)
I0727 16:28:53.863376 11073 solver.cpp:228] Iteration 640, loss = 0.529781
I0727 16:28:53.863499 11073 solver.cpp:244]     Train net output #0: accuracy = 0.148438
I0727 16:28:53.863534 11073 solver.cpp:244]     Train net output #1: loss = -0.529781 (* 1 = -0.529781 loss)
I0727 16:28:53.863564 11073 sgd_solver.cpp:106] Iteration 640, lr = 9.54539e-05
I0727 16:28:55.990770 11073 solver.cpp:337] Iteration 650, Testing net (#0)
I0727 16:28:56.582094 11073 solver.cpp:404]     Test net output #0: accuracy = 0.202344
I0727 16:28:56.582146 11073 solver.cpp:404]     Test net output #1: loss = 0.664492 (* 1 = 0.664492 loss)
I0727 16:28:56.708256 11073 solver.cpp:228] Iteration 650, loss = 0.663322
I0727 16:28:56.708313 11073 solver.cpp:244]     Train net output #0: accuracy = 0.195312
I0727 16:28:56.708333 11073 solver.cpp:244]     Train net output #1: loss = -0.663322 (* 1 = -0.663322 loss)
I0727 16:28:56.708348 11073 sgd_solver.cpp:106] Iteration 650, lr = 9.53867e-05
I0727 16:28:58.238688 11073 solver.cpp:337] Iteration 660, Testing net (#0)
I0727 16:28:58.910284 11073 solver.cpp:404]     Test net output #0: accuracy = 0.203125
I0727 16:28:58.910377 11073 solver.cpp:404]     Test net output #1: loss = 0.684247 (* 1 = 0.684247 loss)
I0727 16:28:59.091234 11073 solver.cpp:228] Iteration 660, loss = 0.571907
I0727 16:28:59.091451 11073 solver.cpp:244]     Train net output #0: accuracy = 0.171875
I0727 16:28:59.091557 11073 solver.cpp:244]     Train net output #1: loss = -0.571907 (* 1 = -0.571907 loss)
I0727 16:28:59.091627 11073 sgd_solver.cpp:106] Iteration 660, lr = 9.53196e-05
I0727 16:29:00.487347 11073 solver.cpp:337] Iteration 670, Testing net (#0)
I0727 16:29:01.486779 11073 solver.cpp:404]     Test net output #0: accuracy = 0.194531
I0727 16:29:01.486884 11073 solver.cpp:404]     Test net output #1: loss = 0.655521 (* 1 = 0.655521 loss)
I0727 16:29:01.645586 11073 solver.cpp:228] Iteration 670, loss = 0.596611
I0727 16:29:01.645692 11073 solver.cpp:244]     Train net output #0: accuracy = 0.15625
I0727 16:29:01.645740 11073 solver.cpp:244]     Train net output #1: loss = -0.596611 (* 1 = -0.596611 loss)
I0727 16:29:01.645778 11073 sgd_solver.cpp:106] Iteration 670, lr = 9.52526e-05
I0727 16:29:02.997982 11073 solver.cpp:337] Iteration 680, Testing net (#0)
I0727 16:29:03.975324 11073 solver.cpp:404]     Test net output #0: accuracy = 0.198437
I0727 16:29:03.975463 11073 solver.cpp:404]     Test net output #1: loss = 0.693392 (* 1 = 0.693392 loss)
I0727 16:29:04.171316 11073 solver.cpp:228] Iteration 680, loss = 0.628965
I0727 16:29:04.171427 11073 solver.cpp:244]     Train net output #0: accuracy = 0.21875
I0727 16:29:04.171494 11073 solver.cpp:244]     Train net output #1: loss = -0.628965 (* 1 = -0.628965 loss)
I0727 16:29:04.171535 11073 sgd_solver.cpp:106] Iteration 680, lr = 9.51857e-05
I0727 16:29:06.550026 11073 solver.cpp:337] Iteration 690, Testing net (#0)
I0727 16:29:07.402832 11073 solver.cpp:404]     Test net output #0: accuracy = 0.204688
I0727 16:29:07.402910 11073 solver.cpp:404]     Test net output #1: loss = 0.680982 (* 1 = 0.680982 loss)
I0727 16:29:07.533488 11073 solver.cpp:228] Iteration 690, loss = 0.630071
I0727 16:29:07.533581 11073 solver.cpp:244]     Train net output #0: accuracy = 0.1875
I0727 16:29:07.533787 11073 solver.cpp:244]     Train net output #1: loss = -0.630071 (* 1 = -0.630071 loss)
I0727 16:29:07.533891 11073 sgd_solver.cpp:106] Iteration 690, lr = 9.51189e-05
