I0712 17:19:56.388553 13346 caffe.cpp:178] Use CPU.
I0712 17:19:56.388880 13346 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 5000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0
snapshot: 5000
snapshot_prefix: "examples/siamese/My_mnist_siamese_0to1c"
solver_mode: CPU
net: "examples/siamese/mnist_siamese_train_test.prototxt"
I0712 17:19:56.388962 13346 solver.cpp:91] Creating training net from net file: examples/siamese/mnist_siamese_train_test.prototxt
I0712 17:19:56.389436 13346 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer pair_data
I0712 17:19:56.389580 13346 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TRAIN
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_train_leveldb_0to1c"
    batch_size: 64
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0712 17:19:56.389695 13346 layer_factory.hpp:77] Creating layer pair_data
I0712 17:19:56.390230 13346 net.cpp:91] Creating Layer pair_data
I0712 17:19:56.390254 13346 net.cpp:399] pair_data -> pair_data
I0712 17:19:56.390277 13346 net.cpp:399] pair_data -> sim
I0712 17:19:56.417834 13350 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_train_leveldb_0to1c
I0712 17:19:56.418911 13346 data_layer.cpp:41] output data size: 64,2,28,28
I0712 17:19:56.419533 13346 net.cpp:141] Setting up pair_data
I0712 17:19:56.419638 13346 net.cpp:148] Top shape: 64 2 28 28 (100352)
I0712 17:19:56.419703 13346 net.cpp:148] Top shape: 64 (64)
I0712 17:19:56.419747 13346 net.cpp:156] Memory required for data: 401664
I0712 17:19:56.419914 13346 layer_factory.hpp:77] Creating layer slice_pair
I0712 17:19:56.419973 13346 net.cpp:91] Creating Layer slice_pair
I0712 17:19:56.420011 13346 net.cpp:425] slice_pair <- pair_data
I0712 17:19:56.420054 13346 net.cpp:399] slice_pair -> data
I0712 17:19:56.420095 13346 net.cpp:399] slice_pair -> data_p
I0712 17:19:56.420140 13346 net.cpp:141] Setting up slice_pair
I0712 17:19:56.420169 13346 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0712 17:19:56.420195 13346 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0712 17:19:56.420217 13346 net.cpp:156] Memory required for data: 803072
I0712 17:19:56.420239 13346 layer_factory.hpp:77] Creating layer conv1
I0712 17:19:56.420284 13346 net.cpp:91] Creating Layer conv1
I0712 17:19:56.420308 13346 net.cpp:425] conv1 <- data
I0712 17:19:56.420334 13346 net.cpp:399] conv1 -> conv1
I0712 17:19:56.420457 13346 net.cpp:141] Setting up conv1
I0712 17:19:56.420492 13346 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0712 17:19:56.420511 13346 net.cpp:156] Memory required for data: 3752192
I0712 17:19:56.420539 13346 layer_factory.hpp:77] Creating layer pool1
I0712 17:19:56.420563 13346 net.cpp:91] Creating Layer pool1
I0712 17:19:56.420580 13346 net.cpp:425] pool1 <- conv1
I0712 17:19:56.420598 13346 net.cpp:399] pool1 -> pool1
I0712 17:19:56.420639 13346 net.cpp:141] Setting up pool1
I0712 17:19:56.420660 13346 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0712 17:19:56.420675 13346 net.cpp:156] Memory required for data: 4489472
I0712 17:19:56.420687 13346 layer_factory.hpp:77] Creating layer conv2
I0712 17:19:56.420711 13346 net.cpp:91] Creating Layer conv2
I0712 17:19:56.420727 13346 net.cpp:425] conv2 <- pool1
I0712 17:19:56.420754 13346 net.cpp:399] conv2 -> conv2
I0712 17:19:56.421005 13346 net.cpp:141] Setting up conv2
I0712 17:19:56.421036 13346 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0712 17:19:56.421051 13346 net.cpp:156] Memory required for data: 5308672
I0712 17:19:56.421072 13346 layer_factory.hpp:77] Creating layer pool2
I0712 17:19:56.421105 13346 net.cpp:91] Creating Layer pool2
I0712 17:19:56.421139 13346 net.cpp:425] pool2 <- conv2
I0712 17:19:56.421162 13346 net.cpp:399] pool2 -> pool2
I0712 17:19:56.421187 13346 net.cpp:141] Setting up pool2
I0712 17:19:56.421206 13346 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0712 17:19:56.421219 13346 net.cpp:156] Memory required for data: 5513472
I0712 17:19:56.421232 13346 layer_factory.hpp:77] Creating layer ip1
I0712 17:19:56.421262 13346 net.cpp:91] Creating Layer ip1
I0712 17:19:56.421277 13346 net.cpp:425] ip1 <- pool2
I0712 17:19:56.421294 13346 net.cpp:399] ip1 -> ip1
I0712 17:19:56.424834 13346 net.cpp:141] Setting up ip1
I0712 17:19:56.424942 13346 net.cpp:148] Top shape: 64 500 (32000)
I0712 17:19:56.424969 13346 net.cpp:156] Memory required for data: 5641472
I0712 17:19:56.425001 13346 layer_factory.hpp:77] Creating layer relu1
I0712 17:19:56.425026 13346 net.cpp:91] Creating Layer relu1
I0712 17:19:56.425045 13346 net.cpp:425] relu1 <- ip1
I0712 17:19:56.425066 13346 net.cpp:386] relu1 -> ip1 (in-place)
I0712 17:19:56.425093 13346 net.cpp:141] Setting up relu1
I0712 17:19:56.425112 13346 net.cpp:148] Top shape: 64 500 (32000)
I0712 17:19:56.425125 13346 net.cpp:156] Memory required for data: 5769472
I0712 17:19:56.425139 13346 layer_factory.hpp:77] Creating layer ip2
I0712 17:19:56.425161 13346 net.cpp:91] Creating Layer ip2
I0712 17:19:56.425179 13346 net.cpp:425] ip2 <- ip1
I0712 17:19:56.425199 13346 net.cpp:399] ip2 -> ip2
I0712 17:19:56.425266 13346 net.cpp:141] Setting up ip2
I0712 17:19:56.425287 13346 net.cpp:148] Top shape: 64 10 (640)
I0712 17:19:56.425298 13346 net.cpp:156] Memory required for data: 5772032
I0712 17:19:56.425325 13346 layer_factory.hpp:77] Creating layer feat
I0712 17:19:56.425346 13346 net.cpp:91] Creating Layer feat
I0712 17:19:56.425362 13346 net.cpp:425] feat <- ip2
I0712 17:19:56.425380 13346 net.cpp:399] feat -> feat
I0712 17:19:56.425405 13346 net.cpp:141] Setting up feat
I0712 17:19:56.425425 13346 net.cpp:148] Top shape: 64 2 (128)
I0712 17:19:56.425438 13346 net.cpp:156] Memory required for data: 5772544
I0712 17:19:56.425458 13346 layer_factory.hpp:77] Creating layer conv1_p
I0712 17:19:56.425482 13346 net.cpp:91] Creating Layer conv1_p
I0712 17:19:56.425498 13346 net.cpp:425] conv1_p <- data_p
I0712 17:19:56.425515 13346 net.cpp:399] conv1_p -> conv1_p
I0712 17:19:56.425560 13346 net.cpp:141] Setting up conv1_p
I0712 17:19:56.425581 13346 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0712 17:19:56.425595 13346 net.cpp:156] Memory required for data: 8721664
I0712 17:19:56.425611 13346 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0712 17:19:56.425626 13346 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0712 17:19:56.425642 13346 layer_factory.hpp:77] Creating layer pool1_p
I0712 17:19:56.425662 13346 net.cpp:91] Creating Layer pool1_p
I0712 17:19:56.425676 13346 net.cpp:425] pool1_p <- conv1_p
I0712 17:19:56.425698 13346 net.cpp:399] pool1_p -> pool1_p
I0712 17:19:56.425722 13346 net.cpp:141] Setting up pool1_p
I0712 17:19:56.425740 13346 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0712 17:19:56.425755 13346 net.cpp:156] Memory required for data: 9458944
I0712 17:19:56.425768 13346 layer_factory.hpp:77] Creating layer conv2_p
I0712 17:19:56.425791 13346 net.cpp:91] Creating Layer conv2_p
I0712 17:19:56.425806 13346 net.cpp:425] conv2_p <- pool1_p
I0712 17:19:56.425827 13346 net.cpp:399] conv2_p -> conv2_p
I0712 17:19:56.426075 13346 net.cpp:141] Setting up conv2_p
I0712 17:19:56.426115 13346 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0712 17:19:56.426136 13346 net.cpp:156] Memory required for data: 10278144
I0712 17:19:56.426161 13346 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0712 17:19:56.426184 13346 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0712 17:19:56.426204 13346 layer_factory.hpp:77] Creating layer pool2_p
I0712 17:19:56.426226 13346 net.cpp:91] Creating Layer pool2_p
I0712 17:19:56.426244 13346 net.cpp:425] pool2_p <- conv2_p
I0712 17:19:56.426326 13346 net.cpp:399] pool2_p -> pool2_p
I0712 17:19:56.426410 13346 net.cpp:141] Setting up pool2_p
I0712 17:19:56.426434 13346 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0712 17:19:56.426450 13346 net.cpp:156] Memory required for data: 10482944
I0712 17:19:56.426465 13346 layer_factory.hpp:77] Creating layer ip1_p
I0712 17:19:56.426496 13346 net.cpp:91] Creating Layer ip1_p
I0712 17:19:56.426514 13346 net.cpp:425] ip1_p <- pool2_p
I0712 17:19:56.426537 13346 net.cpp:399] ip1_p -> ip1_p
I0712 17:19:56.443048 13346 net.cpp:141] Setting up ip1_p
I0712 17:19:56.443177 13346 net.cpp:148] Top shape: 64 500 (32000)
I0712 17:19:56.443208 13346 net.cpp:156] Memory required for data: 10610944
I0712 17:19:56.443238 13346 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0712 17:19:56.443274 13346 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0712 17:19:56.443301 13346 layer_factory.hpp:77] Creating layer relu1_p
I0712 17:19:56.443337 13346 net.cpp:91] Creating Layer relu1_p
I0712 17:19:56.443367 13346 net.cpp:425] relu1_p <- ip1_p
I0712 17:19:56.443404 13346 net.cpp:386] relu1_p -> ip1_p (in-place)
I0712 17:19:56.443431 13346 net.cpp:141] Setting up relu1_p
I0712 17:19:56.443450 13346 net.cpp:148] Top shape: 64 500 (32000)
I0712 17:19:56.443464 13346 net.cpp:156] Memory required for data: 10738944
I0712 17:19:56.443477 13346 layer_factory.hpp:77] Creating layer ip2_p
I0712 17:19:56.443501 13346 net.cpp:91] Creating Layer ip2_p
I0712 17:19:56.443517 13346 net.cpp:425] ip2_p <- ip1_p
I0712 17:19:56.443536 13346 net.cpp:399] ip2_p -> ip2_p
I0712 17:19:56.443624 13346 net.cpp:141] Setting up ip2_p
I0712 17:19:56.443655 13346 net.cpp:148] Top shape: 64 10 (640)
I0712 17:19:56.443666 13346 net.cpp:156] Memory required for data: 10741504
I0712 17:19:56.443682 13346 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0712 17:19:56.443711 13346 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0712 17:19:56.443735 13346 layer_factory.hpp:77] Creating layer feat_p
I0712 17:19:56.443886 13346 net.cpp:91] Creating Layer feat_p
I0712 17:19:56.443922 13346 net.cpp:425] feat_p <- ip2_p
I0712 17:19:56.443948 13346 net.cpp:399] feat_p -> feat_p
I0712 17:19:56.443984 13346 net.cpp:141] Setting up feat_p
I0712 17:19:56.444005 13346 net.cpp:148] Top shape: 64 2 (128)
I0712 17:19:56.444016 13346 net.cpp:156] Memory required for data: 10742016
I0712 17:19:56.444031 13346 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0712 17:19:56.444046 13346 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0712 17:19:56.444058 13346 layer_factory.hpp:77] Creating layer loss
I0712 17:19:56.444083 13346 net.cpp:91] Creating Layer loss
I0712 17:19:56.444097 13346 net.cpp:425] loss <- feat
I0712 17:19:56.444111 13346 net.cpp:425] loss <- feat_p
I0712 17:19:56.444125 13346 net.cpp:425] loss <- sim
I0712 17:19:56.444144 13346 net.cpp:399] loss -> loss
I0712 17:19:56.444174 13346 net.cpp:141] Setting up loss
I0712 17:19:56.444188 13346 net.cpp:148] Top shape: (1)
I0712 17:19:56.444201 13346 net.cpp:151]     with loss weight 1
I0712 17:19:56.444234 13346 net.cpp:156] Memory required for data: 10742020
I0712 17:19:56.444247 13346 net.cpp:217] loss needs backward computation.
I0712 17:19:56.444260 13346 net.cpp:217] feat_p needs backward computation.
I0712 17:19:56.444272 13346 net.cpp:217] ip2_p needs backward computation.
I0712 17:19:56.444285 13346 net.cpp:217] relu1_p needs backward computation.
I0712 17:19:56.444298 13346 net.cpp:217] ip1_p needs backward computation.
I0712 17:19:56.444309 13346 net.cpp:217] pool2_p needs backward computation.
I0712 17:19:56.444322 13346 net.cpp:217] conv2_p needs backward computation.
I0712 17:19:56.444335 13346 net.cpp:217] pool1_p needs backward computation.
I0712 17:19:56.444347 13346 net.cpp:217] conv1_p needs backward computation.
I0712 17:19:56.444360 13346 net.cpp:217] feat needs backward computation.
I0712 17:19:56.444383 13346 net.cpp:217] ip2 needs backward computation.
I0712 17:19:56.444407 13346 net.cpp:217] relu1 needs backward computation.
I0712 17:19:56.444420 13346 net.cpp:217] ip1 needs backward computation.
I0712 17:19:56.444433 13346 net.cpp:217] pool2 needs backward computation.
I0712 17:19:56.444445 13346 net.cpp:217] conv2 needs backward computation.
I0712 17:19:56.444458 13346 net.cpp:217] pool1 needs backward computation.
I0712 17:19:56.444471 13346 net.cpp:217] conv1 needs backward computation.
I0712 17:19:56.444484 13346 net.cpp:219] slice_pair does not need backward computation.
I0712 17:19:56.444497 13346 net.cpp:219] pair_data does not need backward computation.
I0712 17:19:56.444509 13346 net.cpp:261] This network produces output loss
I0712 17:19:56.444634 13346 net.cpp:274] Network initialization done.
I0712 17:19:56.445236 13346 solver.cpp:181] Creating test net (#0) specified by net file: examples/siamese/mnist_siamese_train_test.prototxt
I0712 17:19:56.447536 13346 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0712 17:19:56.447938 13346 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb_0to1c"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0712 17:19:56.450069 13346 layer_factory.hpp:77] Creating layer pair_data
I0712 17:19:56.450825 13346 net.cpp:91] Creating Layer pair_data
I0712 17:19:56.452337 13346 net.cpp:399] pair_data -> pair_data
I0712 17:19:56.452679 13346 net.cpp:399] pair_data -> sim
I0712 17:19:56.516649 13352 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb_0to1c
I0712 17:19:56.517073 13346 data_layer.cpp:41] output data size: 100,2,28,28
I0712 17:19:56.517951 13346 net.cpp:141] Setting up pair_data
I0712 17:19:56.517972 13346 net.cpp:148] Top shape: 100 2 28 28 (156800)
I0712 17:19:56.517981 13346 net.cpp:148] Top shape: 100 (100)
I0712 17:19:56.517985 13346 net.cpp:156] Memory required for data: 627600
I0712 17:19:56.517995 13346 layer_factory.hpp:77] Creating layer slice_pair
I0712 17:19:56.518013 13346 net.cpp:91] Creating Layer slice_pair
I0712 17:19:56.518019 13346 net.cpp:425] slice_pair <- pair_data
I0712 17:19:56.518029 13346 net.cpp:399] slice_pair -> data
I0712 17:19:56.518044 13346 net.cpp:399] slice_pair -> data_p
I0712 17:19:56.518059 13346 net.cpp:141] Setting up slice_pair
I0712 17:19:56.518066 13346 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0712 17:19:56.518072 13346 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0712 17:19:56.518076 13346 net.cpp:156] Memory required for data: 1254800
I0712 17:19:56.518081 13346 layer_factory.hpp:77] Creating layer conv1
I0712 17:19:56.518100 13346 net.cpp:91] Creating Layer conv1
I0712 17:19:56.518105 13346 net.cpp:425] conv1 <- data
I0712 17:19:56.518113 13346 net.cpp:399] conv1 -> conv1
I0712 17:19:56.518159 13346 net.cpp:141] Setting up conv1
I0712 17:19:56.518168 13346 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0712 17:19:56.518173 13346 net.cpp:156] Memory required for data: 5862800
I0712 17:19:56.518184 13346 layer_factory.hpp:77] Creating layer pool1
I0712 17:19:56.518194 13346 net.cpp:91] Creating Layer pool1
I0712 17:19:56.518199 13346 net.cpp:425] pool1 <- conv1
I0712 17:19:56.518206 13346 net.cpp:399] pool1 -> pool1
I0712 17:19:56.518219 13346 net.cpp:141] Setting up pool1
I0712 17:19:56.518226 13346 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0712 17:19:56.518230 13346 net.cpp:156] Memory required for data: 7014800
I0712 17:19:56.518234 13346 layer_factory.hpp:77] Creating layer conv2
I0712 17:19:56.518249 13346 net.cpp:91] Creating Layer conv2
I0712 17:19:56.518290 13346 net.cpp:425] conv2 <- pool1
I0712 17:19:56.518302 13346 net.cpp:399] conv2 -> conv2
I0712 17:19:56.518587 13346 net.cpp:141] Setting up conv2
I0712 17:19:56.518596 13346 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0712 17:19:56.518600 13346 net.cpp:156] Memory required for data: 8294800
I0712 17:19:56.518610 13346 layer_factory.hpp:77] Creating layer pool2
I0712 17:19:56.518617 13346 net.cpp:91] Creating Layer pool2
I0712 17:19:56.518622 13346 net.cpp:425] pool2 <- conv2
I0712 17:19:56.518630 13346 net.cpp:399] pool2 -> pool2
I0712 17:19:56.518638 13346 net.cpp:141] Setting up pool2
I0712 17:19:56.518645 13346 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0712 17:19:56.518648 13346 net.cpp:156] Memory required for data: 8614800
I0712 17:19:56.518653 13346 layer_factory.hpp:77] Creating layer ip1
I0712 17:19:56.518662 13346 net.cpp:91] Creating Layer ip1
I0712 17:19:56.518667 13346 net.cpp:425] ip1 <- pool2
I0712 17:19:56.518674 13346 net.cpp:399] ip1 -> ip1
I0712 17:19:56.523175 13346 net.cpp:141] Setting up ip1
I0712 17:19:56.528092 13346 net.cpp:148] Top shape: 100 500 (50000)
I0712 17:19:56.528848 13346 net.cpp:156] Memory required for data: 8814800
I0712 17:19:56.528934 13346 layer_factory.hpp:77] Creating layer relu1
I0712 17:19:56.528957 13346 net.cpp:91] Creating Layer relu1
I0712 17:19:56.528971 13346 net.cpp:425] relu1 <- ip1
I0712 17:19:56.528986 13346 net.cpp:386] relu1 -> ip1 (in-place)
I0712 17:19:56.529006 13346 net.cpp:141] Setting up relu1
I0712 17:19:56.529022 13346 net.cpp:148] Top shape: 100 500 (50000)
I0712 17:19:56.529058 13346 net.cpp:156] Memory required for data: 9014800
I0712 17:19:56.529078 13346 layer_factory.hpp:77] Creating layer ip2
I0712 17:19:56.529104 13346 net.cpp:91] Creating Layer ip2
I0712 17:19:56.529116 13346 net.cpp:425] ip2 <- ip1
I0712 17:19:56.529129 13346 net.cpp:399] ip2 -> ip2
I0712 17:19:56.529196 13346 net.cpp:141] Setting up ip2
I0712 17:19:56.529211 13346 net.cpp:148] Top shape: 100 10 (1000)
I0712 17:19:56.529222 13346 net.cpp:156] Memory required for data: 9018800
I0712 17:19:56.529234 13346 layer_factory.hpp:77] Creating layer feat
I0712 17:19:56.529247 13346 net.cpp:91] Creating Layer feat
I0712 17:19:56.529258 13346 net.cpp:425] feat <- ip2
I0712 17:19:56.529270 13346 net.cpp:399] feat -> feat
I0712 17:19:56.529290 13346 net.cpp:141] Setting up feat
I0712 17:19:56.529304 13346 net.cpp:148] Top shape: 100 2 (200)
I0712 17:19:56.529314 13346 net.cpp:156] Memory required for data: 9019600
I0712 17:19:56.529326 13346 layer_factory.hpp:77] Creating layer conv1_p
I0712 17:19:56.529343 13346 net.cpp:91] Creating Layer conv1_p
I0712 17:19:56.529355 13346 net.cpp:425] conv1_p <- data_p
I0712 17:19:56.529367 13346 net.cpp:399] conv1_p -> conv1_p
I0712 17:19:56.529402 13346 net.cpp:141] Setting up conv1_p
I0712 17:19:56.529443 13346 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0712 17:19:56.529459 13346 net.cpp:156] Memory required for data: 13627600
I0712 17:19:56.529471 13346 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0712 17:19:56.529482 13346 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0712 17:19:56.529494 13346 layer_factory.hpp:77] Creating layer pool1_p
I0712 17:19:56.529510 13346 net.cpp:91] Creating Layer pool1_p
I0712 17:19:56.529521 13346 net.cpp:425] pool1_p <- conv1_p
I0712 17:19:56.529534 13346 net.cpp:399] pool1_p -> pool1_p
I0712 17:19:56.529551 13346 net.cpp:141] Setting up pool1_p
I0712 17:19:56.529619 13346 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0712 17:19:56.529633 13346 net.cpp:156] Memory required for data: 14779600
I0712 17:19:56.529644 13346 layer_factory.hpp:77] Creating layer conv2_p
I0712 17:19:56.529662 13346 net.cpp:91] Creating Layer conv2_p
I0712 17:19:56.529675 13346 net.cpp:425] conv2_p <- pool1_p
I0712 17:19:56.529688 13346 net.cpp:399] conv2_p -> conv2_p
I0712 17:19:56.529899 13346 net.cpp:141] Setting up conv2_p
I0712 17:19:56.529917 13346 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0712 17:19:56.529934 13346 net.cpp:156] Memory required for data: 16059600
I0712 17:19:56.529958 13346 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0712 17:19:56.529969 13346 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0712 17:19:56.529979 13346 layer_factory.hpp:77] Creating layer pool2_p
I0712 17:19:56.529994 13346 net.cpp:91] Creating Layer pool2_p
I0712 17:19:56.530004 13346 net.cpp:425] pool2_p <- conv2_p
I0712 17:19:56.530016 13346 net.cpp:399] pool2_p -> pool2_p
I0712 17:19:56.530035 13346 net.cpp:141] Setting up pool2_p
I0712 17:19:56.530047 13346 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0712 17:19:56.530056 13346 net.cpp:156] Memory required for data: 16379600
I0712 17:19:56.530066 13346 layer_factory.hpp:77] Creating layer ip1_p
I0712 17:19:56.530079 13346 net.cpp:91] Creating Layer ip1_p
I0712 17:19:56.530091 13346 net.cpp:425] ip1_p <- pool2_p
I0712 17:19:56.530104 13346 net.cpp:399] ip1_p -> ip1_p
I0712 17:19:56.539930 13346 net.cpp:141] Setting up ip1_p
I0712 17:19:56.540086 13346 net.cpp:148] Top shape: 100 500 (50000)
I0712 17:19:56.540143 13346 net.cpp:156] Memory required for data: 16579600
I0712 17:19:56.540180 13346 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0712 17:19:56.540215 13346 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0712 17:19:56.540242 13346 layer_factory.hpp:77] Creating layer relu1_p
I0712 17:19:56.540266 13346 net.cpp:91] Creating Layer relu1_p
I0712 17:19:56.540282 13346 net.cpp:425] relu1_p <- ip1_p
I0712 17:19:56.540302 13346 net.cpp:386] relu1_p -> ip1_p (in-place)
I0712 17:19:56.540326 13346 net.cpp:141] Setting up relu1_p
I0712 17:19:56.540345 13346 net.cpp:148] Top shape: 100 500 (50000)
I0712 17:19:56.540359 13346 net.cpp:156] Memory required for data: 16779600
I0712 17:19:56.540374 13346 layer_factory.hpp:77] Creating layer ip2_p
I0712 17:19:56.540412 13346 net.cpp:91] Creating Layer ip2_p
I0712 17:19:56.540431 13346 net.cpp:425] ip2_p <- ip1_p
I0712 17:19:56.540452 13346 net.cpp:399] ip2_p -> ip2_p
I0712 17:19:56.540607 13346 net.cpp:141] Setting up ip2_p
I0712 17:19:56.540671 13346 net.cpp:148] Top shape: 100 10 (1000)
I0712 17:19:56.540702 13346 net.cpp:156] Memory required for data: 16783600
I0712 17:19:56.540743 13346 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0712 17:19:56.540778 13346 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0712 17:19:56.540805 13346 layer_factory.hpp:77] Creating layer feat_p
I0712 17:19:56.540843 13346 net.cpp:91] Creating Layer feat_p
I0712 17:19:56.540877 13346 net.cpp:425] feat_p <- ip2_p
I0712 17:19:56.540911 13346 net.cpp:399] feat_p -> feat_p
I0712 17:19:56.540980 13346 net.cpp:141] Setting up feat_p
I0712 17:19:56.541019 13346 net.cpp:148] Top shape: 100 2 (200)
I0712 17:19:56.541043 13346 net.cpp:156] Memory required for data: 16784400
I0712 17:19:56.541066 13346 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0712 17:19:56.541091 13346 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0712 17:19:56.541116 13346 layer_factory.hpp:77] Creating layer loss
I0712 17:19:56.541146 13346 net.cpp:91] Creating Layer loss
I0712 17:19:56.541172 13346 net.cpp:425] loss <- feat
I0712 17:19:56.541199 13346 net.cpp:425] loss <- feat_p
I0712 17:19:56.541237 13346 net.cpp:425] loss <- sim
I0712 17:19:56.541270 13346 net.cpp:399] loss -> loss
I0712 17:19:56.541321 13346 net.cpp:141] Setting up loss
I0712 17:19:56.541358 13346 net.cpp:148] Top shape: (1)
I0712 17:19:56.541381 13346 net.cpp:151]     with loss weight 1
I0712 17:19:56.541422 13346 net.cpp:156] Memory required for data: 16784404
I0712 17:19:56.541456 13346 net.cpp:217] loss needs backward computation.
I0712 17:19:56.541486 13346 net.cpp:217] feat_p needs backward computation.
I0712 17:19:56.541514 13346 net.cpp:217] ip2_p needs backward computation.
I0712 17:19:56.541554 13346 net.cpp:217] relu1_p needs backward computation.
I0712 17:19:56.541604 13346 net.cpp:217] ip1_p needs backward computation.
I0712 17:19:56.541666 13346 net.cpp:217] pool2_p needs backward computation.
I0712 17:19:56.541697 13346 net.cpp:217] conv2_p needs backward computation.
I0712 17:19:56.541738 13346 net.cpp:217] pool1_p needs backward computation.
I0712 17:19:56.541771 13346 net.cpp:217] conv1_p needs backward computation.
I0712 17:19:56.541801 13346 net.cpp:217] feat needs backward computation.
I0712 17:19:56.541831 13346 net.cpp:217] ip2 needs backward computation.
I0712 17:19:56.541862 13346 net.cpp:217] relu1 needs backward computation.
I0712 17:19:56.541908 13346 net.cpp:217] ip1 needs backward computation.
I0712 17:19:56.541960 13346 net.cpp:217] pool2 needs backward computation.
I0712 17:19:56.541997 13346 net.cpp:217] conv2 needs backward computation.
I0712 17:19:56.542034 13346 net.cpp:217] pool1 needs backward computation.
I0712 17:19:56.542079 13346 net.cpp:217] conv1 needs backward computation.
I0712 17:19:56.542124 13346 net.cpp:219] slice_pair does not need backward computation.
I0712 17:19:56.542156 13346 net.cpp:219] pair_data does not need backward computation.
I0712 17:19:56.542179 13346 net.cpp:261] This network produces output loss
I0712 17:19:56.542279 13346 net.cpp:274] Network initialization done.
I0712 17:19:56.542966 13346 solver.cpp:60] Solver scaffolding done.
I0712 17:19:56.543447 13346 caffe.cpp:219] Starting Optimization
I0712 17:19:56.543669 13346 solver.cpp:279] Solving mnist_siamese_train_test
I0712 17:19:56.543975 13346 solver.cpp:280] Learning Rate Policy: inv
I0712 17:19:56.545269 13346 solver.cpp:337] Iteration 0, Testing net (#0)
I0712 17:20:06.651876 13346 solver.cpp:404]     Test net output #0: loss = 0.114489 (* 1 = 0.114489 loss)
I0712 17:20:06.868371 13346 solver.cpp:228] Iteration 0, loss = 0.105149
I0712 17:20:06.868533 13346 solver.cpp:244]     Train net output #0: loss = 0.105149 (* 1 = 0.105149 loss)
I0712 17:20:06.868577 13346 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0712 17:20:22.525288 13346 solver.cpp:228] Iteration 100, loss = 0.0832905
I0712 17:20:22.525352 13346 solver.cpp:244]     Train net output #0: loss = 0.0832905 (* 1 = 0.0832905 loss)
I0712 17:20:22.525364 13346 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0712 17:20:38.524772 13346 solver.cpp:228] Iteration 200, loss = 0.108488
I0712 17:20:38.524885 13346 solver.cpp:244]     Train net output #0: loss = 0.108488 (* 1 = 0.108488 loss)
I0712 17:20:38.524898 13346 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0712 17:20:54.451320 13346 solver.cpp:228] Iteration 300, loss = 0.0887751
I0712 17:20:54.451386 13346 solver.cpp:244]     Train net output #0: loss = 0.0887751 (* 1 = 0.0887751 loss)
I0712 17:20:54.451397 13346 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0712 17:21:10.335207 13346 solver.cpp:228] Iteration 400, loss = 0.10133
I0712 17:21:10.335563 13346 solver.cpp:244]     Train net output #0: loss = 0.10133 (* 1 = 0.10133 loss)
I0712 17:21:10.335645 13346 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0712 17:21:26.167755 13346 solver.cpp:337] Iteration 500, Testing net (#0)
I0712 17:21:36.138828 13346 solver.cpp:404]     Test net output #0: loss = 0.093421 (* 1 = 0.093421 loss)
I0712 17:21:36.329318 13346 solver.cpp:228] Iteration 500, loss = 0.101869
I0712 17:21:36.329380 13346 solver.cpp:244]     Train net output #0: loss = 0.101869 (* 1 = 0.101869 loss)
I0712 17:21:36.329391 13346 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0712 17:21:52.972133 13346 solver.cpp:228] Iteration 600, loss = 0.0988685
I0712 17:21:52.972326 13346 solver.cpp:244]     Train net output #0: loss = 0.0988685 (* 1 = 0.0988685 loss)
I0712 17:21:52.972342 13346 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0712 17:22:08.906196 13346 solver.cpp:228] Iteration 700, loss = 0.0974301
I0712 17:22:08.906289 13346 solver.cpp:244]     Train net output #0: loss = 0.0974301 (* 1 = 0.0974301 loss)
I0712 17:22:08.906302 13346 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0712 17:22:24.757490 13346 solver.cpp:228] Iteration 800, loss = 0.100493
I0712 17:22:24.758100 13346 solver.cpp:244]     Train net output #0: loss = 0.100493 (* 1 = 0.100493 loss)
I0712 17:22:24.758183 13346 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0712 17:22:40.662847 13346 solver.cpp:228] Iteration 900, loss = 0.094795
I0712 17:22:40.662907 13346 solver.cpp:244]     Train net output #0: loss = 0.094795 (* 1 = 0.094795 loss)
I0712 17:22:40.662919 13346 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0712 17:22:56.365329 13346 solver.cpp:337] Iteration 1000, Testing net (#0)
I0712 17:23:06.478730 13346 solver.cpp:404]     Test net output #0: loss = 0.075837 (* 1 = 0.075837 loss)
I0712 17:23:06.669108 13346 solver.cpp:228] Iteration 1000, loss = 0.0958058
I0712 17:23:06.669407 13346 solver.cpp:244]     Train net output #0: loss = 0.0958058 (* 1 = 0.0958058 loss)
I0712 17:23:06.669452 13346 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0712 17:23:22.405237 13346 solver.cpp:228] Iteration 1100, loss = 0.103824
I0712 17:23:22.405421 13346 solver.cpp:244]     Train net output #0: loss = 0.103824 (* 1 = 0.103824 loss)
I0712 17:23:22.405458 13346 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0712 17:23:38.384325 13346 solver.cpp:228] Iteration 1200, loss = 0.0894571
I0712 17:23:38.384583 13346 solver.cpp:244]     Train net output #0: loss = 0.0894571 (* 1 = 0.0894571 loss)
I0712 17:23:38.384644 13346 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0712 17:23:54.280616 13346 solver.cpp:228] Iteration 1300, loss = 0.119865
I0712 17:23:54.280673 13346 solver.cpp:244]     Train net output #0: loss = 0.119865 (* 1 = 0.119865 loss)
I0712 17:23:54.280684 13346 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0712 17:24:10.203011 13346 solver.cpp:228] Iteration 1400, loss = 0.0900225
I0712 17:24:10.203270 13346 solver.cpp:244]     Train net output #0: loss = 0.0900225 (* 1 = 0.0900225 loss)
I0712 17:24:10.203411 13346 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0712 17:24:25.874831 13346 solver.cpp:337] Iteration 1500, Testing net (#0)
I0712 17:24:35.806375 13346 solver.cpp:404]     Test net output #0: loss = 0.0974341 (* 1 = 0.0974341 loss)
I0712 17:24:35.990123 13346 solver.cpp:228] Iteration 1500, loss = 0.0930919
I0712 17:24:35.990182 13346 solver.cpp:244]     Train net output #0: loss = 0.0930919 (* 1 = 0.0930919 loss)
I0712 17:24:35.990193 13346 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0712 17:24:51.703227 13346 solver.cpp:228] Iteration 1600, loss = 0.0853703
I0712 17:24:51.703480 13346 solver.cpp:244]     Train net output #0: loss = 0.0853703 (* 1 = 0.0853703 loss)
I0712 17:24:51.703495 13346 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0712 17:25:10.842634 13346 solver.cpp:228] Iteration 1700, loss = 0.0797227
I0712 17:25:10.842895 13346 solver.cpp:244]     Train net output #0: loss = 0.0797227 (* 1 = 0.0797227 loss)
I0712 17:25:10.842979 13346 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0712 17:25:26.439286 13346 solver.cpp:228] Iteration 1800, loss = 0.106472
I0712 17:25:26.439381 13346 solver.cpp:244]     Train net output #0: loss = 0.106472 (* 1 = 0.106472 loss)
I0712 17:25:26.439393 13346 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0712 17:25:41.921277 13346 solver.cpp:228] Iteration 1900, loss = 0.0617461
I0712 17:25:41.921527 13346 solver.cpp:244]     Train net output #0: loss = 0.0617461 (* 1 = 0.0617461 loss)
I0712 17:25:41.921612 13346 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0712 17:25:57.324981 13346 solver.cpp:337] Iteration 2000, Testing net (#0)
I0712 17:26:07.075618 13346 solver.cpp:404]     Test net output #0: loss = 0.0897162 (* 1 = 0.0897162 loss)
I0712 17:26:07.267086 13346 solver.cpp:228] Iteration 2000, loss = 0.0713067
I0712 17:26:07.267163 13346 solver.cpp:244]     Train net output #0: loss = 0.0713067 (* 1 = 0.0713067 loss)
I0712 17:26:07.267174 13346 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0712 17:26:22.833643 13346 solver.cpp:228] Iteration 2100, loss = 0.103283
I0712 17:26:22.833705 13346 solver.cpp:244]     Train net output #0: loss = 0.103283 (* 1 = 0.103283 loss)
I0712 17:26:22.833716 13346 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0712 17:26:38.405630 13346 solver.cpp:228] Iteration 2200, loss = 0.0851773
I0712 17:26:38.406131 13346 solver.cpp:244]     Train net output #0: loss = 0.0851773 (* 1 = 0.0851773 loss)
I0712 17:26:38.406215 13346 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0712 17:26:54.036720 13346 solver.cpp:228] Iteration 2300, loss = 0.0848974
I0712 17:26:54.036939 13346 solver.cpp:244]     Train net output #0: loss = 0.0848974 (* 1 = 0.0848974 loss)
I0712 17:26:54.036978 13346 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0712 17:27:09.826149 13346 solver.cpp:228] Iteration 2400, loss = 0.0870605
I0712 17:27:09.826428 13346 solver.cpp:244]     Train net output #0: loss = 0.0870605 (* 1 = 0.0870605 loss)
I0712 17:27:09.826445 13346 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0712 17:27:25.192766 13346 solver.cpp:337] Iteration 2500, Testing net (#0)
I0712 17:27:34.940857 13346 solver.cpp:404]     Test net output #0: loss = 0.100105 (* 1 = 0.100105 loss)
I0712 17:27:35.139056 13346 solver.cpp:228] Iteration 2500, loss = 0.10529
I0712 17:27:35.139197 13346 solver.cpp:244]     Train net output #0: loss = 0.10529 (* 1 = 0.10529 loss)
I0712 17:27:35.139235 13346 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0712 17:27:50.813457 13346 solver.cpp:228] Iteration 2600, loss = 0.0810469
I0712 17:27:50.813673 13346 solver.cpp:244]     Train net output #0: loss = 0.0810469 (* 1 = 0.0810469 loss)
I0712 17:27:50.813688 13346 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0712 17:28:06.556690 13346 solver.cpp:228] Iteration 2700, loss = 0.0845174
I0712 17:28:06.556825 13346 solver.cpp:244]     Train net output #0: loss = 0.0845174 (* 1 = 0.0845174 loss)
I0712 17:28:06.556892 13346 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0712 17:28:22.355233 13346 solver.cpp:228] Iteration 2800, loss = 0.0955517
I0712 17:28:22.355430 13346 solver.cpp:244]     Train net output #0: loss = 0.0955517 (* 1 = 0.0955517 loss)
I0712 17:28:22.355446 13346 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0712 17:28:38.083312 13346 solver.cpp:228] Iteration 2900, loss = 0.106869
I0712 17:28:38.083669 13346 solver.cpp:244]     Train net output #0: loss = 0.106869 (* 1 = 0.106869 loss)
I0712 17:28:38.083714 13346 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0712 17:28:53.635076 13346 solver.cpp:337] Iteration 3000, Testing net (#0)
I0712 17:29:03.554533 13346 solver.cpp:404]     Test net output #0: loss = 0.105542 (* 1 = 0.105542 loss)
I0712 17:29:03.744906 13346 solver.cpp:228] Iteration 3000, loss = 0.0739942
I0712 17:29:03.745067 13346 solver.cpp:244]     Train net output #0: loss = 0.0739942 (* 1 = 0.0739942 loss)
I0712 17:29:03.745105 13346 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0712 17:29:19.555732 13346 solver.cpp:228] Iteration 3100, loss = 0.0778952
I0712 17:29:19.555929 13346 solver.cpp:244]     Train net output #0: loss = 0.0778952 (* 1 = 0.0778952 loss)
I0712 17:29:19.555971 13346 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0712 17:29:35.338343 13346 solver.cpp:228] Iteration 3200, loss = 0.0529027
I0712 17:29:35.338568 13346 solver.cpp:244]     Train net output #0: loss = 0.0529027 (* 1 = 0.0529027 loss)
I0712 17:29:35.338613 13346 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0712 17:29:51.101727 13346 solver.cpp:228] Iteration 3300, loss = 0.0614718
I0712 17:29:51.101948 13346 solver.cpp:244]     Train net output #0: loss = 0.0614718 (* 1 = 0.0614718 loss)
I0712 17:29:51.101990 13346 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0712 17:30:06.891239 13346 solver.cpp:228] Iteration 3400, loss = 0.0623768
I0712 17:30:06.891505 13346 solver.cpp:244]     Train net output #0: loss = 0.0623768 (* 1 = 0.0623768 loss)
I0712 17:30:06.891551 13346 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0712 17:30:22.502583 13346 solver.cpp:337] Iteration 3500, Testing net (#0)
I0712 17:30:32.371601 13346 solver.cpp:404]     Test net output #0: loss = 0.0869713 (* 1 = 0.0869713 loss)
I0712 17:30:32.567631 13346 solver.cpp:228] Iteration 3500, loss = 0.0699017
I0712 17:30:32.567850 13346 solver.cpp:244]     Train net output #0: loss = 0.0699017 (* 1 = 0.0699017 loss)
I0712 17:30:32.567910 13346 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0712 17:30:48.338893 13346 solver.cpp:228] Iteration 3600, loss = 0.0854095
I0712 17:30:48.339215 13346 solver.cpp:244]     Train net output #0: loss = 0.0854095 (* 1 = 0.0854095 loss)
I0712 17:30:48.339232 13346 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0712 17:31:04.145874 13346 solver.cpp:228] Iteration 3700, loss = 0.06387
I0712 17:31:04.146075 13346 solver.cpp:244]     Train net output #0: loss = 0.06387 (* 1 = 0.06387 loss)
I0712 17:31:04.146116 13346 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0712 17:31:19.858944 13346 solver.cpp:228] Iteration 3800, loss = 0.062478
I0712 17:31:19.859238 13346 solver.cpp:244]     Train net output #0: loss = 0.062478 (* 1 = 0.062478 loss)
I0712 17:31:19.859302 13346 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0712 17:31:35.646340 13346 solver.cpp:228] Iteration 3900, loss = 0.0606174
I0712 17:31:35.646561 13346 solver.cpp:244]     Train net output #0: loss = 0.0606174 (* 1 = 0.0606174 loss)
I0712 17:31:35.646605 13346 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0712 17:31:51.207623 13346 solver.cpp:337] Iteration 4000, Testing net (#0)
I0712 17:32:01.145886 13346 solver.cpp:404]     Test net output #0: loss = 0.122672 (* 1 = 0.122672 loss)
I0712 17:32:01.337237 13346 solver.cpp:228] Iteration 4000, loss = 0.059033
I0712 17:32:01.337481 13346 solver.cpp:244]     Train net output #0: loss = 0.059033 (* 1 = 0.059033 loss)
I0712 17:32:01.337543 13346 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0712 17:32:17.056248 13346 solver.cpp:228] Iteration 4100, loss = 0.0728408
I0712 17:32:17.056476 13346 solver.cpp:244]     Train net output #0: loss = 0.0728408 (* 1 = 0.0728408 loss)
I0712 17:32:17.056521 13346 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0712 17:32:32.708269 13346 solver.cpp:228] Iteration 4200, loss = 0.0585464
I0712 17:32:32.708626 13346 solver.cpp:244]     Train net output #0: loss = 0.0585464 (* 1 = 0.0585464 loss)
I0712 17:32:32.708714 13346 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0712 17:32:48.296140 13346 solver.cpp:228] Iteration 4300, loss = 0.0551192
I0712 17:32:48.296197 13346 solver.cpp:244]     Train net output #0: loss = 0.0551192 (* 1 = 0.0551192 loss)
I0712 17:32:48.296208 13346 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0712 17:33:03.941301 13346 solver.cpp:228] Iteration 4400, loss = 0.0509716
I0712 17:33:03.941570 13346 solver.cpp:244]     Train net output #0: loss = 0.0509716 (* 1 = 0.0509716 loss)
I0712 17:33:03.941651 13346 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0712 17:33:19.517385 13346 solver.cpp:337] Iteration 4500, Testing net (#0)
I0712 17:33:29.367234 13346 solver.cpp:404]     Test net output #0: loss = 0.0951518 (* 1 = 0.0951518 loss)
I0712 17:33:29.557864 13346 solver.cpp:228] Iteration 4500, loss = 0.043362
I0712 17:33:29.558087 13346 solver.cpp:244]     Train net output #0: loss = 0.043362 (* 1 = 0.043362 loss)
I0712 17:33:29.558146 13346 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0712 17:33:45.285257 13346 solver.cpp:228] Iteration 4600, loss = 0.0573548
I0712 17:33:45.285464 13346 solver.cpp:244]     Train net output #0: loss = 0.0573548 (* 1 = 0.0573548 loss)
I0712 17:33:45.285503 13346 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0712 17:34:00.985218 13346 solver.cpp:228] Iteration 4700, loss = 0.0585652
I0712 17:34:00.985275 13346 solver.cpp:244]     Train net output #0: loss = 0.0585652 (* 1 = 0.0585652 loss)
I0712 17:34:00.985286 13346 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0712 17:34:16.557055 13346 solver.cpp:228] Iteration 4800, loss = 0.0410312
I0712 17:34:16.557148 13346 solver.cpp:244]     Train net output #0: loss = 0.0410312 (* 1 = 0.0410312 loss)
I0712 17:34:16.557160 13346 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0712 17:34:32.169325 13346 solver.cpp:228] Iteration 4900, loss = 0.0435299
I0712 17:34:32.169390 13346 solver.cpp:244]     Train net output #0: loss = 0.0435299 (* 1 = 0.0435299 loss)
I0712 17:34:32.169402 13346 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0712 17:34:47.570180 13346 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to1c_iter_5000.caffemodel
I0712 17:34:47.579097 13346 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to1c_iter_5000.solverstate
I0712 17:34:47.654198 13346 solver.cpp:317] Iteration 5000, loss = 0.0396135
I0712 17:34:47.654292 13346 solver.cpp:337] Iteration 5000, Testing net (#0)
I0712 17:34:57.566227 13346 solver.cpp:404]     Test net output #0: loss = 0.0952623 (* 1 = 0.0952623 loss)
I0712 17:34:57.566298 13346 solver.cpp:322] Optimization Done.
I0712 17:34:57.566303 13346 caffe.cpp:222] Optimization Done.
