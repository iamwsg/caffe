I0715 11:42:47.114285 18762 caffe.cpp:178] Use CPU.
I0715 11:42:47.114681 18762 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 50
base_lr: 0.01
display: 100
max_iter: 5000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0
snapshot: 1000
snapshot_prefix: "examples/siamese/My_mnist_siamese_0to6_feat_2_sim"
solver_mode: CPU
net: "examples/siamese/mnist_siamese_train_test_sim.prototxt"
I0715 11:42:47.114867 18762 solver.cpp:91] Creating training net from net file: examples/siamese/mnist_siamese_train_test_sim.prototxt
I0715 11:42:47.115500 18762 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer pair_data
I0715 11:42:47.115526 18762 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer accuracy
I0715 11:42:47.115664 18762 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test_sim"
state {
  phase: TRAIN
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/shaogang/caffe/examples/siamese/mnist_siamese_train_leveldb_0to6"
    batch_size: 64
    backend: LEVELDB
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "concat"
  type: "Concat"
  bottom: "feat"
  bottom: "feat_p"
  top: "comb"
  concat_param {
    axis: 1
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "comb"
  top: "fc1"
  param {
    name: "fc1_w"
    lr_mult: 1
  }
  param {
    name: "fc1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_fc1"
  type: "ReLU"
  bottom: "fc1"
  top: "fc1"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "fc1"
  top: "fc2"
  param {
    name: "fc2_w"
    lr_mult: 1
  }
  param {
    name: "fc2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2_fc2"
  type: "ReLU"
  bottom: "fc2"
  top: "fc2"
}
layer {
  name: "fc3"
  type: "InnerProduct"
  bottom: "fc2"
  top: "fc3"
  param {
    name: "fc3_w"
    lr_mult: 1
  }
  param {
    name: "fc3_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc3"
  bottom: "label"
  top: "loss"
}
I0715 11:42:47.115779 18762 layer_factory.hpp:77] Creating layer pair_data
I0715 11:42:47.116268 18762 net.cpp:91] Creating Layer pair_data
I0715 11:42:47.116282 18762 net.cpp:399] pair_data -> pair_data
I0715 11:42:47.116307 18762 net.cpp:399] pair_data -> label
I0715 11:42:47.128381 18766 db_leveldb.cpp:18] Opened leveldb /home/shaogang/caffe/examples/siamese/mnist_siamese_train_leveldb_0to6
I0715 11:42:47.143268 18762 data_layer.cpp:41] output data size: 64,2,28,28
I0715 11:42:47.144322 18762 net.cpp:141] Setting up pair_data
I0715 11:42:47.144397 18762 net.cpp:148] Top shape: 64 2 28 28 (100352)
I0715 11:42:47.144407 18762 net.cpp:148] Top shape: 64 (64)
I0715 11:42:47.144412 18762 net.cpp:156] Memory required for data: 401664
I0715 11:42:47.144426 18762 layer_factory.hpp:77] Creating layer slice_pair
I0715 11:42:47.144456 18762 net.cpp:91] Creating Layer slice_pair
I0715 11:42:47.144484 18762 net.cpp:425] slice_pair <- pair_data
I0715 11:42:47.144501 18762 net.cpp:399] slice_pair -> data
I0715 11:42:47.144518 18762 net.cpp:399] slice_pair -> data_p
I0715 11:42:47.144737 18762 net.cpp:141] Setting up slice_pair
I0715 11:42:47.144770 18762 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0715 11:42:47.144778 18762 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0715 11:42:47.144783 18762 net.cpp:156] Memory required for data: 803072
I0715 11:42:47.144788 18762 layer_factory.hpp:77] Creating layer conv1
I0715 11:42:47.144811 18762 net.cpp:91] Creating Layer conv1
I0715 11:42:47.144858 18762 net.cpp:425] conv1 <- data
I0715 11:42:47.144871 18762 net.cpp:399] conv1 -> conv1
I0715 11:42:47.144948 18762 net.cpp:141] Setting up conv1
I0715 11:42:47.144987 18762 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0715 11:42:47.144995 18762 net.cpp:156] Memory required for data: 3752192
I0715 11:42:47.145010 18762 layer_factory.hpp:77] Creating layer pool1
I0715 11:42:47.145043 18762 net.cpp:91] Creating Layer pool1
I0715 11:42:47.145051 18762 net.cpp:425] pool1 <- conv1
I0715 11:42:47.145057 18762 net.cpp:399] pool1 -> pool1
I0715 11:42:47.145083 18762 net.cpp:141] Setting up pool1
I0715 11:42:47.145112 18762 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0715 11:42:47.145119 18762 net.cpp:156] Memory required for data: 4489472
I0715 11:42:47.145124 18762 layer_factory.hpp:77] Creating layer conv2
I0715 11:42:47.145140 18762 net.cpp:91] Creating Layer conv2
I0715 11:42:47.145146 18762 net.cpp:425] conv2 <- pool1
I0715 11:42:47.145154 18762 net.cpp:399] conv2 -> conv2
I0715 11:42:47.145485 18762 net.cpp:141] Setting up conv2
I0715 11:42:47.145534 18762 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0715 11:42:47.145542 18762 net.cpp:156] Memory required for data: 5308672
I0715 11:42:47.145555 18762 layer_factory.hpp:77] Creating layer pool2
I0715 11:42:47.145565 18762 net.cpp:91] Creating Layer pool2
I0715 11:42:47.145570 18762 net.cpp:425] pool2 <- conv2
I0715 11:42:47.145582 18762 net.cpp:399] pool2 -> pool2
I0715 11:42:47.145596 18762 net.cpp:141] Setting up pool2
I0715 11:42:47.145604 18762 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0715 11:42:47.145609 18762 net.cpp:156] Memory required for data: 5513472
I0715 11:42:47.145615 18762 layer_factory.hpp:77] Creating layer ip1
I0715 11:42:47.145633 18762 net.cpp:91] Creating Layer ip1
I0715 11:42:47.145664 18762 net.cpp:425] ip1 <- pool2
I0715 11:42:47.145680 18762 net.cpp:399] ip1 -> ip1
I0715 11:42:47.154109 18762 net.cpp:141] Setting up ip1
I0715 11:42:47.154274 18762 net.cpp:148] Top shape: 64 500 (32000)
I0715 11:42:47.154335 18762 net.cpp:156] Memory required for data: 5641472
I0715 11:42:47.154427 18762 layer_factory.hpp:77] Creating layer relu1
I0715 11:42:47.154484 18762 net.cpp:91] Creating Layer relu1
I0715 11:42:47.154647 18762 net.cpp:425] relu1 <- ip1
I0715 11:42:47.154803 18762 net.cpp:386] relu1 -> ip1 (in-place)
I0715 11:42:47.155037 18762 net.cpp:141] Setting up relu1
I0715 11:42:47.155163 18762 net.cpp:148] Top shape: 64 500 (32000)
I0715 11:42:47.155509 18762 net.cpp:156] Memory required for data: 5769472
I0715 11:42:47.155710 18762 layer_factory.hpp:77] Creating layer ip2
I0715 11:42:47.155936 18762 net.cpp:91] Creating Layer ip2
I0715 11:42:47.156015 18762 net.cpp:425] ip2 <- ip1
I0715 11:42:47.156075 18762 net.cpp:399] ip2 -> ip2
I0715 11:42:47.156229 18762 net.cpp:141] Setting up ip2
I0715 11:42:47.156291 18762 net.cpp:148] Top shape: 64 10 (640)
I0715 11:42:47.156550 18762 net.cpp:156] Memory required for data: 5772032
I0715 11:42:47.156656 18762 layer_factory.hpp:77] Creating layer feat
I0715 11:42:47.156736 18762 net.cpp:91] Creating Layer feat
I0715 11:42:47.156800 18762 net.cpp:425] feat <- ip2
I0715 11:42:47.156847 18762 net.cpp:399] feat -> feat
I0715 11:42:47.157079 18762 net.cpp:141] Setting up feat
I0715 11:42:47.157153 18762 net.cpp:148] Top shape: 64 2 (128)
I0715 11:42:47.157210 18762 net.cpp:156] Memory required for data: 5772544
I0715 11:42:47.157258 18762 layer_factory.hpp:77] Creating layer conv1_p
I0715 11:42:47.157367 18762 net.cpp:91] Creating Layer conv1_p
I0715 11:42:47.157424 18762 net.cpp:425] conv1_p <- data_p
I0715 11:42:47.157515 18762 net.cpp:399] conv1_p -> conv1_p
I0715 11:42:47.157652 18762 net.cpp:141] Setting up conv1_p
I0715 11:42:47.157722 18762 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0715 11:42:47.157747 18762 net.cpp:156] Memory required for data: 8721664
I0715 11:42:47.157815 18762 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0715 11:42:47.157857 18762 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0715 11:42:47.158004 18762 layer_factory.hpp:77] Creating layer pool1_p
I0715 11:42:47.158078 18762 net.cpp:91] Creating Layer pool1_p
I0715 11:42:47.158149 18762 net.cpp:425] pool1_p <- conv1_p
I0715 11:42:47.158186 18762 net.cpp:399] pool1_p -> pool1_p
I0715 11:42:47.158249 18762 net.cpp:141] Setting up pool1_p
I0715 11:42:47.158419 18762 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0715 11:42:47.158491 18762 net.cpp:156] Memory required for data: 9458944
I0715 11:42:47.158519 18762 layer_factory.hpp:77] Creating layer conv2_p
I0715 11:42:47.158583 18762 net.cpp:91] Creating Layer conv2_p
I0715 11:42:47.158645 18762 net.cpp:425] conv2_p <- pool1_p
I0715 11:42:47.158716 18762 net.cpp:399] conv2_p -> conv2_p
I0715 11:42:47.159852 18762 net.cpp:141] Setting up conv2_p
I0715 11:42:47.160063 18762 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0715 11:42:47.160317 18762 net.cpp:156] Memory required for data: 10278144
I0715 11:42:47.160396 18762 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0715 11:42:47.160498 18762 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0715 11:42:47.160569 18762 layer_factory.hpp:77] Creating layer pool2_p
I0715 11:42:47.160739 18762 net.cpp:91] Creating Layer pool2_p
I0715 11:42:47.160841 18762 net.cpp:425] pool2_p <- conv2_p
I0715 11:42:47.160897 18762 net.cpp:399] pool2_p -> pool2_p
I0715 11:42:47.161031 18762 net.cpp:141] Setting up pool2_p
I0715 11:42:47.161178 18762 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0715 11:42:47.161298 18762 net.cpp:156] Memory required for data: 10482944
I0715 11:42:47.161344 18762 layer_factory.hpp:77] Creating layer ip1_p
I0715 11:42:47.161396 18762 net.cpp:91] Creating Layer ip1_p
I0715 11:42:47.161429 18762 net.cpp:425] ip1_p <- pool2_p
I0715 11:42:47.161541 18762 net.cpp:399] ip1_p -> ip1_p
I0715 11:42:47.166189 18762 net.cpp:141] Setting up ip1_p
I0715 11:42:47.166298 18762 net.cpp:148] Top shape: 64 500 (32000)
I0715 11:42:47.166321 18762 net.cpp:156] Memory required for data: 10610944
I0715 11:42:47.166339 18762 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0715 11:42:47.166357 18762 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0715 11:42:47.166370 18762 layer_factory.hpp:77] Creating layer relu1_p
I0715 11:42:47.166393 18762 net.cpp:91] Creating Layer relu1_p
I0715 11:42:47.166407 18762 net.cpp:425] relu1_p <- ip1_p
I0715 11:42:47.166424 18762 net.cpp:386] relu1_p -> ip1_p (in-place)
I0715 11:42:47.166445 18762 net.cpp:141] Setting up relu1_p
I0715 11:42:47.166460 18762 net.cpp:148] Top shape: 64 500 (32000)
I0715 11:42:47.166473 18762 net.cpp:156] Memory required for data: 10738944
I0715 11:42:47.166486 18762 layer_factory.hpp:77] Creating layer ip2_p
I0715 11:42:47.166509 18762 net.cpp:91] Creating Layer ip2_p
I0715 11:42:47.166524 18762 net.cpp:425] ip2_p <- ip1_p
I0715 11:42:47.166541 18762 net.cpp:399] ip2_p -> ip2_p
I0715 11:42:47.166609 18762 net.cpp:141] Setting up ip2_p
I0715 11:42:47.166626 18762 net.cpp:148] Top shape: 64 10 (640)
I0715 11:42:47.166640 18762 net.cpp:156] Memory required for data: 10741504
I0715 11:42:47.166664 18762 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0715 11:42:47.166682 18762 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0715 11:42:47.166697 18762 layer_factory.hpp:77] Creating layer feat_p
I0715 11:42:47.166712 18762 net.cpp:91] Creating Layer feat_p
I0715 11:42:47.166726 18762 net.cpp:425] feat_p <- ip2_p
I0715 11:42:47.166743 18762 net.cpp:399] feat_p -> feat_p
I0715 11:42:47.166766 18762 net.cpp:141] Setting up feat_p
I0715 11:42:47.166784 18762 net.cpp:148] Top shape: 64 2 (128)
I0715 11:42:47.166796 18762 net.cpp:156] Memory required for data: 10742016
I0715 11:42:47.166810 18762 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0715 11:42:47.166823 18762 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0715 11:42:47.166846 18762 layer_factory.hpp:77] Creating layer concat
I0715 11:42:47.166880 18762 net.cpp:91] Creating Layer concat
I0715 11:42:47.166894 18762 net.cpp:425] concat <- feat
I0715 11:42:47.166908 18762 net.cpp:425] concat <- feat_p
I0715 11:42:47.166923 18762 net.cpp:399] concat -> comb
I0715 11:42:47.166945 18762 net.cpp:141] Setting up concat
I0715 11:42:47.166961 18762 net.cpp:148] Top shape: 64 4 (256)
I0715 11:42:47.166975 18762 net.cpp:156] Memory required for data: 10743040
I0715 11:42:47.166987 18762 layer_factory.hpp:77] Creating layer fc1
I0715 11:42:47.167007 18762 net.cpp:91] Creating Layer fc1
I0715 11:42:47.167022 18762 net.cpp:425] fc1 <- comb
I0715 11:42:47.167040 18762 net.cpp:399] fc1 -> fc1
I0715 11:42:47.167491 18762 net.cpp:141] Setting up fc1
I0715 11:42:47.167573 18762 net.cpp:148] Top shape: 64 100 (6400)
I0715 11:42:47.167590 18762 net.cpp:156] Memory required for data: 10768640
I0715 11:42:47.167608 18762 layer_factory.hpp:77] Creating layer relu1_fc1
I0715 11:42:47.167630 18762 net.cpp:91] Creating Layer relu1_fc1
I0715 11:42:47.167645 18762 net.cpp:425] relu1_fc1 <- fc1
I0715 11:42:47.167660 18762 net.cpp:386] relu1_fc1 -> fc1 (in-place)
I0715 11:42:47.167681 18762 net.cpp:141] Setting up relu1_fc1
I0715 11:42:47.167703 18762 net.cpp:148] Top shape: 64 100 (6400)
I0715 11:42:47.167719 18762 net.cpp:156] Memory required for data: 10794240
I0715 11:42:47.167732 18762 layer_factory.hpp:77] Creating layer fc2
I0715 11:42:47.167749 18762 net.cpp:91] Creating Layer fc2
I0715 11:42:47.167762 18762 net.cpp:425] fc2 <- fc1
I0715 11:42:47.167778 18762 net.cpp:399] fc2 -> fc2
I0715 11:42:47.167837 18762 net.cpp:141] Setting up fc2
I0715 11:42:47.167855 18762 net.cpp:148] Top shape: 64 50 (3200)
I0715 11:42:47.167867 18762 net.cpp:156] Memory required for data: 10807040
I0715 11:42:47.167882 18762 layer_factory.hpp:77] Creating layer relu2_fc2
I0715 11:42:47.167897 18762 net.cpp:91] Creating Layer relu2_fc2
I0715 11:42:47.167911 18762 net.cpp:425] relu2_fc2 <- fc2
I0715 11:42:47.167923 18762 net.cpp:386] relu2_fc2 -> fc2 (in-place)
I0715 11:42:47.167939 18762 net.cpp:141] Setting up relu2_fc2
I0715 11:42:47.167953 18762 net.cpp:148] Top shape: 64 50 (3200)
I0715 11:42:47.167965 18762 net.cpp:156] Memory required for data: 10819840
I0715 11:42:47.167978 18762 layer_factory.hpp:77] Creating layer fc3
I0715 11:42:47.167994 18762 net.cpp:91] Creating Layer fc3
I0715 11:42:47.168006 18762 net.cpp:425] fc3 <- fc2
I0715 11:42:47.168023 18762 net.cpp:399] fc3 -> fc3
I0715 11:42:47.168050 18762 net.cpp:141] Setting up fc3
I0715 11:42:47.168069 18762 net.cpp:148] Top shape: 64 2 (128)
I0715 11:42:47.168082 18762 net.cpp:156] Memory required for data: 10820352
I0715 11:42:47.168097 18762 layer_factory.hpp:77] Creating layer loss
I0715 11:42:47.168120 18762 net.cpp:91] Creating Layer loss
I0715 11:42:47.168134 18762 net.cpp:425] loss <- fc3
I0715 11:42:47.168148 18762 net.cpp:425] loss <- label
I0715 11:42:47.168172 18762 net.cpp:399] loss -> loss
I0715 11:42:47.168198 18762 layer_factory.hpp:77] Creating layer loss
I0715 11:42:47.168236 18762 net.cpp:141] Setting up loss
I0715 11:42:47.168253 18762 net.cpp:148] Top shape: (1)
I0715 11:42:47.168267 18762 net.cpp:151]     with loss weight 1
I0715 11:42:47.168301 18762 net.cpp:156] Memory required for data: 10820356
I0715 11:42:47.168315 18762 net.cpp:217] loss needs backward computation.
I0715 11:42:47.168329 18762 net.cpp:217] fc3 needs backward computation.
I0715 11:42:47.168342 18762 net.cpp:217] relu2_fc2 needs backward computation.
I0715 11:42:47.168355 18762 net.cpp:217] fc2 needs backward computation.
I0715 11:42:47.168367 18762 net.cpp:217] relu1_fc1 needs backward computation.
I0715 11:42:47.168380 18762 net.cpp:217] fc1 needs backward computation.
I0715 11:42:47.168397 18762 net.cpp:217] concat needs backward computation.
I0715 11:42:47.168416 18762 net.cpp:217] feat_p needs backward computation.
I0715 11:42:47.168429 18762 net.cpp:217] ip2_p needs backward computation.
I0715 11:42:47.168442 18762 net.cpp:217] relu1_p needs backward computation.
I0715 11:42:47.168464 18762 net.cpp:217] ip1_p needs backward computation.
I0715 11:42:47.168489 18762 net.cpp:217] pool2_p needs backward computation.
I0715 11:42:47.168503 18762 net.cpp:217] conv2_p needs backward computation.
I0715 11:42:47.168516 18762 net.cpp:217] pool1_p needs backward computation.
I0715 11:42:47.168529 18762 net.cpp:217] conv1_p needs backward computation.
I0715 11:42:47.168542 18762 net.cpp:217] feat needs backward computation.
I0715 11:42:47.168555 18762 net.cpp:217] ip2 needs backward computation.
I0715 11:42:47.168570 18762 net.cpp:217] relu1 needs backward computation.
I0715 11:42:47.168581 18762 net.cpp:217] ip1 needs backward computation.
I0715 11:42:47.168594 18762 net.cpp:217] pool2 needs backward computation.
I0715 11:42:47.168607 18762 net.cpp:217] conv2 needs backward computation.
I0715 11:42:47.168620 18762 net.cpp:217] pool1 needs backward computation.
I0715 11:42:47.168635 18762 net.cpp:217] conv1 needs backward computation.
I0715 11:42:47.168649 18762 net.cpp:219] slice_pair does not need backward computation.
I0715 11:42:47.168663 18762 net.cpp:219] pair_data does not need backward computation.
I0715 11:42:47.168675 18762 net.cpp:261] This network produces output loss
I0715 11:42:47.168849 18762 net.cpp:274] Network initialization done.
I0715 11:42:47.169579 18762 solver.cpp:181] Creating test net (#0) specified by net file: examples/siamese/mnist_siamese_train_test_sim.prototxt
I0715 11:42:47.169745 18762 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0715 11:42:47.170189 18762 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test_sim"
state {
  phase: TEST
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "/home/shaogang/caffe/examples/siamese/mnist_siamese_test_leveldb_789"
    batch_size: 100
    backend: LEVELDB
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "concat"
  type: "Concat"
  bottom: "feat"
  bottom: "feat_p"
  top: "comb"
  concat_param {
    axis: 1
  }
}
layer {
  name: "fc1"
  type: "InnerProduct"
  bottom: "comb"
  top: "fc1"
  param {
    name: "fc1_w"
    lr_mult: 1
  }
  param {
    name: "fc1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 100
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_fc1"
  type: "ReLU"
  bottom: "fc1"
  top: "fc1"
}
layer {
  name: "fc2"
  type: "InnerProduct"
  bottom: "fc1"
  top: "fc2"
  param {
    name: "fc2_w"
    lr_mult: 1
  }
  param {
    name: "fc2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 50
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu2_fc2"
  type: "ReLU"
  bottom: "fc2"
  top: "fc2"
}
layer {
  name: "fc3"
  type: "InnerProduct"
  bottom: "fc2"
  top: "fc3"
  param {
    name: "fc3_w"
    lr_mult: 1
  }
  param {
    name: "fc3_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc3"
  bottom: "label"
  top: "loss"
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc3"
  bottom: "label"
  top: "accuracy"
  include {
    phase: TEST
  }
}
I0715 11:42:47.172260 18762 layer_factory.hpp:77] Creating layer pair_data
I0715 11:42:47.172482 18762 net.cpp:91] Creating Layer pair_data
I0715 11:42:47.172515 18762 net.cpp:399] pair_data -> pair_data
I0715 11:42:47.172547 18762 net.cpp:399] pair_data -> label
I0715 11:42:47.184653 18768 db_leveldb.cpp:18] Opened leveldb /home/shaogang/caffe/examples/siamese/mnist_siamese_test_leveldb_789
I0715 11:42:47.185111 18762 data_layer.cpp:41] output data size: 100,2,28,28
I0715 11:42:47.186046 18762 net.cpp:141] Setting up pair_data
I0715 11:42:47.186075 18762 net.cpp:148] Top shape: 100 2 28 28 (156800)
I0715 11:42:47.186117 18762 net.cpp:148] Top shape: 100 (100)
I0715 11:42:47.186122 18762 net.cpp:156] Memory required for data: 627600
I0715 11:42:47.186132 18762 layer_factory.hpp:77] Creating layer label_pair_data_1_split
I0715 11:42:47.186151 18762 net.cpp:91] Creating Layer label_pair_data_1_split
I0715 11:42:47.186156 18762 net.cpp:425] label_pair_data_1_split <- label
I0715 11:42:47.186167 18762 net.cpp:399] label_pair_data_1_split -> label_pair_data_1_split_0
I0715 11:42:47.186179 18762 net.cpp:399] label_pair_data_1_split -> label_pair_data_1_split_1
I0715 11:42:47.186192 18762 net.cpp:141] Setting up label_pair_data_1_split
I0715 11:42:47.186198 18762 net.cpp:148] Top shape: 100 (100)
I0715 11:42:47.186203 18762 net.cpp:148] Top shape: 100 (100)
I0715 11:42:47.186208 18762 net.cpp:156] Memory required for data: 628400
I0715 11:42:47.186211 18762 layer_factory.hpp:77] Creating layer slice_pair
I0715 11:42:47.186223 18762 net.cpp:91] Creating Layer slice_pair
I0715 11:42:47.186228 18762 net.cpp:425] slice_pair <- pair_data
I0715 11:42:47.186233 18762 net.cpp:399] slice_pair -> data
I0715 11:42:47.186244 18762 net.cpp:399] slice_pair -> data_p
I0715 11:42:47.186254 18762 net.cpp:141] Setting up slice_pair
I0715 11:42:47.186262 18762 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0715 11:42:47.186269 18762 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0715 11:42:47.186274 18762 net.cpp:156] Memory required for data: 1255600
I0715 11:42:47.186278 18762 layer_factory.hpp:77] Creating layer conv1
I0715 11:42:47.186295 18762 net.cpp:91] Creating Layer conv1
I0715 11:42:47.186300 18762 net.cpp:425] conv1 <- data
I0715 11:42:47.186307 18762 net.cpp:399] conv1 -> conv1
I0715 11:42:47.186349 18762 net.cpp:141] Setting up conv1
I0715 11:42:47.186357 18762 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0715 11:42:47.186362 18762 net.cpp:156] Memory required for data: 5863600
I0715 11:42:47.186373 18762 layer_factory.hpp:77] Creating layer pool1
I0715 11:42:47.186384 18762 net.cpp:91] Creating Layer pool1
I0715 11:42:47.186389 18762 net.cpp:425] pool1 <- conv1
I0715 11:42:47.186396 18762 net.cpp:399] pool1 -> pool1
I0715 11:42:47.186408 18762 net.cpp:141] Setting up pool1
I0715 11:42:47.186414 18762 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0715 11:42:47.186419 18762 net.cpp:156] Memory required for data: 7015600
I0715 11:42:47.186424 18762 layer_factory.hpp:77] Creating layer conv2
I0715 11:42:47.186435 18762 net.cpp:91] Creating Layer conv2
I0715 11:42:47.186440 18762 net.cpp:425] conv2 <- pool1
I0715 11:42:47.186453 18762 net.cpp:399] conv2 -> conv2
I0715 11:42:47.186753 18762 net.cpp:141] Setting up conv2
I0715 11:42:47.186764 18762 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0715 11:42:47.186769 18762 net.cpp:156] Memory required for data: 8295600
I0715 11:42:47.186777 18762 layer_factory.hpp:77] Creating layer pool2
I0715 11:42:47.186784 18762 net.cpp:91] Creating Layer pool2
I0715 11:42:47.186789 18762 net.cpp:425] pool2 <- conv2
I0715 11:42:47.186796 18762 net.cpp:399] pool2 -> pool2
I0715 11:42:47.186806 18762 net.cpp:141] Setting up pool2
I0715 11:42:47.186812 18762 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0715 11:42:47.186817 18762 net.cpp:156] Memory required for data: 8615600
I0715 11:42:47.186822 18762 layer_factory.hpp:77] Creating layer ip1
I0715 11:42:47.186832 18762 net.cpp:91] Creating Layer ip1
I0715 11:42:47.186837 18762 net.cpp:425] ip1 <- pool2
I0715 11:42:47.186844 18762 net.cpp:399] ip1 -> ip1
I0715 11:42:47.190392 18762 net.cpp:141] Setting up ip1
I0715 11:42:47.191395 18769 blocking_queue.cpp:50] Waiting for data
I0715 11:42:47.194619 18762 net.cpp:148] Top shape: 100 500 (50000)
I0715 11:42:47.194954 18762 net.cpp:156] Memory required for data: 8815600
I0715 11:42:47.195224 18762 layer_factory.hpp:77] Creating layer relu1
I0715 11:42:47.195263 18762 net.cpp:91] Creating Layer relu1
I0715 11:42:47.195281 18762 net.cpp:425] relu1 <- ip1
I0715 11:42:47.195297 18762 net.cpp:386] relu1 -> ip1 (in-place)
I0715 11:42:47.195318 18762 net.cpp:141] Setting up relu1
I0715 11:42:47.195345 18762 net.cpp:148] Top shape: 100 500 (50000)
I0715 11:42:47.195368 18762 net.cpp:156] Memory required for data: 9015600
I0715 11:42:47.195380 18762 layer_factory.hpp:77] Creating layer ip2
I0715 11:42:47.195399 18762 net.cpp:91] Creating Layer ip2
I0715 11:42:47.195412 18762 net.cpp:425] ip2 <- ip1
I0715 11:42:47.195427 18762 net.cpp:399] ip2 -> ip2
I0715 11:42:47.195502 18762 net.cpp:141] Setting up ip2
I0715 11:42:47.195519 18762 net.cpp:148] Top shape: 100 10 (1000)
I0715 11:42:47.195533 18762 net.cpp:156] Memory required for data: 9019600
I0715 11:42:47.195547 18762 layer_factory.hpp:77] Creating layer feat
I0715 11:42:47.195564 18762 net.cpp:91] Creating Layer feat
I0715 11:42:47.195577 18762 net.cpp:425] feat <- ip2
I0715 11:42:47.195592 18762 net.cpp:399] feat -> feat
I0715 11:42:47.195613 18762 net.cpp:141] Setting up feat
I0715 11:42:47.195629 18762 net.cpp:148] Top shape: 100 2 (200)
I0715 11:42:47.195641 18762 net.cpp:156] Memory required for data: 9020400
I0715 11:42:47.195658 18762 layer_factory.hpp:77] Creating layer conv1_p
I0715 11:42:47.195678 18762 net.cpp:91] Creating Layer conv1_p
I0715 11:42:47.195693 18762 net.cpp:425] conv1_p <- data_p
I0715 11:42:47.195708 18762 net.cpp:399] conv1_p -> conv1_p
I0715 11:42:47.195746 18762 net.cpp:141] Setting up conv1_p
I0715 11:42:47.195765 18762 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0715 11:42:47.195776 18762 net.cpp:156] Memory required for data: 13628400
I0715 11:42:47.195791 18762 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0715 11:42:47.195803 18762 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0715 11:42:47.195816 18762 layer_factory.hpp:77] Creating layer pool1_p
I0715 11:42:47.195832 18762 net.cpp:91] Creating Layer pool1_p
I0715 11:42:47.195845 18762 net.cpp:425] pool1_p <- conv1_p
I0715 11:42:47.195863 18762 net.cpp:399] pool1_p -> pool1_p
I0715 11:42:47.195881 18762 net.cpp:141] Setting up pool1_p
I0715 11:42:47.195897 18762 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0715 11:42:47.195909 18762 net.cpp:156] Memory required for data: 14780400
I0715 11:42:47.195921 18762 layer_factory.hpp:77] Creating layer conv2_p
I0715 11:42:47.195941 18762 net.cpp:91] Creating Layer conv2_p
I0715 11:42:47.195955 18762 net.cpp:425] conv2_p <- pool1_p
I0715 11:42:47.195971 18762 net.cpp:399] conv2_p -> conv2_p
I0715 11:42:47.196185 18762 net.cpp:141] Setting up conv2_p
I0715 11:42:47.197917 18762 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0715 11:42:47.202765 18762 net.cpp:156] Memory required for data: 16060400
I0715 11:42:47.203322 18762 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0715 11:42:47.205026 18762 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0715 11:42:47.206342 18762 layer_factory.hpp:77] Creating layer pool2_p
I0715 11:42:47.206431 18762 net.cpp:91] Creating Layer pool2_p
I0715 11:42:47.206509 18762 net.cpp:425] pool2_p <- conv2_p
I0715 11:42:47.206570 18762 net.cpp:399] pool2_p -> pool2_p
I0715 11:42:47.206718 18762 net.cpp:141] Setting up pool2_p
I0715 11:42:47.206776 18762 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0715 11:42:47.206799 18762 net.cpp:156] Memory required for data: 16380400
I0715 11:42:47.206836 18762 layer_factory.hpp:77] Creating layer ip1_p
I0715 11:42:47.206872 18762 net.cpp:91] Creating Layer ip1_p
I0715 11:42:47.206902 18762 net.cpp:425] ip1_p <- pool2_p
I0715 11:42:47.206955 18762 net.cpp:399] ip1_p -> ip1_p
I0715 11:42:47.222939 18762 net.cpp:141] Setting up ip1_p
I0715 11:42:47.223275 18762 net.cpp:148] Top shape: 100 500 (50000)
I0715 11:42:47.223306 18762 net.cpp:156] Memory required for data: 16580400
I0715 11:42:47.223333 18762 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0715 11:42:47.223358 18762 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0715 11:42:47.223381 18762 layer_factory.hpp:77] Creating layer relu1_p
I0715 11:42:47.223414 18762 net.cpp:91] Creating Layer relu1_p
I0715 11:42:47.223449 18762 net.cpp:425] relu1_p <- ip1_p
I0715 11:42:47.223489 18762 net.cpp:386] relu1_p -> ip1_p (in-place)
I0715 11:42:47.223521 18762 net.cpp:141] Setting up relu1_p
I0715 11:42:47.223546 18762 net.cpp:148] Top shape: 100 500 (50000)
I0715 11:42:47.223565 18762 net.cpp:156] Memory required for data: 16780400
I0715 11:42:47.223585 18762 layer_factory.hpp:77] Creating layer ip2_p
I0715 11:42:47.223616 18762 net.cpp:91] Creating Layer ip2_p
I0715 11:42:47.223639 18762 net.cpp:425] ip2_p <- ip1_p
I0715 11:42:47.223666 18762 net.cpp:399] ip2_p -> ip2_p
I0715 11:42:47.223767 18762 net.cpp:141] Setting up ip2_p
I0715 11:42:47.223794 18762 net.cpp:148] Top shape: 100 10 (1000)
I0715 11:42:47.223814 18762 net.cpp:156] Memory required for data: 16784400
I0715 11:42:47.223839 18762 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0715 11:42:47.223862 18762 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0715 11:42:47.223883 18762 layer_factory.hpp:77] Creating layer feat_p
I0715 11:42:47.223912 18762 net.cpp:91] Creating Layer feat_p
I0715 11:42:47.223934 18762 net.cpp:425] feat_p <- ip2_p
I0715 11:42:47.223958 18762 net.cpp:399] feat_p -> feat_p
I0715 11:42:47.223994 18762 net.cpp:141] Setting up feat_p
I0715 11:42:47.224020 18762 net.cpp:148] Top shape: 100 2 (200)
I0715 11:42:47.224040 18762 net.cpp:156] Memory required for data: 16785200
I0715 11:42:47.224061 18762 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0715 11:42:47.224081 18762 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0715 11:42:47.224102 18762 layer_factory.hpp:77] Creating layer concat
I0715 11:42:47.224135 18762 net.cpp:91] Creating Layer concat
I0715 11:42:47.224158 18762 net.cpp:425] concat <- feat
I0715 11:42:47.224469 18762 net.cpp:425] concat <- feat_p
I0715 11:42:47.224539 18762 net.cpp:399] concat -> comb
I0715 11:42:47.224582 18762 net.cpp:141] Setting up concat
I0715 11:42:47.224611 18762 net.cpp:148] Top shape: 100 4 (400)
I0715 11:42:47.224632 18762 net.cpp:156] Memory required for data: 16786800
I0715 11:42:47.224653 18762 layer_factory.hpp:77] Creating layer fc1
I0715 11:42:47.224684 18762 net.cpp:91] Creating Layer fc1
I0715 11:42:47.224706 18762 net.cpp:425] fc1 <- comb
I0715 11:42:47.224743 18762 net.cpp:399] fc1 -> fc1
I0715 11:42:47.224800 18762 net.cpp:141] Setting up fc1
I0715 11:42:47.224828 18762 net.cpp:148] Top shape: 100 100 (10000)
I0715 11:42:47.224846 18762 net.cpp:156] Memory required for data: 16826800
I0715 11:42:47.224872 18762 layer_factory.hpp:77] Creating layer relu1_fc1
I0715 11:42:47.224900 18762 net.cpp:91] Creating Layer relu1_fc1
I0715 11:42:47.224922 18762 net.cpp:425] relu1_fc1 <- fc1
I0715 11:42:47.224946 18762 net.cpp:386] relu1_fc1 -> fc1 (in-place)
I0715 11:42:47.224972 18762 net.cpp:141] Setting up relu1_fc1
I0715 11:42:47.224993 18762 net.cpp:148] Top shape: 100 100 (10000)
I0715 11:42:47.225013 18762 net.cpp:156] Memory required for data: 16866800
I0715 11:42:47.225034 18762 layer_factory.hpp:77] Creating layer fc2
I0715 11:42:47.225062 18762 net.cpp:91] Creating Layer fc2
I0715 11:42:47.225085 18762 net.cpp:425] fc2 <- fc1
I0715 11:42:47.225108 18762 net.cpp:399] fc2 -> fc2
I0715 11:42:47.225198 18762 net.cpp:141] Setting up fc2
I0715 11:42:47.225226 18762 net.cpp:148] Top shape: 100 50 (5000)
I0715 11:42:47.225245 18762 net.cpp:156] Memory required for data: 16886800
I0715 11:42:47.225268 18762 layer_factory.hpp:77] Creating layer relu2_fc2
I0715 11:42:47.225292 18762 net.cpp:91] Creating Layer relu2_fc2
I0715 11:42:47.225312 18762 net.cpp:425] relu2_fc2 <- fc2
I0715 11:42:47.225333 18762 net.cpp:386] relu2_fc2 -> fc2 (in-place)
I0715 11:42:47.225358 18762 net.cpp:141] Setting up relu2_fc2
I0715 11:42:47.225379 18762 net.cpp:148] Top shape: 100 50 (5000)
I0715 11:42:47.225397 18762 net.cpp:156] Memory required for data: 16906800
I0715 11:42:47.225417 18762 layer_factory.hpp:77] Creating layer fc3
I0715 11:42:47.225441 18762 net.cpp:91] Creating Layer fc3
I0715 11:42:47.225476 18762 net.cpp:425] fc3 <- fc2
I0715 11:42:47.225517 18762 net.cpp:399] fc3 -> fc3
I0715 11:42:47.225555 18762 net.cpp:141] Setting up fc3
I0715 11:42:47.225580 18762 net.cpp:148] Top shape: 100 2 (200)
I0715 11:42:47.225617 18762 net.cpp:156] Memory required for data: 16907600
I0715 11:42:47.225646 18762 layer_factory.hpp:77] Creating layer fc3_fc3_0_split
I0715 11:42:47.225675 18762 net.cpp:91] Creating Layer fc3_fc3_0_split
I0715 11:42:47.225699 18762 net.cpp:425] fc3_fc3_0_split <- fc3
I0715 11:42:47.225724 18762 net.cpp:399] fc3_fc3_0_split -> fc3_fc3_0_split_0
I0715 11:42:47.225764 18762 net.cpp:399] fc3_fc3_0_split -> fc3_fc3_0_split_1
I0715 11:42:47.225795 18762 net.cpp:141] Setting up fc3_fc3_0_split
I0715 11:42:47.225819 18762 net.cpp:148] Top shape: 100 2 (200)
I0715 11:42:47.225841 18762 net.cpp:148] Top shape: 100 2 (200)
I0715 11:42:47.225859 18762 net.cpp:156] Memory required for data: 16909200
I0715 11:42:47.225879 18762 layer_factory.hpp:77] Creating layer loss
I0715 11:42:47.225903 18762 net.cpp:91] Creating Layer loss
I0715 11:42:47.225924 18762 net.cpp:425] loss <- fc3_fc3_0_split_0
I0715 11:42:47.225945 18762 net.cpp:425] loss <- label_pair_data_1_split_0
I0715 11:42:47.225971 18762 net.cpp:399] loss -> loss
I0715 11:42:47.226001 18762 layer_factory.hpp:77] Creating layer loss
I0715 11:42:47.226043 18762 net.cpp:141] Setting up loss
I0715 11:42:47.226075 18762 net.cpp:148] Top shape: (1)
I0715 11:42:47.226104 18762 net.cpp:151]     with loss weight 1
I0715 11:42:47.226140 18762 net.cpp:156] Memory required for data: 16909204
I0715 11:42:47.226161 18762 layer_factory.hpp:77] Creating layer accuracy
I0715 11:42:47.226184 18762 net.cpp:91] Creating Layer accuracy
I0715 11:42:47.226204 18762 net.cpp:425] accuracy <- fc3_fc3_0_split_1
I0715 11:42:47.226225 18762 net.cpp:425] accuracy <- label_pair_data_1_split_1
I0715 11:42:47.226249 18762 net.cpp:399] accuracy -> accuracy
I0715 11:42:47.226276 18762 net.cpp:141] Setting up accuracy
I0715 11:42:47.226299 18762 net.cpp:148] Top shape: (1)
I0715 11:42:47.226318 18762 net.cpp:156] Memory required for data: 16909208
I0715 11:42:47.226338 18762 net.cpp:219] accuracy does not need backward computation.
I0715 11:42:47.226358 18762 net.cpp:217] loss needs backward computation.
I0715 11:42:47.226379 18762 net.cpp:217] fc3_fc3_0_split needs backward computation.
I0715 11:42:47.226400 18762 net.cpp:217] fc3 needs backward computation.
I0715 11:42:47.226420 18762 net.cpp:217] relu2_fc2 needs backward computation.
I0715 11:42:47.226439 18762 net.cpp:217] fc2 needs backward computation.
I0715 11:42:47.226459 18762 net.cpp:217] relu1_fc1 needs backward computation.
I0715 11:42:47.226480 18762 net.cpp:217] fc1 needs backward computation.
I0715 11:42:47.226500 18762 net.cpp:217] concat needs backward computation.
I0715 11:42:47.226521 18762 net.cpp:217] feat_p needs backward computation.
I0715 11:42:47.226541 18762 net.cpp:217] ip2_p needs backward computation.
I0715 11:42:47.226562 18762 net.cpp:217] relu1_p needs backward computation.
I0715 11:42:47.226582 18762 net.cpp:217] ip1_p needs backward computation.
I0715 11:42:47.226701 18762 net.cpp:217] pool2_p needs backward computation.
I0715 11:42:47.226732 18762 net.cpp:217] conv2_p needs backward computation.
I0715 11:42:47.226763 18762 net.cpp:217] pool1_p needs backward computation.
I0715 11:42:47.226788 18762 net.cpp:217] conv1_p needs backward computation.
I0715 11:42:47.226864 18762 net.cpp:217] feat needs backward computation.
I0715 11:42:47.226897 18762 net.cpp:217] ip2 needs backward computation.
I0715 11:42:47.226918 18762 net.cpp:217] relu1 needs backward computation.
I0715 11:42:47.226938 18762 net.cpp:217] ip1 needs backward computation.
I0715 11:42:47.226958 18762 net.cpp:217] pool2 needs backward computation.
I0715 11:42:47.226979 18762 net.cpp:217] conv2 needs backward computation.
I0715 11:42:47.226997 18762 net.cpp:217] pool1 needs backward computation.
I0715 11:42:47.227017 18762 net.cpp:217] conv1 needs backward computation.
I0715 11:42:47.227040 18762 net.cpp:219] slice_pair does not need backward computation.
I0715 11:42:47.227084 18762 net.cpp:219] label_pair_data_1_split does not need backward computation.
I0715 11:42:47.227107 18762 net.cpp:219] pair_data does not need backward computation.
I0715 11:42:47.227126 18762 net.cpp:261] This network produces output accuracy
I0715 11:42:47.227149 18762 net.cpp:261] This network produces output loss
I0715 11:42:47.227208 18762 net.cpp:274] Network initialization done.
I0715 11:42:47.227442 18762 solver.cpp:60] Solver scaffolding done.
I0715 11:42:47.227526 18762 caffe.cpp:219] Starting Optimization
I0715 11:42:47.227625 18762 solver.cpp:279] Solving mnist_siamese_train_test_sim
I0715 11:42:47.227651 18762 solver.cpp:280] Learning Rate Policy: inv
I0715 11:42:47.228366 18762 solver.cpp:337] Iteration 0, Testing net (#0)
I0715 11:42:57.583096 18762 solver.cpp:404]     Test net output #0: accuracy = 0.65
I0715 11:42:57.583255 18762 solver.cpp:404]     Test net output #1: loss = 0.669385 (* 1 = 0.669385 loss)
I0715 11:42:57.792037 18762 solver.cpp:228] Iteration 0, loss = 0.653368
I0715 11:42:57.792179 18762 solver.cpp:244]     Train net output #0: loss = 0.653368 (* 1 = 0.653368 loss)
I0715 11:42:57.792218 18762 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0715 11:43:05.647346 18762 solver.cpp:337] Iteration 50, Testing net (#0)
I0715 11:43:15.799826 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6653
I0715 11:43:15.800029 18762 solver.cpp:404]     Test net output #1: loss = 0.828376 (* 1 = 0.828376 loss)
I0715 11:43:23.782874 18762 solver.cpp:337] Iteration 100, Testing net (#0)
I0715 11:43:33.984064 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6653
I0715 11:43:33.984211 18762 solver.cpp:404]     Test net output #1: loss = 0.752242 (* 1 = 0.752242 loss)
I0715 11:43:34.202880 18762 solver.cpp:228] Iteration 100, loss = 0.302314
I0715 11:43:34.203009 18762 solver.cpp:244]     Train net output #0: loss = 0.302314 (* 1 = 0.302314 loss)
I0715 11:43:34.203039 18762 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0715 11:43:43.823189 18762 solver.cpp:337] Iteration 150, Testing net (#0)
I0715 11:43:55.759095 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6653
I0715 11:43:55.764976 18762 solver.cpp:404]     Test net output #1: loss = 0.717673 (* 1 = 0.717673 loss)
I0715 11:44:04.274319 18762 solver.cpp:337] Iteration 200, Testing net (#0)
I0715 11:44:16.327029 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6653
I0715 11:44:16.327153 18762 solver.cpp:404]     Test net output #1: loss = 0.738501 (* 1 = 0.738501 loss)
I0715 11:44:16.537303 18762 solver.cpp:228] Iteration 200, loss = 0.440529
I0715 11:44:16.537361 18762 solver.cpp:244]     Train net output #0: loss = 0.440529 (* 1 = 0.440529 loss)
I0715 11:44:16.537371 18762 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0715 11:44:26.195361 18762 solver.cpp:337] Iteration 250, Testing net (#0)
I0715 11:44:37.293015 18762 solver.cpp:404]     Test net output #0: accuracy = 0.665
I0715 11:44:37.293254 18762 solver.cpp:404]     Test net output #1: loss = 0.756458 (* 1 = 0.756458 loss)
I0715 11:44:45.336266 18762 solver.cpp:337] Iteration 300, Testing net (#0)
I0715 11:44:56.123744 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6659
I0715 11:44:56.123858 18762 solver.cpp:404]     Test net output #1: loss = 0.707592 (* 1 = 0.707592 loss)
I0715 11:44:56.322031 18762 solver.cpp:228] Iteration 300, loss = 0.287761
I0715 11:44:56.322419 18762 solver.cpp:244]     Train net output #0: loss = 0.287761 (* 1 = 0.287761 loss)
I0715 11:44:56.322468 18762 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0715 11:45:04.380048 18762 solver.cpp:337] Iteration 350, Testing net (#0)
I0715 11:45:14.680747 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6652
I0715 11:45:14.681037 18762 solver.cpp:404]     Test net output #1: loss = 0.785894 (* 1 = 0.785894 loss)
I0715 11:45:22.690428 18762 solver.cpp:337] Iteration 400, Testing net (#0)
I0715 11:45:32.895817 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6671
I0715 11:45:32.896067 18762 solver.cpp:404]     Test net output #1: loss = 0.749092 (* 1 = 0.749092 loss)
I0715 11:45:33.115980 18762 solver.cpp:228] Iteration 400, loss = 0.237006
I0715 11:45:33.116142 18762 solver.cpp:244]     Train net output #0: loss = 0.237006 (* 1 = 0.237006 loss)
I0715 11:45:33.116173 18762 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0715 11:45:41.265674 18762 solver.cpp:337] Iteration 450, Testing net (#0)
I0715 11:45:51.600332 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6764
I0715 11:45:51.600489 18762 solver.cpp:404]     Test net output #1: loss = 0.737719 (* 1 = 0.737719 loss)
I0715 11:45:59.562361 18762 solver.cpp:337] Iteration 500, Testing net (#0)
I0715 11:46:09.816325 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6792
I0715 11:46:09.818122 18762 solver.cpp:404]     Test net output #1: loss = 0.71629 (* 1 = 0.71629 loss)
I0715 11:46:10.020447 18762 solver.cpp:228] Iteration 500, loss = 0.179292
I0715 11:46:10.020550 18762 solver.cpp:244]     Train net output #0: loss = 0.179292 (* 1 = 0.179292 loss)
I0715 11:46:10.020575 18762 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0715 11:46:17.862855 18762 solver.cpp:337] Iteration 550, Testing net (#0)
I0715 11:46:28.015878 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6672
I0715 11:46:28.015993 18762 solver.cpp:404]     Test net output #1: loss = 0.730328 (* 1 = 0.730328 loss)
I0715 11:46:36.004540 18762 solver.cpp:337] Iteration 600, Testing net (#0)
I0715 11:46:46.167759 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7077
I0715 11:46:46.167899 18762 solver.cpp:404]     Test net output #1: loss = 0.680302 (* 1 = 0.680302 loss)
I0715 11:46:46.383877 18762 solver.cpp:228] Iteration 600, loss = 0.131846
I0715 11:46:46.383996 18762 solver.cpp:244]     Train net output #0: loss = 0.131846 (* 1 = 0.131846 loss)
I0715 11:46:46.384022 18762 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0715 11:46:54.175575 18762 solver.cpp:337] Iteration 650, Testing net (#0)
I0715 11:47:04.389173 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6971
I0715 11:47:04.389336 18762 solver.cpp:404]     Test net output #1: loss = 0.781336 (* 1 = 0.781336 loss)
I0715 11:47:12.421571 18762 solver.cpp:337] Iteration 700, Testing net (#0)
I0715 11:47:22.569489 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7252
I0715 11:47:22.569677 18762 solver.cpp:404]     Test net output #1: loss = 0.726718 (* 1 = 0.726718 loss)
I0715 11:47:22.797001 18762 solver.cpp:228] Iteration 700, loss = 0.147849
I0715 11:47:22.797055 18762 solver.cpp:244]     Train net output #0: loss = 0.147849 (* 1 = 0.147849 loss)
I0715 11:47:22.797066 18762 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0715 11:47:30.630508 18762 solver.cpp:337] Iteration 750, Testing net (#0)
I0715 11:47:40.757109 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7111
I0715 11:47:40.757231 18762 solver.cpp:404]     Test net output #1: loss = 0.835397 (* 1 = 0.835397 loss)
I0715 11:47:48.698230 18762 solver.cpp:337] Iteration 800, Testing net (#0)
I0715 11:47:59.575403 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6985
I0715 11:47:59.575819 18762 solver.cpp:404]     Test net output #1: loss = 0.892164 (* 1 = 0.892164 loss)
I0715 11:47:59.816910 18762 solver.cpp:228] Iteration 800, loss = 0.13494
I0715 11:47:59.816975 18762 solver.cpp:244]     Train net output #0: loss = 0.13494 (* 1 = 0.13494 loss)
I0715 11:47:59.816989 18762 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0715 11:48:08.758232 18762 solver.cpp:337] Iteration 850, Testing net (#0)
I0715 11:48:20.021883 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7203
I0715 11:48:20.022088 18762 solver.cpp:404]     Test net output #1: loss = 0.695181 (* 1 = 0.695181 loss)
I0715 11:48:28.853142 18762 solver.cpp:337] Iteration 900, Testing net (#0)
I0715 11:48:39.400269 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6951
I0715 11:48:39.400482 18762 solver.cpp:404]     Test net output #1: loss = 1.06449 (* 1 = 1.06449 loss)
I0715 11:48:39.624472 18762 solver.cpp:228] Iteration 900, loss = 0.0504173
I0715 11:48:39.624529 18762 solver.cpp:244]     Train net output #0: loss = 0.0504173 (* 1 = 0.0504173 loss)
I0715 11:48:39.624539 18762 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0715 11:48:48.353281 18762 solver.cpp:337] Iteration 950, Testing net (#0)
I0715 11:48:58.607822 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6774
I0715 11:48:58.607985 18762 solver.cpp:404]     Test net output #1: loss = 0.803066 (* 1 = 0.803066 loss)
I0715 11:49:07.471567 18762 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_1000.caffemodel
I0715 11:49:07.480669 18762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_1000.solverstate
I0715 11:49:07.483445 18762 solver.cpp:337] Iteration 1000, Testing net (#0)
I0715 11:49:17.609580 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7296
I0715 11:49:17.609766 18762 solver.cpp:404]     Test net output #1: loss = 0.865404 (* 1 = 0.865404 loss)
I0715 11:49:17.806684 18762 solver.cpp:228] Iteration 1000, loss = 0.0841007
I0715 11:49:17.806826 18762 solver.cpp:244]     Train net output #0: loss = 0.0841008 (* 1 = 0.0841008 loss)
I0715 11:49:17.806944 18762 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0715 11:49:25.606459 18762 solver.cpp:337] Iteration 1050, Testing net (#0)
I0715 11:49:35.535887 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7192
I0715 11:49:35.536011 18762 solver.cpp:404]     Test net output #1: loss = 0.784496 (* 1 = 0.784496 loss)
I0715 11:49:43.491753 18762 solver.cpp:337] Iteration 1100, Testing net (#0)
I0715 11:49:53.430690 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7097
I0715 11:49:53.430873 18762 solver.cpp:404]     Test net output #1: loss = 0.782543 (* 1 = 0.782543 loss)
I0715 11:49:53.618814 18762 solver.cpp:228] Iteration 1100, loss = 0.0497417
I0715 11:49:53.618954 18762 solver.cpp:244]     Train net output #0: loss = 0.0497417 (* 1 = 0.0497417 loss)
I0715 11:49:53.619055 18762 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0715 11:50:01.465399 18762 solver.cpp:337] Iteration 1150, Testing net (#0)
I0715 11:50:11.405722 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6901
I0715 11:50:11.405838 18762 solver.cpp:404]     Test net output #1: loss = 0.870975 (* 1 = 0.870975 loss)
I0715 11:50:19.339998 18762 solver.cpp:337] Iteration 1200, Testing net (#0)
I0715 11:50:29.284355 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6474
I0715 11:50:29.284534 18762 solver.cpp:404]     Test net output #1: loss = 0.889022 (* 1 = 0.889022 loss)
I0715 11:50:29.476125 18762 solver.cpp:228] Iteration 1200, loss = 0.127897
I0715 11:50:29.476182 18762 solver.cpp:244]     Train net output #0: loss = 0.127897 (* 1 = 0.127897 loss)
I0715 11:50:29.476193 18762 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0715 11:50:37.235405 18762 solver.cpp:337] Iteration 1250, Testing net (#0)
I0715 11:50:47.181236 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7072
I0715 11:50:47.181370 18762 solver.cpp:404]     Test net output #1: loss = 0.772588 (* 1 = 0.772588 loss)
I0715 11:50:55.130070 18762 solver.cpp:337] Iteration 1300, Testing net (#0)
I0715 11:51:05.140095 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7152
I0715 11:51:05.140271 18762 solver.cpp:404]     Test net output #1: loss = 0.906155 (* 1 = 0.906155 loss)
I0715 11:51:05.345981 18762 solver.cpp:228] Iteration 1300, loss = 0.0626044
I0715 11:51:05.346037 18762 solver.cpp:244]     Train net output #0: loss = 0.0626043 (* 1 = 0.0626043 loss)
I0715 11:51:05.346047 18762 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0715 11:51:13.112586 18762 solver.cpp:337] Iteration 1350, Testing net (#0)
I0715 11:51:23.088119 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7073
I0715 11:51:23.088237 18762 solver.cpp:404]     Test net output #1: loss = 0.87142 (* 1 = 0.87142 loss)
I0715 11:51:30.991075 18762 solver.cpp:337] Iteration 1400, Testing net (#0)
I0715 11:51:40.947903 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7303
I0715 11:51:40.948189 18762 solver.cpp:404]     Test net output #1: loss = 0.725967 (* 1 = 0.725967 loss)
I0715 11:51:41.138607 18762 solver.cpp:228] Iteration 1400, loss = 0.0256163
I0715 11:51:41.138747 18762 solver.cpp:244]     Train net output #0: loss = 0.0256162 (* 1 = 0.0256162 loss)
I0715 11:51:41.138838 18762 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0715 11:51:48.881969 18762 solver.cpp:337] Iteration 1450, Testing net (#0)
I0715 11:51:58.808945 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6654
I0715 11:51:58.809056 18762 solver.cpp:404]     Test net output #1: loss = 0.7907 (* 1 = 0.7907 loss)
I0715 11:52:06.760460 18762 solver.cpp:337] Iteration 1500, Testing net (#0)
I0715 11:52:16.670509 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6765
I0715 11:52:16.670687 18762 solver.cpp:404]     Test net output #1: loss = 0.865545 (* 1 = 0.865545 loss)
I0715 11:52:16.859562 18762 solver.cpp:228] Iteration 1500, loss = 0.0395453
I0715 11:52:16.859679 18762 solver.cpp:244]     Train net output #0: loss = 0.0395452 (* 1 = 0.0395452 loss)
I0715 11:52:16.859706 18762 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0715 11:52:27.094580 18762 solver.cpp:337] Iteration 1550, Testing net (#0)
I0715 11:52:37.109985 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6982
I0715 11:52:37.110141 18762 solver.cpp:404]     Test net output #1: loss = 0.893532 (* 1 = 0.893532 loss)
I0715 11:52:45.060771 18762 solver.cpp:337] Iteration 1600, Testing net (#0)
I0715 11:52:55.021545 18762 solver.cpp:404]     Test net output #0: accuracy = 0.693
I0715 11:52:55.021724 18762 solver.cpp:404]     Test net output #1: loss = 0.869518 (* 1 = 0.869518 loss)
I0715 11:52:55.219797 18762 solver.cpp:228] Iteration 1600, loss = 0.00497037
I0715 11:52:55.219853 18762 solver.cpp:244]     Train net output #0: loss = 0.00497028 (* 1 = 0.00497028 loss)
I0715 11:52:55.219864 18762 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0715 11:53:02.990401 18762 solver.cpp:337] Iteration 1650, Testing net (#0)
I0715 11:53:12.899860 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6897
I0715 11:53:12.900065 18762 solver.cpp:404]     Test net output #1: loss = 0.818295 (* 1 = 0.818295 loss)
I0715 11:53:20.765636 18762 solver.cpp:337] Iteration 1700, Testing net (#0)
I0715 11:53:30.708323 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7188
I0715 11:53:30.708422 18762 solver.cpp:404]     Test net output #1: loss = 0.729987 (* 1 = 0.729987 loss)
I0715 11:53:30.893851 18762 solver.cpp:228] Iteration 1700, loss = 0.0201363
I0715 11:53:30.893911 18762 solver.cpp:244]     Train net output #0: loss = 0.0201363 (* 1 = 0.0201363 loss)
I0715 11:53:30.893923 18762 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0715 11:53:38.581269 18762 solver.cpp:337] Iteration 1750, Testing net (#0)
I0715 11:53:48.517387 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7061
I0715 11:53:48.517585 18762 solver.cpp:404]     Test net output #1: loss = 0.818787 (* 1 = 0.818787 loss)
I0715 11:53:56.470793 18762 solver.cpp:337] Iteration 1800, Testing net (#0)
I0715 11:54:06.454262 18762 solver.cpp:404]     Test net output #0: accuracy = 0.698
I0715 11:54:06.454443 18762 solver.cpp:404]     Test net output #1: loss = 0.899082 (* 1 = 0.899082 loss)
I0715 11:54:06.643442 18762 solver.cpp:228] Iteration 1800, loss = 0.0146329
I0715 11:54:06.643611 18762 solver.cpp:244]     Train net output #0: loss = 0.0146328 (* 1 = 0.0146328 loss)
I0715 11:54:06.643640 18762 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0715 11:54:14.433588 18762 solver.cpp:337] Iteration 1850, Testing net (#0)
I0715 11:54:24.377005 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6435
I0715 11:54:24.377126 18762 solver.cpp:404]     Test net output #1: loss = 0.937548 (* 1 = 0.937548 loss)
I0715 11:54:32.231686 18762 solver.cpp:337] Iteration 1900, Testing net (#0)
I0715 11:54:42.272445 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6903
I0715 11:54:42.272754 18762 solver.cpp:404]     Test net output #1: loss = 0.80021 (* 1 = 0.80021 loss)
I0715 11:54:42.469195 18762 solver.cpp:228] Iteration 1900, loss = 0.083653
I0715 11:54:42.469254 18762 solver.cpp:244]     Train net output #0: loss = 0.0836529 (* 1 = 0.0836529 loss)
I0715 11:54:42.469265 18762 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0715 11:54:50.188732 18762 solver.cpp:337] Iteration 1950, Testing net (#0)
I0715 11:55:00.074398 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6358
I0715 11:55:00.074468 18762 solver.cpp:404]     Test net output #1: loss = 0.910818 (* 1 = 0.910818 loss)
I0715 11:55:07.968780 18762 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_2000.caffemodel
I0715 11:55:07.978191 18762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_2000.solverstate
I0715 11:55:07.980934 18762 solver.cpp:337] Iteration 2000, Testing net (#0)
I0715 11:55:17.865450 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7039
I0715 11:55:17.865790 18762 solver.cpp:404]     Test net output #1: loss = 1.08483 (* 1 = 1.08483 loss)
I0715 11:55:18.060860 18762 solver.cpp:228] Iteration 2000, loss = 0.0299414
I0715 11:55:18.060917 18762 solver.cpp:244]     Train net output #0: loss = 0.0299413 (* 1 = 0.0299413 loss)
I0715 11:55:18.060928 18762 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0715 11:55:25.758818 18762 solver.cpp:337] Iteration 2050, Testing net (#0)
I0715 11:55:35.640204 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6413
I0715 11:55:35.640261 18762 solver.cpp:404]     Test net output #1: loss = 0.966515 (* 1 = 0.966515 loss)
I0715 11:55:43.512902 18762 solver.cpp:337] Iteration 2100, Testing net (#0)
I0715 11:55:55.246557 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7112
I0715 11:55:55.246641 18762 solver.cpp:404]     Test net output #1: loss = 1.00592 (* 1 = 1.00592 loss)
I0715 11:55:55.431913 18762 solver.cpp:228] Iteration 2100, loss = 0.0514187
I0715 11:55:55.431970 18762 solver.cpp:244]     Train net output #0: loss = 0.0514187 (* 1 = 0.0514187 loss)
I0715 11:55:55.431982 18762 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0715 11:56:04.609027 18762 solver.cpp:337] Iteration 2150, Testing net (#0)
I0715 11:56:16.926434 18762 solver.cpp:404]     Test net output #0: accuracy = 0.675
I0715 11:56:16.926717 18762 solver.cpp:404]     Test net output #1: loss = 1.03238 (* 1 = 1.03238 loss)
I0715 11:56:24.988379 18762 solver.cpp:337] Iteration 2200, Testing net (#0)
I0715 11:56:34.937705 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6652
I0715 11:56:34.937885 18762 solver.cpp:404]     Test net output #1: loss = 0.955326 (* 1 = 0.955326 loss)
I0715 11:56:35.136914 18762 solver.cpp:228] Iteration 2200, loss = 0.0581506
I0715 11:56:35.137063 18762 solver.cpp:244]     Train net output #0: loss = 0.0581506 (* 1 = 0.0581506 loss)
I0715 11:56:35.137089 18762 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0715 11:56:42.862308 18762 solver.cpp:337] Iteration 2250, Testing net (#0)
I0715 11:56:52.744595 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7318
I0715 11:56:52.744664 18762 solver.cpp:404]     Test net output #1: loss = 0.924764 (* 1 = 0.924764 loss)
I0715 11:57:00.651737 18762 solver.cpp:337] Iteration 2300, Testing net (#0)
I0715 11:57:10.505530 18762 solver.cpp:404]     Test net output #0: accuracy = 0.672
I0715 11:57:10.505913 18762 solver.cpp:404]     Test net output #1: loss = 1.01066 (* 1 = 1.01066 loss)
I0715 11:57:10.705768 18762 solver.cpp:228] Iteration 2300, loss = 0.0670556
I0715 11:57:10.705973 18762 solver.cpp:244]     Train net output #0: loss = 0.0670556 (* 1 = 0.0670556 loss)
I0715 11:57:10.706006 18762 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0715 11:57:18.490766 18762 solver.cpp:337] Iteration 2350, Testing net (#0)
I0715 11:57:28.333503 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7281
I0715 11:57:28.333559 18762 solver.cpp:404]     Test net output #1: loss = 0.842935 (* 1 = 0.842935 loss)
I0715 11:57:36.251824 18762 solver.cpp:337] Iteration 2400, Testing net (#0)
I0715 11:57:46.182292 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6538
I0715 11:57:46.182447 18762 solver.cpp:404]     Test net output #1: loss = 1.06278 (* 1 = 1.06278 loss)
I0715 11:57:46.382632 18762 solver.cpp:228] Iteration 2400, loss = 0.013816
I0715 11:57:46.382693 18762 solver.cpp:244]     Train net output #0: loss = 0.013816 (* 1 = 0.013816 loss)
I0715 11:57:46.382704 18762 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0715 11:57:54.140219 18762 solver.cpp:337] Iteration 2450, Testing net (#0)
I0715 11:58:04.153098 18762 solver.cpp:404]     Test net output #0: accuracy = 0.681
I0715 11:58:04.153302 18762 solver.cpp:404]     Test net output #1: loss = 1.13148 (* 1 = 1.13148 loss)
I0715 11:58:13.802914 18762 solver.cpp:337] Iteration 2500, Testing net (#0)
I0715 11:58:26.170665 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6996
I0715 11:58:26.170843 18762 solver.cpp:404]     Test net output #1: loss = 1.11567 (* 1 = 1.11567 loss)
I0715 11:58:26.374626 18762 solver.cpp:228] Iteration 2500, loss = 0.0131301
I0715 11:58:26.374850 18762 solver.cpp:244]     Train net output #0: loss = 0.0131301 (* 1 = 0.0131301 loss)
I0715 11:58:26.375039 18762 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0715 11:58:34.545238 18762 solver.cpp:337] Iteration 2550, Testing net (#0)
I0715 11:58:45.331269 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6358
I0715 11:58:45.331431 18762 solver.cpp:404]     Test net output #1: loss = 1.06965 (* 1 = 1.06965 loss)
I0715 11:58:53.591943 18762 solver.cpp:337] Iteration 2600, Testing net (#0)
I0715 11:59:03.895155 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6719
I0715 11:59:03.895310 18762 solver.cpp:404]     Test net output #1: loss = 0.953693 (* 1 = 0.953693 loss)
I0715 11:59:04.088856 18762 solver.cpp:228] Iteration 2600, loss = 0.00809156
I0715 11:59:04.088922 18762 solver.cpp:244]     Train net output #0: loss = 0.00809162 (* 1 = 0.00809162 loss)
I0715 11:59:04.088933 18762 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0715 11:59:12.148133 18762 solver.cpp:337] Iteration 2650, Testing net (#0)
I0715 11:59:22.334497 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6726
I0715 11:59:22.334571 18762 solver.cpp:404]     Test net output #1: loss = 0.991077 (* 1 = 0.991077 loss)
I0715 11:59:30.506506 18762 solver.cpp:337] Iteration 2700, Testing net (#0)
I0715 11:59:40.874891 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6925
I0715 11:59:40.875166 18762 solver.cpp:404]     Test net output #1: loss = 1.05729 (* 1 = 1.05729 loss)
I0715 11:59:41.083884 18762 solver.cpp:228] Iteration 2700, loss = 0.00513305
I0715 11:59:41.084003 18762 solver.cpp:244]     Train net output #0: loss = 0.0051331 (* 1 = 0.0051331 loss)
I0715 11:59:41.084064 18762 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0715 11:59:49.149255 18762 solver.cpp:337] Iteration 2750, Testing net (#0)
I0715 11:59:59.362905 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7154
I0715 11:59:59.362968 18762 solver.cpp:404]     Test net output #1: loss = 1.07165 (* 1 = 1.07165 loss)
I0715 12:00:07.778817 18762 solver.cpp:337] Iteration 2800, Testing net (#0)
I0715 12:00:18.031621 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6608
I0715 12:00:18.031713 18762 solver.cpp:404]     Test net output #1: loss = 1.08041 (* 1 = 1.08041 loss)
I0715 12:00:18.222795 18762 solver.cpp:228] Iteration 2800, loss = 0.00582512
I0715 12:00:18.222851 18762 solver.cpp:244]     Train net output #0: loss = 0.00582516 (* 1 = 0.00582516 loss)
I0715 12:00:18.222889 18762 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0715 12:00:26.327003 18762 solver.cpp:337] Iteration 2850, Testing net (#0)
I0715 12:00:36.618477 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6937
I0715 12:00:36.618659 18762 solver.cpp:404]     Test net output #1: loss = 1.01735 (* 1 = 1.01735 loss)
I0715 12:00:44.801969 18762 solver.cpp:337] Iteration 2900, Testing net (#0)
I0715 12:00:55.089776 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6518
I0715 12:00:55.089922 18762 solver.cpp:404]     Test net output #1: loss = 1.05891 (* 1 = 1.05891 loss)
I0715 12:00:55.276551 18762 solver.cpp:228] Iteration 2900, loss = 0.0121375
I0715 12:00:55.276618 18762 solver.cpp:244]     Train net output #0: loss = 0.0121376 (* 1 = 0.0121376 loss)
I0715 12:00:55.276630 18762 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0715 12:01:03.368335 18762 solver.cpp:337] Iteration 2950, Testing net (#0)
I0715 12:01:13.622706 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6984
I0715 12:01:13.622928 18762 solver.cpp:404]     Test net output #1: loss = 1.0791 (* 1 = 1.0791 loss)
I0715 12:01:21.820011 18762 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_3000.caffemodel
I0715 12:01:21.830940 18762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_3000.solverstate
I0715 12:01:21.834203 18762 solver.cpp:337] Iteration 3000, Testing net (#0)
I0715 12:01:32.209537 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7039
I0715 12:01:32.209763 18762 solver.cpp:404]     Test net output #1: loss = 1.09063 (* 1 = 1.09063 loss)
I0715 12:01:32.425578 18762 solver.cpp:228] Iteration 3000, loss = 0.00214966
I0715 12:01:32.425907 18762 solver.cpp:244]     Train net output #0: loss = 0.00214968 (* 1 = 0.00214968 loss)
I0715 12:01:32.425966 18762 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0715 12:01:40.411491 18762 solver.cpp:337] Iteration 3050, Testing net (#0)
I0715 12:01:50.575021 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7121
I0715 12:01:50.575083 18762 solver.cpp:404]     Test net output #1: loss = 1.18876 (* 1 = 1.18876 loss)
I0715 12:01:58.730804 18762 solver.cpp:337] Iteration 3100, Testing net (#0)
I0715 12:02:09.106848 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6728
I0715 12:02:09.107023 18762 solver.cpp:404]     Test net output #1: loss = 1.10778 (* 1 = 1.10778 loss)
I0715 12:02:09.293674 18762 solver.cpp:228] Iteration 3100, loss = 0.010923
I0715 12:02:09.293800 18762 solver.cpp:244]     Train net output #0: loss = 0.0109231 (* 1 = 0.0109231 loss)
I0715 12:02:09.293829 18762 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0715 12:02:17.343598 18762 solver.cpp:337] Iteration 3150, Testing net (#0)
I0715 12:02:27.667524 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6485
I0715 12:02:27.667719 18762 solver.cpp:404]     Test net output #1: loss = 1.19736 (* 1 = 1.19736 loss)
I0715 12:02:35.895440 18762 solver.cpp:337] Iteration 3200, Testing net (#0)
I0715 12:02:46.212216 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6812
I0715 12:02:46.212388 18762 solver.cpp:404]     Test net output #1: loss = 1.09187 (* 1 = 1.09187 loss)
I0715 12:02:46.424178 18762 solver.cpp:228] Iteration 3200, loss = 0.0261597
I0715 12:02:46.424289 18762 solver.cpp:244]     Train net output #0: loss = 0.0261597 (* 1 = 0.0261597 loss)
I0715 12:02:46.424306 18762 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0715 12:02:54.432273 18762 solver.cpp:337] Iteration 3250, Testing net (#0)
I0715 12:03:04.744523 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6565
I0715 12:03:04.744583 18762 solver.cpp:404]     Test net output #1: loss = 1.1167 (* 1 = 1.1167 loss)
I0715 12:03:12.943811 18762 solver.cpp:337] Iteration 3300, Testing net (#0)
I0715 12:03:25.005470 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7275
I0715 12:03:25.005697 18762 solver.cpp:404]     Test net output #1: loss = 0.949387 (* 1 = 0.949387 loss)
I0715 12:03:25.216951 18762 solver.cpp:228] Iteration 3300, loss = 0.0143139
I0715 12:03:25.217139 18762 solver.cpp:244]     Train net output #0: loss = 0.014314 (* 1 = 0.014314 loss)
I0715 12:03:25.217239 18762 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0715 12:03:35.378450 18762 solver.cpp:337] Iteration 3350, Testing net (#0)
I0715 12:03:48.197608 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6222
I0715 12:03:48.197748 18762 solver.cpp:404]     Test net output #1: loss = 1.28605 (* 1 = 1.28605 loss)
I0715 12:03:58.608922 18762 solver.cpp:337] Iteration 3400, Testing net (#0)
I0715 12:04:11.620565 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6429
I0715 12:04:11.620774 18762 solver.cpp:404]     Test net output #1: loss = 1.1335 (* 1 = 1.1335 loss)
I0715 12:04:11.811257 18762 solver.cpp:228] Iteration 3400, loss = 0.0043481
I0715 12:04:11.811328 18762 solver.cpp:244]     Train net output #0: loss = 0.00434811 (* 1 = 0.00434811 loss)
I0715 12:04:11.811339 18762 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0715 12:04:21.121042 18762 solver.cpp:337] Iteration 3450, Testing net (#0)
I0715 12:04:31.548239 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6498
I0715 12:04:31.548465 18762 solver.cpp:404]     Test net output #1: loss = 1.28612 (* 1 = 1.28612 loss)
I0715 12:04:39.787739 18762 solver.cpp:337] Iteration 3500, Testing net (#0)
I0715 12:04:50.017410 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6959
I0715 12:04:50.017463 18762 solver.cpp:404]     Test net output #1: loss = 1.09837 (* 1 = 1.09837 loss)
I0715 12:04:50.201287 18762 solver.cpp:228] Iteration 3500, loss = 0.00233164
I0715 12:04:50.201349 18762 solver.cpp:244]     Train net output #0: loss = 0.00233165 (* 1 = 0.00233165 loss)
I0715 12:04:50.201359 18762 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0715 12:04:58.204555 18762 solver.cpp:337] Iteration 3550, Testing net (#0)
I0715 12:05:08.514230 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6432
I0715 12:05:08.514449 18762 solver.cpp:404]     Test net output #1: loss = 1.04686 (* 1 = 1.04686 loss)
I0715 12:05:16.697720 18762 solver.cpp:337] Iteration 3600, Testing net (#0)
I0715 12:05:26.961844 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6729
I0715 12:05:26.961905 18762 solver.cpp:404]     Test net output #1: loss = 1.02808 (* 1 = 1.02808 loss)
I0715 12:05:27.150970 18762 solver.cpp:228] Iteration 3600, loss = 0.00303077
I0715 12:05:27.151114 18762 solver.cpp:244]     Train net output #0: loss = 0.00303077 (* 1 = 0.00303077 loss)
I0715 12:05:27.151157 18762 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0715 12:05:35.305745 18762 solver.cpp:337] Iteration 3650, Testing net (#0)
I0715 12:05:45.620067 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6634
I0715 12:05:45.620152 18762 solver.cpp:404]     Test net output #1: loss = 1.39293 (* 1 = 1.39293 loss)
I0715 12:05:53.814646 18762 solver.cpp:337] Iteration 3700, Testing net (#0)
I0715 12:06:04.159531 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6914
I0715 12:06:04.159726 18762 solver.cpp:404]     Test net output #1: loss = 1.15677 (* 1 = 1.15677 loss)
I0715 12:06:04.347648 18762 solver.cpp:228] Iteration 3700, loss = 0.0022481
I0715 12:06:04.347707 18762 solver.cpp:244]     Train net output #0: loss = 0.00224808 (* 1 = 0.00224808 loss)
I0715 12:06:04.347721 18762 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0715 12:06:12.368726 18762 solver.cpp:337] Iteration 3750, Testing net (#0)
I0715 12:06:22.541038 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6967
I0715 12:06:22.541131 18762 solver.cpp:404]     Test net output #1: loss = 1.11509 (* 1 = 1.11509 loss)
I0715 12:06:30.692925 18762 solver.cpp:337] Iteration 3800, Testing net (#0)
I0715 12:06:40.880657 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6848
I0715 12:06:40.880717 18762 solver.cpp:404]     Test net output #1: loss = 1.21025 (* 1 = 1.21025 loss)
I0715 12:06:41.061172 18762 solver.cpp:228] Iteration 3800, loss = 0.00611177
I0715 12:06:41.061233 18762 solver.cpp:244]     Train net output #0: loss = 0.00611177 (* 1 = 0.00611177 loss)
I0715 12:06:41.061244 18762 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0715 12:06:49.115823 18762 solver.cpp:337] Iteration 3850, Testing net (#0)
I0715 12:06:59.427024 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6775
I0715 12:06:59.427422 18762 solver.cpp:404]     Test net output #1: loss = 1.22663 (* 1 = 1.22663 loss)
I0715 12:07:07.711294 18762 solver.cpp:337] Iteration 3900, Testing net (#0)
I0715 12:07:17.915153 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6977
I0715 12:07:17.915225 18762 solver.cpp:404]     Test net output #1: loss = 1.03799 (* 1 = 1.03799 loss)
I0715 12:07:18.102615 18762 solver.cpp:228] Iteration 3900, loss = 0.0160701
I0715 12:07:18.102691 18762 solver.cpp:244]     Train net output #0: loss = 0.0160701 (* 1 = 0.0160701 loss)
I0715 12:07:18.102704 18762 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0715 12:07:26.220185 18762 solver.cpp:337] Iteration 3950, Testing net (#0)
I0715 12:07:36.541021 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6817
I0715 12:07:36.541191 18762 solver.cpp:404]     Test net output #1: loss = 1.33795 (* 1 = 1.33795 loss)
I0715 12:07:44.871728 18762 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_4000.caffemodel
I0715 12:07:44.882143 18762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_4000.solverstate
I0715 12:07:44.885179 18762 solver.cpp:337] Iteration 4000, Testing net (#0)
I0715 12:07:55.243356 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6983
I0715 12:07:55.243500 18762 solver.cpp:404]     Test net output #1: loss = 1.0991 (* 1 = 1.0991 loss)
I0715 12:07:55.438145 18762 solver.cpp:228] Iteration 4000, loss = 0.00340021
I0715 12:07:55.438279 18762 solver.cpp:244]     Train net output #0: loss = 0.00340019 (* 1 = 0.00340019 loss)
I0715 12:07:55.438308 18762 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0715 12:08:03.540660 18762 solver.cpp:337] Iteration 4050, Testing net (#0)
I0715 12:08:13.874692 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6663
I0715 12:08:13.874948 18762 solver.cpp:404]     Test net output #1: loss = 1.16695 (* 1 = 1.16695 loss)
I0715 12:08:22.082768 18762 solver.cpp:337] Iteration 4100, Testing net (#0)
I0715 12:08:32.299073 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6607
I0715 12:08:32.299145 18762 solver.cpp:404]     Test net output #1: loss = 1.3213 (* 1 = 1.3213 loss)
I0715 12:08:32.483508 18762 solver.cpp:228] Iteration 4100, loss = 0.00131393
I0715 12:08:32.483594 18762 solver.cpp:244]     Train net output #0: loss = 0.0013139 (* 1 = 0.0013139 loss)
I0715 12:08:32.483606 18762 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0715 12:08:40.564713 18762 solver.cpp:337] Iteration 4150, Testing net (#0)
I0715 12:08:50.903043 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6594
I0715 12:08:50.903245 18762 solver.cpp:404]     Test net output #1: loss = 1.30704 (* 1 = 1.30704 loss)
I0715 12:08:59.119446 18762 solver.cpp:337] Iteration 4200, Testing net (#0)
I0715 12:09:09.475978 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6416
I0715 12:09:09.476128 18762 solver.cpp:404]     Test net output #1: loss = 1.34523 (* 1 = 1.34523 loss)
I0715 12:09:09.657912 18762 solver.cpp:228] Iteration 4200, loss = 0.0011381
I0715 12:09:09.658087 18762 solver.cpp:244]     Train net output #0: loss = 0.00113807 (* 1 = 0.00113807 loss)
I0715 12:09:09.658118 18762 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0715 12:09:17.706501 18762 solver.cpp:337] Iteration 4250, Testing net (#0)
I0715 12:09:30.657022 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6693
I0715 12:09:30.663234 18762 solver.cpp:404]     Test net output #1: loss = 1.16739 (* 1 = 1.16739 loss)
I0715 12:09:40.972529 18762 solver.cpp:337] Iteration 4300, Testing net (#0)
I0715 12:09:53.889838 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6196
I0715 12:09:53.890679 18762 solver.cpp:404]     Test net output #1: loss = 1.3121 (* 1 = 1.3121 loss)
I0715 12:09:54.082149 18762 solver.cpp:228] Iteration 4300, loss = 0.00267135
I0715 12:09:54.082314 18762 solver.cpp:244]     Train net output #0: loss = 0.00267132 (* 1 = 0.00267132 loss)
I0715 12:09:54.082360 18762 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0715 12:10:04.191603 18762 solver.cpp:337] Iteration 4350, Testing net (#0)
I0715 12:10:17.076671 18762 solver.cpp:404]     Test net output #0: accuracy = 0.5979
I0715 12:10:17.076884 18762 solver.cpp:404]     Test net output #1: loss = 1.41186 (* 1 = 1.41186 loss)
I0715 12:10:27.452226 18762 solver.cpp:337] Iteration 4400, Testing net (#0)
I0715 12:10:37.916141 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7115
I0715 12:10:37.916234 18762 solver.cpp:404]     Test net output #1: loss = 1.13358 (* 1 = 1.13358 loss)
I0715 12:10:38.103447 18762 solver.cpp:228] Iteration 4400, loss = 0.00443523
I0715 12:10:38.103588 18762 solver.cpp:244]     Train net output #0: loss = 0.0044352 (* 1 = 0.0044352 loss)
I0715 12:10:38.103608 18762 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0715 12:10:46.281299 18762 solver.cpp:337] Iteration 4450, Testing net (#0)
I0715 12:10:56.572228 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6577
I0715 12:10:56.572288 18762 solver.cpp:404]     Test net output #1: loss = 1.29958 (* 1 = 1.29958 loss)
I0715 12:11:04.830202 18762 solver.cpp:337] Iteration 4500, Testing net (#0)
I0715 12:11:15.112056 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6982
I0715 12:11:15.112151 18762 solver.cpp:404]     Test net output #1: loss = 1.13648 (* 1 = 1.13648 loss)
I0715 12:11:15.305483 18762 solver.cpp:228] Iteration 4500, loss = 0.00458329
I0715 12:11:15.305562 18762 solver.cpp:244]     Train net output #0: loss = 0.00458326 (* 1 = 0.00458326 loss)
I0715 12:11:15.305574 18762 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0715 12:11:23.361551 18762 solver.cpp:337] Iteration 4550, Testing net (#0)
I0715 12:11:33.689524 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6803
I0715 12:11:33.689584 18762 solver.cpp:404]     Test net output #1: loss = 1.15604 (* 1 = 1.15604 loss)
I0715 12:11:41.927657 18762 solver.cpp:337] Iteration 4600, Testing net (#0)
I0715 12:11:52.132952 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6916
I0715 12:11:52.133038 18762 solver.cpp:404]     Test net output #1: loss = 1.16425 (* 1 = 1.16425 loss)
I0715 12:11:52.328086 18762 solver.cpp:228] Iteration 4600, loss = 0.00208485
I0715 12:11:52.328150 18762 solver.cpp:244]     Train net output #0: loss = 0.00208482 (* 1 = 0.00208482 loss)
I0715 12:11:52.328161 18762 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0715 12:12:00.429945 18762 solver.cpp:337] Iteration 4650, Testing net (#0)
I0715 12:12:10.624083 18762 solver.cpp:404]     Test net output #0: accuracy = 0.688
I0715 12:12:10.624141 18762 solver.cpp:404]     Test net output #1: loss = 1.30802 (* 1 = 1.30802 loss)
I0715 12:12:18.793730 18762 solver.cpp:337] Iteration 4700, Testing net (#0)
I0715 12:12:29.077371 18762 solver.cpp:404]     Test net output #0: accuracy = 0.657
I0715 12:12:29.077553 18762 solver.cpp:404]     Test net output #1: loss = 1.32298 (* 1 = 1.32298 loss)
I0715 12:12:29.322939 18762 solver.cpp:228] Iteration 4700, loss = 0.0327325
I0715 12:12:29.322999 18762 solver.cpp:244]     Train net output #0: loss = 0.0327324 (* 1 = 0.0327324 loss)
I0715 12:12:29.323009 18762 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0715 12:12:37.485268 18762 solver.cpp:337] Iteration 4750, Testing net (#0)
I0715 12:12:47.790899 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7127
I0715 12:12:47.791097 18762 solver.cpp:404]     Test net output #1: loss = 1.29077 (* 1 = 1.29077 loss)
I0715 12:12:56.001632 18762 solver.cpp:337] Iteration 4800, Testing net (#0)
I0715 12:13:06.320751 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6701
I0715 12:13:06.320926 18762 solver.cpp:404]     Test net output #1: loss = 1.35542 (* 1 = 1.35542 loss)
I0715 12:13:06.508759 18762 solver.cpp:228] Iteration 4800, loss = 0.0709553
I0715 12:13:06.508889 18762 solver.cpp:244]     Train net output #0: loss = 0.0709553 (* 1 = 0.0709553 loss)
I0715 12:13:06.508918 18762 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0715 12:13:14.638825 18762 solver.cpp:337] Iteration 4850, Testing net (#0)
I0715 12:13:24.919070 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7063
I0715 12:13:24.919226 18762 solver.cpp:404]     Test net output #1: loss = 1.19428 (* 1 = 1.19428 loss)
I0715 12:13:33.154703 18762 solver.cpp:337] Iteration 4900, Testing net (#0)
I0715 12:13:43.459764 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6895
I0715 12:13:43.459998 18762 solver.cpp:404]     Test net output #1: loss = 1.30736 (* 1 = 1.30736 loss)
I0715 12:13:43.674180 18762 solver.cpp:228] Iteration 4900, loss = 0.00208021
I0715 12:13:43.674263 18762 solver.cpp:244]     Train net output #0: loss = 0.00208012 (* 1 = 0.00208012 loss)
I0715 12:13:43.674278 18762 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0715 12:13:51.660673 18762 solver.cpp:337] Iteration 4950, Testing net (#0)
I0715 12:14:01.948012 18762 solver.cpp:404]     Test net output #0: accuracy = 0.7035
I0715 12:14:01.948215 18762 solver.cpp:404]     Test net output #1: loss = 1.24979 (* 1 = 1.24979 loss)
I0715 12:14:10.145949 18762 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_5000.caffemodel
I0715 12:14:10.156556 18762 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to6_feat_2_sim_iter_5000.solverstate
I0715 12:14:10.232384 18762 solver.cpp:317] Iteration 5000, loss = 0.00185312
I0715 12:14:10.232441 18762 solver.cpp:337] Iteration 5000, Testing net (#0)
I0715 12:14:20.514828 18762 solver.cpp:404]     Test net output #0: accuracy = 0.6235
I0715 12:14:20.514930 18762 solver.cpp:404]     Test net output #1: loss = 1.41317 (* 1 = 1.41317 loss)
I0715 12:14:20.514940 18762 solver.cpp:322] Optimization Done.
I0715 12:14:20.514942 18762 caffe.cpp:222] Optimization Done.
