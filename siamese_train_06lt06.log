I0713 15:32:59.985530 29118 caffe.cpp:178] Use CPU.
I0713 15:32:59.986254 29118 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 5000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0
snapshot: 5000
snapshot_prefix: "examples/siamese/My_mnist_siamese_0to6lt06"
solver_mode: CPU
net: "examples/siamese/mnist_siamese_train_test.prototxt"
I0713 15:32:59.986397 29118 solver.cpp:91] Creating training net from net file: examples/siamese/mnist_siamese_train_test.prototxt
I0713 15:32:59.987372 29118 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer pair_data
I0713 15:32:59.987597 29118 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TRAIN
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_train_leveldb_0to6_l"
    batch_size: 64
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0713 15:32:59.987792 29118 layer_factory.hpp:77] Creating layer pair_data
I0713 15:32:59.988487 29118 net.cpp:91] Creating Layer pair_data
I0713 15:32:59.988533 29118 net.cpp:399] pair_data -> pair_data
I0713 15:32:59.988575 29118 net.cpp:399] pair_data -> sim
I0713 15:33:00.007380 29122 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_train_leveldb_0to6_l
I0713 15:33:00.026770 29118 data_layer.cpp:41] output data size: 64,2,28,28
I0713 15:33:00.036516 29118 net.cpp:141] Setting up pair_data
I0713 15:33:00.036664 29118 net.cpp:148] Top shape: 64 2 28 28 (100352)
I0713 15:33:00.036715 29118 net.cpp:148] Top shape: 64 (64)
I0713 15:33:00.036730 29118 net.cpp:156] Memory required for data: 401664
I0713 15:33:00.036751 29118 layer_factory.hpp:77] Creating layer slice_pair
I0713 15:33:00.036798 29118 net.cpp:91] Creating Layer slice_pair
I0713 15:33:00.036818 29118 net.cpp:425] slice_pair <- pair_data
I0713 15:33:00.036840 29118 net.cpp:399] slice_pair -> data
I0713 15:33:00.036887 29118 net.cpp:399] slice_pair -> data_p
I0713 15:33:00.036918 29118 net.cpp:141] Setting up slice_pair
I0713 15:33:00.036938 29118 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0713 15:33:00.036955 29118 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0713 15:33:00.036970 29118 net.cpp:156] Memory required for data: 803072
I0713 15:33:00.036986 29118 layer_factory.hpp:77] Creating layer conv1
I0713 15:33:00.037025 29118 net.cpp:91] Creating Layer conv1
I0713 15:33:00.037075 29118 net.cpp:425] conv1 <- data
I0713 15:33:00.037102 29118 net.cpp:399] conv1 -> conv1
I0713 15:33:00.039046 29118 net.cpp:141] Setting up conv1
I0713 15:33:00.039129 29118 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0713 15:33:00.039149 29118 net.cpp:156] Memory required for data: 3752192
I0713 15:33:00.039177 29118 layer_factory.hpp:77] Creating layer pool1
I0713 15:33:00.039201 29118 net.cpp:91] Creating Layer pool1
I0713 15:33:00.039217 29118 net.cpp:425] pool1 <- conv1
I0713 15:33:00.039237 29118 net.cpp:399] pool1 -> pool1
I0713 15:33:00.039278 29118 net.cpp:141] Setting up pool1
I0713 15:33:00.039299 29118 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0713 15:33:00.039314 29118 net.cpp:156] Memory required for data: 4489472
I0713 15:33:00.039330 29118 layer_factory.hpp:77] Creating layer conv2
I0713 15:33:00.039353 29118 net.cpp:91] Creating Layer conv2
I0713 15:33:00.039371 29118 net.cpp:425] conv2 <- pool1
I0713 15:33:00.039397 29118 net.cpp:399] conv2 -> conv2
I0713 15:33:00.039988 29118 net.cpp:141] Setting up conv2
I0713 15:33:00.040020 29118 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0713 15:33:00.040025 29118 net.cpp:156] Memory required for data: 5308672
I0713 15:33:00.040033 29118 layer_factory.hpp:77] Creating layer pool2
I0713 15:33:00.040045 29118 net.cpp:91] Creating Layer pool2
I0713 15:33:00.040072 29118 net.cpp:425] pool2 <- conv2
I0713 15:33:00.040078 29118 net.cpp:399] pool2 -> pool2
I0713 15:33:00.040088 29118 net.cpp:141] Setting up pool2
I0713 15:33:00.040093 29118 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0713 15:33:00.040096 29118 net.cpp:156] Memory required for data: 5513472
I0713 15:33:00.040099 29118 layer_factory.hpp:77] Creating layer ip1
I0713 15:33:00.040117 29118 net.cpp:91] Creating Layer ip1
I0713 15:33:00.040119 29118 net.cpp:425] ip1 <- pool2
I0713 15:33:00.040125 29118 net.cpp:399] ip1 -> ip1
I0713 15:33:00.045352 29118 net.cpp:141] Setting up ip1
I0713 15:33:00.045408 29118 net.cpp:148] Top shape: 64 500 (32000)
I0713 15:33:00.045413 29118 net.cpp:156] Memory required for data: 5641472
I0713 15:33:00.045428 29118 layer_factory.hpp:77] Creating layer relu1
I0713 15:33:00.045444 29118 net.cpp:91] Creating Layer relu1
I0713 15:33:00.045449 29118 net.cpp:425] relu1 <- ip1
I0713 15:33:00.045454 29118 net.cpp:386] relu1 -> ip1 (in-place)
I0713 15:33:00.045465 29118 net.cpp:141] Setting up relu1
I0713 15:33:00.045483 29118 net.cpp:148] Top shape: 64 500 (32000)
I0713 15:33:00.045487 29118 net.cpp:156] Memory required for data: 5769472
I0713 15:33:00.045490 29118 layer_factory.hpp:77] Creating layer ip2
I0713 15:33:00.045501 29118 net.cpp:91] Creating Layer ip2
I0713 15:33:00.045506 29118 net.cpp:425] ip2 <- ip1
I0713 15:33:00.045512 29118 net.cpp:399] ip2 -> ip2
I0713 15:33:00.045583 29118 net.cpp:141] Setting up ip2
I0713 15:33:00.045601 29118 net.cpp:148] Top shape: 64 10 (640)
I0713 15:33:00.045604 29118 net.cpp:156] Memory required for data: 5772032
I0713 15:33:00.045609 29118 layer_factory.hpp:77] Creating layer feat
I0713 15:33:00.045615 29118 net.cpp:91] Creating Layer feat
I0713 15:33:00.045619 29118 net.cpp:425] feat <- ip2
I0713 15:33:00.045624 29118 net.cpp:399] feat -> feat
I0713 15:33:00.045635 29118 net.cpp:141] Setting up feat
I0713 15:33:00.045651 29118 net.cpp:148] Top shape: 64 2 (128)
I0713 15:33:00.045655 29118 net.cpp:156] Memory required for data: 5772544
I0713 15:33:00.045660 29118 layer_factory.hpp:77] Creating layer conv1_p
I0713 15:33:00.045672 29118 net.cpp:91] Creating Layer conv1_p
I0713 15:33:00.045675 29118 net.cpp:425] conv1_p <- data_p
I0713 15:33:00.045681 29118 net.cpp:399] conv1_p -> conv1_p
I0713 15:33:00.045722 29118 net.cpp:141] Setting up conv1_p
I0713 15:33:00.045727 29118 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0713 15:33:00.045730 29118 net.cpp:156] Memory required for data: 8721664
I0713 15:33:00.045733 29118 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0713 15:33:00.045737 29118 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0713 15:33:00.045753 29118 layer_factory.hpp:77] Creating layer pool1_p
I0713 15:33:00.045759 29118 net.cpp:91] Creating Layer pool1_p
I0713 15:33:00.045763 29118 net.cpp:425] pool1_p <- conv1_p
I0713 15:33:00.045768 29118 net.cpp:399] pool1_p -> pool1_p
I0713 15:33:00.045819 29118 net.cpp:141] Setting up pool1_p
I0713 15:33:00.045840 29118 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0713 15:33:00.045842 29118 net.cpp:156] Memory required for data: 9458944
I0713 15:33:00.045845 29118 layer_factory.hpp:77] Creating layer conv2_p
I0713 15:33:00.045857 29118 net.cpp:91] Creating Layer conv2_p
I0713 15:33:00.045861 29118 net.cpp:425] conv2_p <- pool1_p
I0713 15:33:00.045866 29118 net.cpp:399] conv2_p -> conv2_p
I0713 15:33:00.046095 29118 net.cpp:141] Setting up conv2_p
I0713 15:33:00.046114 29118 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0713 15:33:00.046118 29118 net.cpp:156] Memory required for data: 10278144
I0713 15:33:00.046121 29118 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0713 15:33:00.046125 29118 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0713 15:33:00.046128 29118 layer_factory.hpp:77] Creating layer pool2_p
I0713 15:33:00.046133 29118 net.cpp:91] Creating Layer pool2_p
I0713 15:33:00.046136 29118 net.cpp:425] pool2_p <- conv2_p
I0713 15:33:00.046146 29118 net.cpp:399] pool2_p -> pool2_p
I0713 15:33:00.046185 29118 net.cpp:141] Setting up pool2_p
I0713 15:33:00.046190 29118 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0713 15:33:00.046193 29118 net.cpp:156] Memory required for data: 10482944
I0713 15:33:00.046196 29118 layer_factory.hpp:77] Creating layer ip1_p
I0713 15:33:00.046205 29118 net.cpp:91] Creating Layer ip1_p
I0713 15:33:00.046208 29118 net.cpp:425] ip1_p <- pool2_p
I0713 15:33:00.046213 29118 net.cpp:399] ip1_p -> ip1_p
I0713 15:33:00.049453 29118 net.cpp:141] Setting up ip1_p
I0713 15:33:00.049527 29118 net.cpp:148] Top shape: 64 500 (32000)
I0713 15:33:00.049535 29118 net.cpp:156] Memory required for data: 10610944
I0713 15:33:00.049543 29118 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0713 15:33:00.049548 29118 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0713 15:33:00.049552 29118 layer_factory.hpp:77] Creating layer relu1_p
I0713 15:33:00.049562 29118 net.cpp:91] Creating Layer relu1_p
I0713 15:33:00.049566 29118 net.cpp:425] relu1_p <- ip1_p
I0713 15:33:00.049576 29118 net.cpp:386] relu1_p -> ip1_p (in-place)
I0713 15:33:00.049597 29118 net.cpp:141] Setting up relu1_p
I0713 15:33:00.049603 29118 net.cpp:148] Top shape: 64 500 (32000)
I0713 15:33:00.049607 29118 net.cpp:156] Memory required for data: 10738944
I0713 15:33:00.049609 29118 layer_factory.hpp:77] Creating layer ip2_p
I0713 15:33:00.049618 29118 net.cpp:91] Creating Layer ip2_p
I0713 15:33:00.049621 29118 net.cpp:425] ip2_p <- ip1_p
I0713 15:33:00.049628 29118 net.cpp:399] ip2_p -> ip2_p
I0713 15:33:00.049685 29118 net.cpp:141] Setting up ip2_p
I0713 15:33:00.049703 29118 net.cpp:148] Top shape: 64 10 (640)
I0713 15:33:00.049706 29118 net.cpp:156] Memory required for data: 10741504
I0713 15:33:00.049712 29118 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0713 15:33:00.049716 29118 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0713 15:33:00.049720 29118 layer_factory.hpp:77] Creating layer feat_p
I0713 15:33:00.049724 29118 net.cpp:91] Creating Layer feat_p
I0713 15:33:00.049727 29118 net.cpp:425] feat_p <- ip2_p
I0713 15:33:00.049734 29118 net.cpp:399] feat_p -> feat_p
I0713 15:33:00.049757 29118 net.cpp:141] Setting up feat_p
I0713 15:33:00.049762 29118 net.cpp:148] Top shape: 64 2 (128)
I0713 15:33:00.049764 29118 net.cpp:156] Memory required for data: 10742016
I0713 15:33:00.049767 29118 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0713 15:33:00.049772 29118 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0713 15:33:00.049774 29118 layer_factory.hpp:77] Creating layer loss
I0713 15:33:00.049788 29118 net.cpp:91] Creating Layer loss
I0713 15:33:00.049804 29118 net.cpp:425] loss <- feat
I0713 15:33:00.049808 29118 net.cpp:425] loss <- feat_p
I0713 15:33:00.049813 29118 net.cpp:425] loss <- sim
I0713 15:33:00.049821 29118 net.cpp:399] loss -> loss
I0713 15:33:00.049835 29118 net.cpp:141] Setting up loss
I0713 15:33:00.049851 29118 net.cpp:148] Top shape: (1)
I0713 15:33:00.049855 29118 net.cpp:151]     with loss weight 1
I0713 15:33:00.049873 29118 net.cpp:156] Memory required for data: 10742020
I0713 15:33:00.049890 29118 net.cpp:217] loss needs backward computation.
I0713 15:33:00.049893 29118 net.cpp:217] feat_p needs backward computation.
I0713 15:33:00.049896 29118 net.cpp:217] ip2_p needs backward computation.
I0713 15:33:00.049899 29118 net.cpp:217] relu1_p needs backward computation.
I0713 15:33:00.049902 29118 net.cpp:217] ip1_p needs backward computation.
I0713 15:33:00.049906 29118 net.cpp:217] pool2_p needs backward computation.
I0713 15:33:00.049908 29118 net.cpp:217] conv2_p needs backward computation.
I0713 15:33:00.049911 29118 net.cpp:217] pool1_p needs backward computation.
I0713 15:33:00.049914 29118 net.cpp:217] conv1_p needs backward computation.
I0713 15:33:00.049918 29118 net.cpp:217] feat needs backward computation.
I0713 15:33:00.049921 29118 net.cpp:217] ip2 needs backward computation.
I0713 15:33:00.049952 29118 net.cpp:217] relu1 needs backward computation.
I0713 15:33:00.049957 29118 net.cpp:217] ip1 needs backward computation.
I0713 15:33:00.049959 29118 net.cpp:217] pool2 needs backward computation.
I0713 15:33:00.049962 29118 net.cpp:217] conv2 needs backward computation.
I0713 15:33:00.049965 29118 net.cpp:217] pool1 needs backward computation.
I0713 15:33:00.049968 29118 net.cpp:217] conv1 needs backward computation.
I0713 15:33:00.049973 29118 net.cpp:219] slice_pair does not need backward computation.
I0713 15:33:00.049976 29118 net.cpp:219] pair_data does not need backward computation.
I0713 15:33:00.049979 29118 net.cpp:261] This network produces output loss
I0713 15:33:00.050113 29118 net.cpp:274] Network initialization done.
I0713 15:33:00.050673 29118 solver.cpp:181] Creating test net (#0) specified by net file: examples/siamese/mnist_siamese_train_test.prototxt
I0713 15:33:00.050746 29118 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0713 15:33:00.050899 29118 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb_0to6"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0713 15:33:00.050997 29118 layer_factory.hpp:77] Creating layer pair_data
I0713 15:33:00.051105 29118 net.cpp:91] Creating Layer pair_data
I0713 15:33:00.051126 29118 net.cpp:399] pair_data -> pair_data
I0713 15:33:00.051136 29118 net.cpp:399] pair_data -> sim
I0713 15:33:00.056080 29124 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb_0to6
I0713 15:33:00.056301 29118 data_layer.cpp:41] output data size: 100,2,28,28
I0713 15:33:00.057059 29118 net.cpp:141] Setting up pair_data
I0713 15:33:00.057111 29118 net.cpp:148] Top shape: 100 2 28 28 (156800)
I0713 15:33:00.057117 29118 net.cpp:148] Top shape: 100 (100)
I0713 15:33:00.057121 29118 net.cpp:156] Memory required for data: 627600
I0713 15:33:00.057127 29118 layer_factory.hpp:77] Creating layer slice_pair
I0713 15:33:00.057142 29118 net.cpp:91] Creating Layer slice_pair
I0713 15:33:00.057145 29118 net.cpp:425] slice_pair <- pair_data
I0713 15:33:00.057152 29118 net.cpp:399] slice_pair -> data
I0713 15:33:00.057160 29118 net.cpp:399] slice_pair -> data_p
I0713 15:33:00.057169 29118 net.cpp:141] Setting up slice_pair
I0713 15:33:00.057173 29118 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0713 15:33:00.057178 29118 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0713 15:33:00.057191 29118 net.cpp:156] Memory required for data: 1254800
I0713 15:33:00.057195 29118 layer_factory.hpp:77] Creating layer conv1
I0713 15:33:00.057206 29118 net.cpp:91] Creating Layer conv1
I0713 15:33:00.057209 29118 net.cpp:425] conv1 <- data
I0713 15:33:00.057214 29118 net.cpp:399] conv1 -> conv1
I0713 15:33:00.057242 29118 net.cpp:141] Setting up conv1
I0713 15:33:00.057258 29118 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0713 15:33:00.057261 29118 net.cpp:156] Memory required for data: 5862800
I0713 15:33:00.057345 29118 layer_factory.hpp:77] Creating layer pool1
I0713 15:33:00.057356 29118 net.cpp:91] Creating Layer pool1
I0713 15:33:00.057359 29118 net.cpp:425] pool1 <- conv1
I0713 15:33:00.057364 29118 net.cpp:399] pool1 -> pool1
I0713 15:33:00.057394 29118 net.cpp:141] Setting up pool1
I0713 15:33:00.057411 29118 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0713 15:33:00.057415 29118 net.cpp:156] Memory required for data: 7014800
I0713 15:33:00.057417 29118 layer_factory.hpp:77] Creating layer conv2
I0713 15:33:00.057426 29118 net.cpp:91] Creating Layer conv2
I0713 15:33:00.057461 29118 net.cpp:425] conv2 <- pool1
I0713 15:33:00.057468 29118 net.cpp:399] conv2 -> conv2
I0713 15:33:00.057672 29118 net.cpp:141] Setting up conv2
I0713 15:33:00.057688 29118 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0713 15:33:00.057693 29118 net.cpp:156] Memory required for data: 8294800
I0713 15:33:00.057698 29118 layer_factory.hpp:77] Creating layer pool2
I0713 15:33:00.057703 29118 net.cpp:91] Creating Layer pool2
I0713 15:33:00.057706 29118 net.cpp:425] pool2 <- conv2
I0713 15:33:00.057710 29118 net.cpp:399] pool2 -> pool2
I0713 15:33:00.057718 29118 net.cpp:141] Setting up pool2
I0713 15:33:00.057732 29118 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0713 15:33:00.057735 29118 net.cpp:156] Memory required for data: 8614800
I0713 15:33:00.057739 29118 layer_factory.hpp:77] Creating layer ip1
I0713 15:33:00.057744 29118 net.cpp:91] Creating Layer ip1
I0713 15:33:00.057747 29118 net.cpp:425] ip1 <- pool2
I0713 15:33:00.057752 29118 net.cpp:399] ip1 -> ip1
I0713 15:33:00.059432 29125 blocking_queue.cpp:50] Waiting for data
I0713 15:33:00.060648 29118 net.cpp:141] Setting up ip1
I0713 15:33:00.060680 29118 net.cpp:148] Top shape: 100 500 (50000)
I0713 15:33:00.060684 29118 net.cpp:156] Memory required for data: 8814800
I0713 15:33:00.060695 29118 layer_factory.hpp:77] Creating layer relu1
I0713 15:33:00.060703 29118 net.cpp:91] Creating Layer relu1
I0713 15:33:00.060706 29118 net.cpp:425] relu1 <- ip1
I0713 15:33:00.060711 29118 net.cpp:386] relu1 -> ip1 (in-place)
I0713 15:33:00.060719 29118 net.cpp:141] Setting up relu1
I0713 15:33:00.060724 29118 net.cpp:148] Top shape: 100 500 (50000)
I0713 15:33:00.060725 29118 net.cpp:156] Memory required for data: 9014800
I0713 15:33:00.060729 29118 layer_factory.hpp:77] Creating layer ip2
I0713 15:33:00.060736 29118 net.cpp:91] Creating Layer ip2
I0713 15:33:00.060739 29118 net.cpp:425] ip2 <- ip1
I0713 15:33:00.060744 29118 net.cpp:399] ip2 -> ip2
I0713 15:33:00.060791 29118 net.cpp:141] Setting up ip2
I0713 15:33:00.060807 29118 net.cpp:148] Top shape: 100 10 (1000)
I0713 15:33:00.060811 29118 net.cpp:156] Memory required for data: 9018800
I0713 15:33:00.060816 29118 layer_factory.hpp:77] Creating layer feat
I0713 15:33:00.060853 29118 net.cpp:91] Creating Layer feat
I0713 15:33:00.060868 29118 net.cpp:425] feat <- ip2
I0713 15:33:00.060873 29118 net.cpp:399] feat -> feat
I0713 15:33:00.060883 29118 net.cpp:141] Setting up feat
I0713 15:33:00.060888 29118 net.cpp:148] Top shape: 100 2 (200)
I0713 15:33:00.060890 29118 net.cpp:156] Memory required for data: 9019600
I0713 15:33:00.060896 29118 layer_factory.hpp:77] Creating layer conv1_p
I0713 15:33:00.060904 29118 net.cpp:91] Creating Layer conv1_p
I0713 15:33:00.060907 29118 net.cpp:425] conv1_p <- data_p
I0713 15:33:00.060912 29118 net.cpp:399] conv1_p -> conv1_p
I0713 15:33:00.060937 29118 net.cpp:141] Setting up conv1_p
I0713 15:33:00.060953 29118 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0713 15:33:00.060956 29118 net.cpp:156] Memory required for data: 13627600
I0713 15:33:00.060961 29118 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0713 15:33:00.060963 29118 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0713 15:33:00.060967 29118 layer_factory.hpp:77] Creating layer pool1_p
I0713 15:33:00.060973 29118 net.cpp:91] Creating Layer pool1_p
I0713 15:33:00.060976 29118 net.cpp:425] pool1_p <- conv1_p
I0713 15:33:00.060981 29118 net.cpp:399] pool1_p -> pool1_p
I0713 15:33:00.060988 29118 net.cpp:141] Setting up pool1_p
I0713 15:33:00.060993 29118 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0713 15:33:00.060995 29118 net.cpp:156] Memory required for data: 14779600
I0713 15:33:00.060998 29118 layer_factory.hpp:77] Creating layer conv2_p
I0713 15:33:00.061004 29118 net.cpp:91] Creating Layer conv2_p
I0713 15:33:00.061007 29118 net.cpp:425] conv2_p <- pool1_p
I0713 15:33:00.061013 29118 net.cpp:399] conv2_p -> conv2_p
I0713 15:33:00.061416 29118 net.cpp:141] Setting up conv2_p
I0713 15:33:00.061432 29118 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0713 15:33:00.061453 29118 net.cpp:156] Memory required for data: 16059600
I0713 15:33:00.061458 29118 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0713 15:33:00.061463 29118 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0713 15:33:00.061466 29118 layer_factory.hpp:77] Creating layer pool2_p
I0713 15:33:00.061472 29118 net.cpp:91] Creating Layer pool2_p
I0713 15:33:00.061475 29118 net.cpp:425] pool2_p <- conv2_p
I0713 15:33:00.061480 29118 net.cpp:399] pool2_p -> pool2_p
I0713 15:33:00.061491 29118 net.cpp:141] Setting up pool2_p
I0713 15:33:00.061496 29118 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0713 15:33:00.061498 29118 net.cpp:156] Memory required for data: 16379600
I0713 15:33:00.061501 29118 layer_factory.hpp:77] Creating layer ip1_p
I0713 15:33:00.061507 29118 net.cpp:91] Creating Layer ip1_p
I0713 15:33:00.061509 29118 net.cpp:425] ip1_p <- pool2_p
I0713 15:33:00.061516 29118 net.cpp:399] ip1_p -> ip1_p
I0713 15:33:00.064556 29118 net.cpp:141] Setting up ip1_p
I0713 15:33:00.064626 29118 net.cpp:148] Top shape: 100 500 (50000)
I0713 15:33:00.064630 29118 net.cpp:156] Memory required for data: 16579600
I0713 15:33:00.064637 29118 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0713 15:33:00.064642 29118 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0713 15:33:00.064646 29118 layer_factory.hpp:77] Creating layer relu1_p
I0713 15:33:00.064657 29118 net.cpp:91] Creating Layer relu1_p
I0713 15:33:00.064661 29118 net.cpp:425] relu1_p <- ip1_p
I0713 15:33:00.064668 29118 net.cpp:386] relu1_p -> ip1_p (in-place)
I0713 15:33:00.064678 29118 net.cpp:141] Setting up relu1_p
I0713 15:33:00.064682 29118 net.cpp:148] Top shape: 100 500 (50000)
I0713 15:33:00.064684 29118 net.cpp:156] Memory required for data: 16779600
I0713 15:33:00.064687 29118 layer_factory.hpp:77] Creating layer ip2_p
I0713 15:33:00.064700 29118 net.cpp:91] Creating Layer ip2_p
I0713 15:33:00.064703 29118 net.cpp:425] ip2_p <- ip1_p
I0713 15:33:00.064709 29118 net.cpp:399] ip2_p -> ip2_p
I0713 15:33:00.064764 29118 net.cpp:141] Setting up ip2_p
I0713 15:33:00.064769 29118 net.cpp:148] Top shape: 100 10 (1000)
I0713 15:33:00.064770 29118 net.cpp:156] Memory required for data: 16783600
I0713 15:33:00.064776 29118 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0713 15:33:00.064780 29118 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0713 15:33:00.064784 29118 layer_factory.hpp:77] Creating layer feat_p
I0713 15:33:00.064788 29118 net.cpp:91] Creating Layer feat_p
I0713 15:33:00.064791 29118 net.cpp:425] feat_p <- ip2_p
I0713 15:33:00.064796 29118 net.cpp:399] feat_p -> feat_p
I0713 15:33:00.064805 29118 net.cpp:141] Setting up feat_p
I0713 15:33:00.064810 29118 net.cpp:148] Top shape: 100 2 (200)
I0713 15:33:00.064811 29118 net.cpp:156] Memory required for data: 16784400
I0713 15:33:00.064815 29118 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0713 15:33:00.064817 29118 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0713 15:33:00.064821 29118 layer_factory.hpp:77] Creating layer loss
I0713 15:33:00.064826 29118 net.cpp:91] Creating Layer loss
I0713 15:33:00.064829 29118 net.cpp:425] loss <- feat
I0713 15:33:00.064832 29118 net.cpp:425] loss <- feat_p
I0713 15:33:00.064836 29118 net.cpp:425] loss <- sim
I0713 15:33:00.064842 29118 net.cpp:399] loss -> loss
I0713 15:33:00.064852 29118 net.cpp:141] Setting up loss
I0713 15:33:00.064856 29118 net.cpp:148] Top shape: (1)
I0713 15:33:00.064858 29118 net.cpp:151]     with loss weight 1
I0713 15:33:00.064872 29118 net.cpp:156] Memory required for data: 16784404
I0713 15:33:00.064875 29118 net.cpp:217] loss needs backward computation.
I0713 15:33:00.064878 29118 net.cpp:217] feat_p needs backward computation.
I0713 15:33:00.064882 29118 net.cpp:217] ip2_p needs backward computation.
I0713 15:33:00.064884 29118 net.cpp:217] relu1_p needs backward computation.
I0713 15:33:00.064911 29118 net.cpp:217] ip1_p needs backward computation.
I0713 15:33:00.064914 29118 net.cpp:217] pool2_p needs backward computation.
I0713 15:33:00.064918 29118 net.cpp:217] conv2_p needs backward computation.
I0713 15:33:00.064920 29118 net.cpp:217] pool1_p needs backward computation.
I0713 15:33:00.064924 29118 net.cpp:217] conv1_p needs backward computation.
I0713 15:33:00.064925 29118 net.cpp:217] feat needs backward computation.
I0713 15:33:00.064929 29118 net.cpp:217] ip2 needs backward computation.
I0713 15:33:00.064931 29118 net.cpp:217] relu1 needs backward computation.
I0713 15:33:00.064934 29118 net.cpp:217] ip1 needs backward computation.
I0713 15:33:00.064937 29118 net.cpp:217] pool2 needs backward computation.
I0713 15:33:00.064939 29118 net.cpp:217] conv2 needs backward computation.
I0713 15:33:00.064942 29118 net.cpp:217] pool1 needs backward computation.
I0713 15:33:00.064945 29118 net.cpp:217] conv1 needs backward computation.
I0713 15:33:00.064949 29118 net.cpp:219] slice_pair does not need backward computation.
I0713 15:33:00.064952 29118 net.cpp:219] pair_data does not need backward computation.
I0713 15:33:00.064955 29118 net.cpp:261] This network produces output loss
I0713 15:33:00.064975 29118 net.cpp:274] Network initialization done.
I0713 15:33:00.065083 29118 solver.cpp:60] Solver scaffolding done.
I0713 15:33:00.065106 29118 caffe.cpp:219] Starting Optimization
I0713 15:33:00.065110 29118 solver.cpp:279] Solving mnist_siamese_train_test
I0713 15:33:00.065112 29118 solver.cpp:280] Learning Rate Policy: inv
I0713 15:33:00.067055 29118 solver.cpp:337] Iteration 0, Testing net (#0)
I0713 15:33:10.482722 29118 solver.cpp:404]     Test net output #0: loss = 0.180353 (* 1 = 0.180353 loss)
I0713 15:33:10.811990 29118 solver.cpp:228] Iteration 0, loss = 0.187258
I0713 15:33:10.812090 29118 solver.cpp:244]     Train net output #0: loss = 0.187258 (* 1 = 0.187258 loss)
I0713 15:33:10.812124 29118 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0713 15:33:29.239578 29118 solver.cpp:228] Iteration 100, loss = 0.0508582
I0713 15:33:29.239744 29118 solver.cpp:244]     Train net output #0: loss = 0.0508582 (* 1 = 0.0508582 loss)
I0713 15:33:29.239773 29118 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0713 15:33:47.070534 29118 solver.cpp:228] Iteration 200, loss = 0.0247363
I0713 15:33:47.070636 29118 solver.cpp:244]     Train net output #0: loss = 0.0247363 (* 1 = 0.0247363 loss)
I0713 15:33:47.070647 29118 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0713 15:34:04.488010 29118 solver.cpp:228] Iteration 300, loss = 0.0215001
I0713 15:34:04.488066 29118 solver.cpp:244]     Train net output #0: loss = 0.0215001 (* 1 = 0.0215001 loss)
I0713 15:34:04.488076 29118 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0713 15:34:22.878509 29118 solver.cpp:228] Iteration 400, loss = 0.0197396
I0713 15:34:22.878702 29118 solver.cpp:244]     Train net output #0: loss = 0.0197396 (* 1 = 0.0197396 loss)
I0713 15:34:22.878726 29118 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0713 15:34:38.777523 29118 solver.cpp:337] Iteration 500, Testing net (#0)
I0713 15:34:48.712231 29118 solver.cpp:404]     Test net output #0: loss = 0.0255872 (* 1 = 0.0255872 loss)
I0713 15:34:48.895036 29118 solver.cpp:228] Iteration 500, loss = 0.047781
I0713 15:34:48.895095 29118 solver.cpp:244]     Train net output #0: loss = 0.047781 (* 1 = 0.047781 loss)
I0713 15:34:48.895105 29118 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0713 15:35:04.786841 29118 solver.cpp:228] Iteration 600, loss = 0.0266093
I0713 15:35:04.787065 29118 solver.cpp:244]     Train net output #0: loss = 0.0266093 (* 1 = 0.0266093 loss)
I0713 15:35:04.787080 29118 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0713 15:35:20.700551 29118 solver.cpp:228] Iteration 700, loss = 0.0322001
I0713 15:35:20.700609 29118 solver.cpp:244]     Train net output #0: loss = 0.0322001 (* 1 = 0.0322001 loss)
I0713 15:35:20.700620 29118 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0713 15:35:36.565441 29118 solver.cpp:228] Iteration 800, loss = 0.0142927
I0713 15:35:36.566260 29118 solver.cpp:244]     Train net output #0: loss = 0.0142927 (* 1 = 0.0142927 loss)
I0713 15:35:36.566334 29118 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0713 15:35:52.256908 29118 solver.cpp:228] Iteration 900, loss = 0.0298361
I0713 15:35:52.257105 29118 solver.cpp:244]     Train net output #0: loss = 0.0298361 (* 1 = 0.0298361 loss)
I0713 15:35:52.257138 29118 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0713 15:36:11.262356 29118 solver.cpp:337] Iteration 1000, Testing net (#0)
I0713 15:36:21.875686 29118 solver.cpp:404]     Test net output #0: loss = 0.0201349 (* 1 = 0.0201349 loss)
I0713 15:36:22.063699 29118 solver.cpp:228] Iteration 1000, loss = 0.0264767
I0713 15:36:22.063850 29118 solver.cpp:244]     Train net output #0: loss = 0.0264767 (* 1 = 0.0264767 loss)
I0713 15:36:22.063882 29118 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0713 15:36:38.150590 29118 solver.cpp:228] Iteration 1100, loss = 0.0200438
I0713 15:36:38.150646 29118 solver.cpp:244]     Train net output #0: loss = 0.0200438 (* 1 = 0.0200438 loss)
I0713 15:36:38.150658 29118 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0713 15:36:55.205971 29118 solver.cpp:228] Iteration 1200, loss = 0.0151883
I0713 15:36:55.206303 29118 solver.cpp:244]     Train net output #0: loss = 0.0151883 (* 1 = 0.0151883 loss)
I0713 15:36:55.206395 29118 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0713 15:37:13.783572 29118 solver.cpp:228] Iteration 1300, loss = 0.00617088
I0713 15:37:13.783629 29118 solver.cpp:244]     Train net output #0: loss = 0.00617088 (* 1 = 0.00617088 loss)
I0713 15:37:13.783640 29118 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0713 15:37:31.353456 29118 solver.cpp:228] Iteration 1400, loss = 0.0190306
I0713 15:37:31.353798 29118 solver.cpp:244]     Train net output #0: loss = 0.0190306 (* 1 = 0.0190306 loss)
I0713 15:37:31.353871 29118 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0713 15:37:47.577361 29118 solver.cpp:337] Iteration 1500, Testing net (#0)
I0713 15:37:58.594123 29118 solver.cpp:404]     Test net output #0: loss = 0.0169069 (* 1 = 0.0169069 loss)
I0713 15:37:58.779796 29118 solver.cpp:228] Iteration 1500, loss = 0.0206562
I0713 15:37:58.779861 29118 solver.cpp:244]     Train net output #0: loss = 0.0206562 (* 1 = 0.0206562 loss)
I0713 15:37:58.779875 29118 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0713 15:38:16.077846 29118 solver.cpp:228] Iteration 1600, loss = 0.0164131
I0713 15:38:16.078255 29118 solver.cpp:244]     Train net output #0: loss = 0.0164131 (* 1 = 0.0164131 loss)
I0713 15:38:16.078332 29118 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0713 15:38:32.918923 29118 solver.cpp:228] Iteration 1700, loss = 0.00940787
I0713 15:38:32.918982 29118 solver.cpp:244]     Train net output #0: loss = 0.00940787 (* 1 = 0.00940787 loss)
I0713 15:38:32.918992 29118 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0713 15:38:48.922751 29118 solver.cpp:228] Iteration 1800, loss = 0.0153236
I0713 15:38:48.922935 29118 solver.cpp:244]     Train net output #0: loss = 0.0153236 (* 1 = 0.0153236 loss)
I0713 15:38:48.922948 29118 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0713 15:39:04.869197 29118 solver.cpp:228] Iteration 1900, loss = 0.014114
I0713 15:39:04.869256 29118 solver.cpp:244]     Train net output #0: loss = 0.014114 (* 1 = 0.014114 loss)
I0713 15:39:04.869267 29118 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0713 15:39:20.910846 29118 solver.cpp:337] Iteration 2000, Testing net (#0)
I0713 15:39:31.674222 29118 solver.cpp:404]     Test net output #0: loss = 0.015258 (* 1 = 0.015258 loss)
I0713 15:39:31.939615 29118 solver.cpp:228] Iteration 2000, loss = 0.00950842
I0713 15:39:31.939677 29118 solver.cpp:244]     Train net output #0: loss = 0.00950841 (* 1 = 0.00950841 loss)
I0713 15:39:31.939687 29118 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0713 15:39:48.052543 29118 solver.cpp:228] Iteration 2100, loss = 0.0130755
I0713 15:39:48.052603 29118 solver.cpp:244]     Train net output #0: loss = 0.0130755 (* 1 = 0.0130755 loss)
I0713 15:39:48.052613 29118 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0713 15:40:04.004304 29118 solver.cpp:228] Iteration 2200, loss = 0.0156733
I0713 15:40:04.004724 29118 solver.cpp:244]     Train net output #0: loss = 0.0156732 (* 1 = 0.0156732 loss)
I0713 15:40:04.004802 29118 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0713 15:40:20.638334 29118 solver.cpp:228] Iteration 2300, loss = 0.024778
I0713 15:40:20.638391 29118 solver.cpp:244]     Train net output #0: loss = 0.024778 (* 1 = 0.024778 loss)
I0713 15:40:20.638401 29118 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0713 15:40:36.602555 29118 solver.cpp:228] Iteration 2400, loss = 0.0083068
I0713 15:40:36.602978 29118 solver.cpp:244]     Train net output #0: loss = 0.00830678 (* 1 = 0.00830678 loss)
I0713 15:40:36.603052 29118 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0713 15:40:53.254976 29118 solver.cpp:337] Iteration 2500, Testing net (#0)
I0713 15:41:04.017215 29118 solver.cpp:404]     Test net output #0: loss = 0.0142469 (* 1 = 0.0142469 loss)
I0713 15:41:04.213034 29118 solver.cpp:228] Iteration 2500, loss = 0.0124384
I0713 15:41:04.213091 29118 solver.cpp:244]     Train net output #0: loss = 0.0124384 (* 1 = 0.0124384 loss)
I0713 15:41:04.213102 29118 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0713 15:41:20.376052 29118 solver.cpp:228] Iteration 2600, loss = 0.0117811
I0713 15:41:20.376384 29118 solver.cpp:244]     Train net output #0: loss = 0.0117811 (* 1 = 0.0117811 loss)
I0713 15:41:20.376456 29118 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0713 15:41:39.053712 29118 solver.cpp:228] Iteration 2700, loss = 0.00881047
I0713 15:41:39.053782 29118 solver.cpp:244]     Train net output #0: loss = 0.00881046 (* 1 = 0.00881046 loss)
I0713 15:41:39.053795 29118 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0713 15:41:56.795987 29118 solver.cpp:228] Iteration 2800, loss = 0.0209917
I0713 15:41:56.796159 29118 solver.cpp:244]     Train net output #0: loss = 0.0209917 (* 1 = 0.0209917 loss)
I0713 15:41:56.796171 29118 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0713 15:42:14.272644 29118 solver.cpp:228] Iteration 2900, loss = 0.0185084
I0713 15:42:14.272899 29118 solver.cpp:244]     Train net output #0: loss = 0.0185084 (* 1 = 0.0185084 loss)
I0713 15:42:14.272943 29118 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0713 15:42:30.382112 29118 solver.cpp:337] Iteration 3000, Testing net (#0)
I0713 15:42:40.530598 29118 solver.cpp:404]     Test net output #0: loss = 0.0135564 (* 1 = 0.0135564 loss)
I0713 15:42:40.735532 29118 solver.cpp:228] Iteration 3000, loss = 0.00582833
I0713 15:42:40.735594 29118 solver.cpp:244]     Train net output #0: loss = 0.00582833 (* 1 = 0.00582833 loss)
I0713 15:42:40.735635 29118 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0713 15:42:56.784027 29118 solver.cpp:228] Iteration 3100, loss = 0.0162901
I0713 15:42:56.784260 29118 solver.cpp:244]     Train net output #0: loss = 0.0162901 (* 1 = 0.0162901 loss)
I0713 15:42:56.784330 29118 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0713 15:43:12.759563 29118 solver.cpp:228] Iteration 3200, loss = 0.0133711
I0713 15:43:12.759719 29118 solver.cpp:244]     Train net output #0: loss = 0.0133711 (* 1 = 0.0133711 loss)
I0713 15:43:12.759732 29118 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0713 15:43:28.804747 29118 solver.cpp:228] Iteration 3300, loss = 0.010066
I0713 15:43:28.804803 29118 solver.cpp:244]     Train net output #0: loss = 0.010066 (* 1 = 0.010066 loss)
I0713 15:43:28.804814 29118 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0713 15:43:44.896993 29118 solver.cpp:228] Iteration 3400, loss = 0.0133901
I0713 15:43:44.897320 29118 solver.cpp:244]     Train net output #0: loss = 0.0133902 (* 1 = 0.0133902 loss)
I0713 15:43:44.897393 29118 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0713 15:44:00.849602 29118 solver.cpp:337] Iteration 3500, Testing net (#0)
I0713 15:44:10.909977 29118 solver.cpp:404]     Test net output #0: loss = 0.0126185 (* 1 = 0.0126185 loss)
I0713 15:44:11.088984 29118 solver.cpp:228] Iteration 3500, loss = 0.010961
I0713 15:44:11.089061 29118 solver.cpp:244]     Train net output #0: loss = 0.010961 (* 1 = 0.010961 loss)
I0713 15:44:11.089201 29118 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0713 15:44:27.004762 29118 solver.cpp:228] Iteration 3600, loss = 0.0115522
I0713 15:44:27.005025 29118 solver.cpp:244]     Train net output #0: loss = 0.0115523 (* 1 = 0.0115523 loss)
I0713 15:44:27.005038 29118 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0713 15:44:43.031937 29118 solver.cpp:228] Iteration 3700, loss = 0.00635544
I0713 15:44:43.031994 29118 solver.cpp:244]     Train net output #0: loss = 0.00635545 (* 1 = 0.00635545 loss)
I0713 15:44:43.032006 29118 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0713 15:44:59.073067 29118 solver.cpp:228] Iteration 3800, loss = 0.00886301
I0713 15:44:59.073297 29118 solver.cpp:244]     Train net output #0: loss = 0.00886302 (* 1 = 0.00886302 loss)
I0713 15:44:59.073309 29118 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0713 15:45:15.250592 29118 solver.cpp:228] Iteration 3900, loss = 0.0101712
I0713 15:45:15.250653 29118 solver.cpp:244]     Train net output #0: loss = 0.0101712 (* 1 = 0.0101712 loss)
I0713 15:45:15.250663 29118 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0713 15:45:37.316506 29118 solver.cpp:337] Iteration 4000, Testing net (#0)
I0713 15:45:48.610438 29118 solver.cpp:404]     Test net output #0: loss = 0.0117714 (* 1 = 0.0117714 loss)
I0713 15:45:48.783797 29118 solver.cpp:228] Iteration 4000, loss = 0.00593368
I0713 15:45:48.783854 29118 solver.cpp:244]     Train net output #0: loss = 0.00593369 (* 1 = 0.00593369 loss)
I0713 15:45:48.783864 29118 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0713 15:46:04.811463 29118 solver.cpp:228] Iteration 4100, loss = 0.0124384
I0713 15:46:04.811518 29118 solver.cpp:244]     Train net output #0: loss = 0.0124384 (* 1 = 0.0124384 loss)
I0713 15:46:04.811529 29118 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0713 15:46:21.431022 29118 solver.cpp:228] Iteration 4200, loss = 0.00654618
I0713 15:46:21.431108 29118 solver.cpp:244]     Train net output #0: loss = 0.00654619 (* 1 = 0.00654619 loss)
I0713 15:46:21.431120 29118 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0713 15:46:38.370654 29118 solver.cpp:228] Iteration 4300, loss = 0.0228192
I0713 15:46:38.370712 29118 solver.cpp:244]     Train net output #0: loss = 0.0228192 (* 1 = 0.0228192 loss)
I0713 15:46:38.370723 29118 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0713 15:46:55.160830 29118 solver.cpp:228] Iteration 4400, loss = 0.0070084
I0713 15:46:55.161067 29118 solver.cpp:244]     Train net output #0: loss = 0.00700841 (* 1 = 0.00700841 loss)
I0713 15:46:55.161082 29118 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0713 15:47:11.388144 29118 solver.cpp:337] Iteration 4500, Testing net (#0)
I0713 15:47:21.634567 29118 solver.cpp:404]     Test net output #0: loss = 0.0113754 (* 1 = 0.0113754 loss)
I0713 15:47:21.844666 29118 solver.cpp:228] Iteration 4500, loss = 0.00638483
I0713 15:47:21.844770 29118 solver.cpp:244]     Train net output #0: loss = 0.00638485 (* 1 = 0.00638485 loss)
I0713 15:47:21.844795 29118 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0713 15:47:38.591008 29118 solver.cpp:228] Iteration 4600, loss = 0.00943361
I0713 15:47:38.591339 29118 solver.cpp:244]     Train net output #0: loss = 0.00943362 (* 1 = 0.00943362 loss)
I0713 15:47:38.591413 29118 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0713 15:47:54.443369 29118 solver.cpp:228] Iteration 4700, loss = 0.00709885
I0713 15:47:54.443428 29118 solver.cpp:244]     Train net output #0: loss = 0.00709886 (* 1 = 0.00709886 loss)
I0713 15:47:54.443439 29118 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0713 15:48:12.350558 29118 solver.cpp:228] Iteration 4800, loss = 0.00558713
I0713 15:48:12.350900 29118 solver.cpp:244]     Train net output #0: loss = 0.00558714 (* 1 = 0.00558714 loss)
I0713 15:48:12.350951 29118 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0713 15:48:28.437166 29118 solver.cpp:228] Iteration 4900, loss = 0.00723811
I0713 15:48:28.437225 29118 solver.cpp:244]     Train net output #0: loss = 0.00723812 (* 1 = 0.00723812 loss)
I0713 15:48:28.437237 29118 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0713 15:48:44.441964 29118 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to6lt06_iter_5000.caffemodel
I0713 15:48:44.453346 29118 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to6lt06_iter_5000.solverstate
I0713 15:48:44.560129 29118 solver.cpp:317] Iteration 5000, loss = 0.0085035
I0713 15:48:44.560187 29118 solver.cpp:337] Iteration 5000, Testing net (#0)
I0713 15:48:54.797704 29118 solver.cpp:404]     Test net output #0: loss = 0.0110596 (* 1 = 0.0110596 loss)
I0713 15:48:54.797965 29118 solver.cpp:322] Optimization Done.
I0713 15:48:54.798046 29118 caffe.cpp:222] Optimization Done.
