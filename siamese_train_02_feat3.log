I0714 07:54:59.900063 32200 caffe.cpp:178] Use CPU.
I0714 07:54:59.903925 32200 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 5000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0
snapshot: 1000
snapshot_prefix: "examples/siamese/My_mnist_siamese_0to2_feat3"
solver_mode: CPU
net: "examples/siamese/mnist_siamese_train_test_feat3.prototxt"
I0714 07:54:59.905947 32200 solver.cpp:91] Creating training net from net file: examples/siamese/mnist_siamese_train_test_feat3.prototxt
I0714 07:54:59.906523 32200 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer pair_data
I0714 07:54:59.910064 32200 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test_feat3"
state {
  phase: TRAIN
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_train_leveldb_0to2"
    batch_size: 64
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0714 07:54:59.911592 32200 layer_factory.hpp:77] Creating layer pair_data
I0714 07:54:59.912400 32200 net.cpp:91] Creating Layer pair_data
I0714 07:54:59.912490 32200 net.cpp:399] pair_data -> pair_data
I0714 07:54:59.912562 32200 net.cpp:399] pair_data -> sim
I0714 07:54:59.925297 32204 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_train_leveldb_0to2
I0714 07:54:59.941314 32200 data_layer.cpp:41] output data size: 64,2,28,28
I0714 07:54:59.943320 32200 net.cpp:141] Setting up pair_data
I0714 07:54:59.943516 32200 net.cpp:148] Top shape: 64 2 28 28 (100352)
I0714 07:54:59.943555 32200 net.cpp:148] Top shape: 64 (64)
I0714 07:54:59.943564 32200 net.cpp:156] Memory required for data: 401664
I0714 07:54:59.943701 32200 layer_factory.hpp:77] Creating layer slice_pair
I0714 07:54:59.943763 32200 net.cpp:91] Creating Layer slice_pair
I0714 07:54:59.943778 32200 net.cpp:425] slice_pair <- pair_data
I0714 07:54:59.943800 32200 net.cpp:399] slice_pair -> data
I0714 07:54:59.943820 32200 net.cpp:399] slice_pair -> data_p
I0714 07:54:59.943837 32200 net.cpp:141] Setting up slice_pair
I0714 07:54:59.943848 32200 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0714 07:54:59.943856 32200 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0714 07:54:59.943861 32200 net.cpp:156] Memory required for data: 803072
I0714 07:54:59.943866 32200 layer_factory.hpp:77] Creating layer conv1
I0714 07:54:59.943931 32200 net.cpp:91] Creating Layer conv1
I0714 07:54:59.943943 32200 net.cpp:425] conv1 <- data
I0714 07:54:59.943964 32200 net.cpp:399] conv1 -> conv1
I0714 07:54:59.944125 32200 net.cpp:141] Setting up conv1
I0714 07:54:59.944152 32200 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0714 07:54:59.944160 32200 net.cpp:156] Memory required for data: 3752192
I0714 07:54:59.944205 32200 layer_factory.hpp:77] Creating layer pool1
I0714 07:54:59.944218 32200 net.cpp:91] Creating Layer pool1
I0714 07:54:59.944226 32200 net.cpp:425] pool1 <- conv1
I0714 07:54:59.944241 32200 net.cpp:399] pool1 -> pool1
I0714 07:54:59.944325 32200 net.cpp:141] Setting up pool1
I0714 07:54:59.944334 32200 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0714 07:54:59.944340 32200 net.cpp:156] Memory required for data: 4489472
I0714 07:54:59.944346 32200 layer_factory.hpp:77] Creating layer conv2
I0714 07:54:59.944376 32200 net.cpp:91] Creating Layer conv2
I0714 07:54:59.944386 32200 net.cpp:425] conv2 <- pool1
I0714 07:54:59.944396 32200 net.cpp:399] conv2 -> conv2
I0714 07:54:59.944870 32200 net.cpp:141] Setting up conv2
I0714 07:54:59.944910 32200 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0714 07:54:59.944982 32200 net.cpp:156] Memory required for data: 5308672
I0714 07:54:59.945006 32200 layer_factory.hpp:77] Creating layer pool2
I0714 07:54:59.945020 32200 net.cpp:91] Creating Layer pool2
I0714 07:54:59.945173 32200 net.cpp:425] pool2 <- conv2
I0714 07:54:59.945189 32200 net.cpp:399] pool2 -> pool2
I0714 07:54:59.945219 32200 net.cpp:141] Setting up pool2
I0714 07:54:59.945250 32200 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0714 07:54:59.945261 32200 net.cpp:156] Memory required for data: 5513472
I0714 07:54:59.945272 32200 layer_factory.hpp:77] Creating layer ip1
I0714 07:54:59.945338 32200 net.cpp:91] Creating Layer ip1
I0714 07:54:59.945355 32200 net.cpp:425] ip1 <- pool2
I0714 07:54:59.945405 32200 net.cpp:399] ip1 -> ip1
I0714 07:54:59.951606 32200 net.cpp:141] Setting up ip1
I0714 07:54:59.963681 32200 net.cpp:148] Top shape: 64 500 (32000)
I0714 07:54:59.963788 32200 net.cpp:156] Memory required for data: 5641472
I0714 07:54:59.963845 32200 layer_factory.hpp:77] Creating layer relu1
I0714 07:54:59.963888 32200 net.cpp:91] Creating Layer relu1
I0714 07:54:59.963919 32200 net.cpp:425] relu1 <- ip1
I0714 07:54:59.963963 32200 net.cpp:386] relu1 -> ip1 (in-place)
I0714 07:54:59.964005 32200 net.cpp:141] Setting up relu1
I0714 07:54:59.964038 32200 net.cpp:148] Top shape: 64 500 (32000)
I0714 07:54:59.964066 32200 net.cpp:156] Memory required for data: 5769472
I0714 07:54:59.964092 32200 layer_factory.hpp:77] Creating layer ip2
I0714 07:54:59.964128 32200 net.cpp:91] Creating Layer ip2
I0714 07:54:59.964332 32200 net.cpp:425] ip2 <- ip1
I0714 07:54:59.964385 32200 net.cpp:399] ip2 -> ip2
I0714 07:54:59.964498 32200 net.cpp:141] Setting up ip2
I0714 07:54:59.964534 32200 net.cpp:148] Top shape: 64 10 (640)
I0714 07:54:59.964560 32200 net.cpp:156] Memory required for data: 5772032
I0714 07:54:59.964586 32200 layer_factory.hpp:77] Creating layer feat
I0714 07:54:59.964617 32200 net.cpp:91] Creating Layer feat
I0714 07:54:59.964643 32200 net.cpp:425] feat <- ip2
I0714 07:54:59.964673 32200 net.cpp:399] feat -> feat
I0714 07:54:59.964715 32200 net.cpp:141] Setting up feat
I0714 07:54:59.964743 32200 net.cpp:148] Top shape: 64 3 (192)
I0714 07:54:59.964766 32200 net.cpp:156] Memory required for data: 5772800
I0714 07:54:59.964797 32200 layer_factory.hpp:77] Creating layer conv1_p
I0714 07:54:59.964833 32200 net.cpp:91] Creating Layer conv1_p
I0714 07:54:59.964861 32200 net.cpp:425] conv1_p <- data_p
I0714 07:54:59.964892 32200 net.cpp:399] conv1_p -> conv1_p
I0714 07:54:59.964994 32200 net.cpp:141] Setting up conv1_p
I0714 07:54:59.965032 32200 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0714 07:54:59.965055 32200 net.cpp:156] Memory required for data: 8721920
I0714 07:54:59.965080 32200 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0714 07:54:59.965106 32200 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0714 07:54:59.965128 32200 layer_factory.hpp:77] Creating layer pool1_p
I0714 07:54:59.965162 32200 net.cpp:91] Creating Layer pool1_p
I0714 07:54:59.965189 32200 net.cpp:425] pool1_p <- conv1_p
I0714 07:54:59.971631 32200 net.cpp:399] pool1_p -> pool1_p
I0714 07:54:59.972935 32200 net.cpp:141] Setting up pool1_p
I0714 07:54:59.973042 32200 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0714 07:54:59.973085 32200 net.cpp:156] Memory required for data: 9459200
I0714 07:54:59.973121 32200 layer_factory.hpp:77] Creating layer conv2_p
I0714 07:54:59.973160 32200 net.cpp:91] Creating Layer conv2_p
I0714 07:54:59.973223 32200 net.cpp:425] conv2_p <- pool1_p
I0714 07:54:59.973364 32200 net.cpp:399] conv2_p -> conv2_p
I0714 07:54:59.974889 32200 net.cpp:141] Setting up conv2_p
I0714 07:54:59.974949 32200 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0714 07:54:59.974969 32200 net.cpp:156] Memory required for data: 10278400
I0714 07:54:59.974989 32200 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0714 07:54:59.975009 32200 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0714 07:54:59.975028 32200 layer_factory.hpp:77] Creating layer pool2_p
I0714 07:54:59.975050 32200 net.cpp:91] Creating Layer pool2_p
I0714 07:54:59.975077 32200 net.cpp:425] pool2_p <- conv2_p
I0714 07:54:59.975114 32200 net.cpp:399] pool2_p -> pool2_p
I0714 07:54:59.975143 32200 net.cpp:141] Setting up pool2_p
I0714 07:54:59.975164 32200 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0714 07:54:59.975179 32200 net.cpp:156] Memory required for data: 10483200
I0714 07:54:59.975200 32200 layer_factory.hpp:77] Creating layer ip1_p
I0714 07:54:59.975230 32200 net.cpp:91] Creating Layer ip1_p
I0714 07:54:59.975252 32200 net.cpp:425] ip1_p <- pool2_p
I0714 07:54:59.975275 32200 net.cpp:399] ip1_p -> ip1_p
I0714 07:54:59.978909 32200 net.cpp:141] Setting up ip1_p
I0714 07:54:59.979094 32200 net.cpp:148] Top shape: 64 500 (32000)
I0714 07:54:59.979122 32200 net.cpp:156] Memory required for data: 10611200
I0714 07:54:59.979146 32200 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0714 07:54:59.979166 32200 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0714 07:54:59.979183 32200 layer_factory.hpp:77] Creating layer relu1_p
I0714 07:54:59.979207 32200 net.cpp:91] Creating Layer relu1_p
I0714 07:54:59.979223 32200 net.cpp:425] relu1_p <- ip1_p
I0714 07:54:59.979240 32200 net.cpp:386] relu1_p -> ip1_p (in-place)
I0714 07:54:59.979262 32200 net.cpp:141] Setting up relu1_p
I0714 07:54:59.979280 32200 net.cpp:148] Top shape: 64 500 (32000)
I0714 07:54:59.979293 32200 net.cpp:156] Memory required for data: 10739200
I0714 07:54:59.979307 32200 layer_factory.hpp:77] Creating layer ip2_p
I0714 07:54:59.979332 32200 net.cpp:91] Creating Layer ip2_p
I0714 07:54:59.979348 32200 net.cpp:425] ip2_p <- ip1_p
I0714 07:54:59.979367 32200 net.cpp:399] ip2_p -> ip2_p
I0714 07:54:59.979437 32200 net.cpp:141] Setting up ip2_p
I0714 07:54:59.979456 32200 net.cpp:148] Top shape: 64 10 (640)
I0714 07:54:59.979470 32200 net.cpp:156] Memory required for data: 10741760
I0714 07:54:59.979486 32200 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0714 07:54:59.979501 32200 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0714 07:54:59.979636 32200 layer_factory.hpp:77] Creating layer feat_p
I0714 07:54:59.979662 32200 net.cpp:91] Creating Layer feat_p
I0714 07:54:59.979677 32200 net.cpp:425] feat_p <- ip2_p
I0714 07:54:59.979694 32200 net.cpp:399] feat_p -> feat_p
I0714 07:54:59.979722 32200 net.cpp:141] Setting up feat_p
I0714 07:54:59.979739 32200 net.cpp:148] Top shape: 64 3 (192)
I0714 07:54:59.979751 32200 net.cpp:156] Memory required for data: 10742528
I0714 07:54:59.979765 32200 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0714 07:54:59.979780 32200 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0714 07:54:59.979794 32200 layer_factory.hpp:77] Creating layer loss
I0714 07:54:59.979820 32200 net.cpp:91] Creating Layer loss
I0714 07:54:59.979835 32200 net.cpp:425] loss <- feat
I0714 07:54:59.979849 32200 net.cpp:425] loss <- feat_p
I0714 07:54:59.979864 32200 net.cpp:425] loss <- sim
I0714 07:54:59.979882 32200 net.cpp:399] loss -> loss
I0714 07:54:59.979908 32200 net.cpp:141] Setting up loss
I0714 07:54:59.979925 32200 net.cpp:148] Top shape: (1)
I0714 07:54:59.979938 32200 net.cpp:151]     with loss weight 1
I0714 07:54:59.979967 32200 net.cpp:156] Memory required for data: 10742532
I0714 07:54:59.979980 32200 net.cpp:217] loss needs backward computation.
I0714 07:54:59.979993 32200 net.cpp:217] feat_p needs backward computation.
I0714 07:54:59.980006 32200 net.cpp:217] ip2_p needs backward computation.
I0714 07:54:59.980020 32200 net.cpp:217] relu1_p needs backward computation.
I0714 07:54:59.980032 32200 net.cpp:217] ip1_p needs backward computation.
I0714 07:54:59.980046 32200 net.cpp:217] pool2_p needs backward computation.
I0714 07:54:59.980058 32200 net.cpp:217] conv2_p needs backward computation.
I0714 07:54:59.980072 32200 net.cpp:217] pool1_p needs backward computation.
I0714 07:54:59.980084 32200 net.cpp:217] conv1_p needs backward computation.
I0714 07:54:59.980098 32200 net.cpp:217] feat needs backward computation.
I0714 07:54:59.980118 32200 net.cpp:217] ip2 needs backward computation.
I0714 07:54:59.980139 32200 net.cpp:217] relu1 needs backward computation.
I0714 07:54:59.980152 32200 net.cpp:217] ip1 needs backward computation.
I0714 07:54:59.980165 32200 net.cpp:217] pool2 needs backward computation.
I0714 07:54:59.980178 32200 net.cpp:217] conv2 needs backward computation.
I0714 07:54:59.980191 32200 net.cpp:217] pool1 needs backward computation.
I0714 07:54:59.980206 32200 net.cpp:217] conv1 needs backward computation.
I0714 07:54:59.980218 32200 net.cpp:219] slice_pair does not need backward computation.
I0714 07:54:59.980232 32200 net.cpp:219] pair_data does not need backward computation.
I0714 07:54:59.980244 32200 net.cpp:261] This network produces output loss
I0714 07:54:59.980409 32200 net.cpp:274] Network initialization done.
I0714 07:54:59.981014 32200 solver.cpp:181] Creating test net (#0) specified by net file: examples/siamese/mnist_siamese_train_test_feat3.prototxt
I0714 07:54:59.981091 32200 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0714 07:54:59.981312 32200 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test_feat3"
state {
  phase: TEST
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb_0to2"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 3
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0714 07:54:59.982187 32200 layer_factory.hpp:77] Creating layer pair_data
I0714 07:54:59.982341 32200 net.cpp:91] Creating Layer pair_data
I0714 07:54:59.982373 32200 net.cpp:399] pair_data -> pair_data
I0714 07:54:59.982403 32200 net.cpp:399] pair_data -> sim
I0714 07:54:59.998318 32206 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb_0to2
I0714 07:55:00.004621 32200 data_layer.cpp:41] output data size: 100,2,28,28
I0714 07:55:00.011631 32200 net.cpp:141] Setting up pair_data
I0714 07:55:00.011728 32200 net.cpp:148] Top shape: 100 2 28 28 (156800)
I0714 07:55:00.011739 32200 net.cpp:148] Top shape: 100 (100)
I0714 07:55:00.011744 32200 net.cpp:156] Memory required for data: 627600
I0714 07:55:00.011752 32200 layer_factory.hpp:77] Creating layer slice_pair
I0714 07:55:00.011775 32200 net.cpp:91] Creating Layer slice_pair
I0714 07:55:00.011781 32200 net.cpp:425] slice_pair <- pair_data
I0714 07:55:00.011791 32200 net.cpp:399] slice_pair -> data
I0714 07:55:00.011876 32200 net.cpp:399] slice_pair -> data_p
I0714 07:55:00.011901 32200 net.cpp:141] Setting up slice_pair
I0714 07:55:00.011909 32200 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0714 07:55:00.011916 32200 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0714 07:55:00.011921 32200 net.cpp:156] Memory required for data: 1254800
I0714 07:55:00.011927 32200 layer_factory.hpp:77] Creating layer conv1
I0714 07:55:00.011946 32200 net.cpp:91] Creating Layer conv1
I0714 07:55:00.011951 32200 net.cpp:425] conv1 <- data
I0714 07:55:00.011960 32200 net.cpp:399] conv1 -> conv1
I0714 07:55:00.012004 32200 net.cpp:141] Setting up conv1
I0714 07:55:00.012013 32200 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0714 07:55:00.012017 32200 net.cpp:156] Memory required for data: 5862800
I0714 07:55:00.012030 32200 layer_factory.hpp:77] Creating layer pool1
I0714 07:55:00.012039 32200 net.cpp:91] Creating Layer pool1
I0714 07:55:00.012044 32200 net.cpp:425] pool1 <- conv1
I0714 07:55:00.012051 32200 net.cpp:399] pool1 -> pool1
I0714 07:55:00.012064 32200 net.cpp:141] Setting up pool1
I0714 07:55:00.012071 32200 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0714 07:55:00.012076 32200 net.cpp:156] Memory required for data: 7014800
I0714 07:55:00.012081 32200 layer_factory.hpp:77] Creating layer conv2
I0714 07:55:00.012092 32200 net.cpp:91] Creating Layer conv2
I0714 07:55:00.012143 32200 net.cpp:425] conv2 <- pool1
I0714 07:55:00.012152 32200 net.cpp:399] conv2 -> conv2
I0714 07:55:00.012473 32200 net.cpp:141] Setting up conv2
I0714 07:55:00.012483 32200 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0714 07:55:00.012487 32200 net.cpp:156] Memory required for data: 8294800
I0714 07:55:00.012496 32200 layer_factory.hpp:77] Creating layer pool2
I0714 07:55:00.012503 32200 net.cpp:91] Creating Layer pool2
I0714 07:55:00.012508 32200 net.cpp:425] pool2 <- conv2
I0714 07:55:00.012514 32200 net.cpp:399] pool2 -> pool2
I0714 07:55:00.012523 32200 net.cpp:141] Setting up pool2
I0714 07:55:00.012531 32200 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0714 07:55:00.012534 32200 net.cpp:156] Memory required for data: 8614800
I0714 07:55:00.012538 32200 layer_factory.hpp:77] Creating layer ip1
I0714 07:55:00.012549 32200 net.cpp:91] Creating Layer ip1
I0714 07:55:00.012553 32200 net.cpp:425] ip1 <- pool2
I0714 07:55:00.012560 32200 net.cpp:399] ip1 -> ip1
I0714 07:55:00.022696 32200 net.cpp:141] Setting up ip1
I0714 07:55:00.030233 32200 net.cpp:148] Top shape: 100 500 (50000)
I0714 07:55:00.030303 32200 net.cpp:156] Memory required for data: 8814800
I0714 07:55:00.030387 32200 layer_factory.hpp:77] Creating layer relu1
I0714 07:55:00.030437 32200 net.cpp:91] Creating Layer relu1
I0714 07:55:00.030503 32200 net.cpp:425] relu1 <- ip1
I0714 07:55:00.030539 32200 net.cpp:386] relu1 -> ip1 (in-place)
I0714 07:55:00.030601 32200 net.cpp:141] Setting up relu1
I0714 07:55:00.030635 32200 net.cpp:148] Top shape: 100 500 (50000)
I0714 07:55:00.030715 32200 net.cpp:156] Memory required for data: 9014800
I0714 07:55:00.030772 32200 layer_factory.hpp:77] Creating layer ip2
I0714 07:55:00.030814 32200 net.cpp:91] Creating Layer ip2
I0714 07:55:00.030866 32200 net.cpp:425] ip2 <- ip1
I0714 07:55:00.030902 32200 net.cpp:399] ip2 -> ip2
I0714 07:55:00.031023 32200 net.cpp:141] Setting up ip2
I0714 07:55:00.031082 32200 net.cpp:148] Top shape: 100 10 (1000)
I0714 07:55:00.031111 32200 net.cpp:156] Memory required for data: 9018800
I0714 07:55:00.031164 32200 layer_factory.hpp:77] Creating layer feat
I0714 07:55:00.031199 32200 net.cpp:91] Creating Layer feat
I0714 07:55:00.031225 32200 net.cpp:425] feat <- ip2
I0714 07:55:00.031255 32200 net.cpp:399] feat -> feat
I0714 07:55:00.031299 32200 net.cpp:141] Setting up feat
I0714 07:55:00.031330 32200 net.cpp:148] Top shape: 100 3 (300)
I0714 07:55:00.031354 32200 net.cpp:156] Memory required for data: 9020000
I0714 07:55:00.031384 32200 layer_factory.hpp:77] Creating layer conv1_p
I0714 07:55:00.031420 32200 net.cpp:91] Creating Layer conv1_p
I0714 07:55:00.031446 32200 net.cpp:425] conv1_p <- data_p
I0714 07:55:00.031474 32200 net.cpp:399] conv1_p -> conv1_p
I0714 07:55:00.031538 32200 net.cpp:141] Setting up conv1_p
I0714 07:55:00.031569 32200 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0714 07:55:00.031591 32200 net.cpp:156] Memory required for data: 13628000
I0714 07:55:00.031615 32200 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0714 07:55:00.031642 32200 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0714 07:55:00.031666 32200 layer_factory.hpp:77] Creating layer pool1_p
I0714 07:55:00.031694 32200 net.cpp:91] Creating Layer pool1_p
I0714 07:55:00.031718 32200 net.cpp:425] pool1_p <- conv1_p
I0714 07:55:00.031747 32200 net.cpp:399] pool1_p -> pool1_p
I0714 07:55:00.031782 32200 net.cpp:141] Setting up pool1_p
I0714 07:55:00.031810 32200 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0714 07:55:00.031833 32200 net.cpp:156] Memory required for data: 14780000
I0714 07:55:00.031857 32200 layer_factory.hpp:77] Creating layer conv2_p
I0714 07:55:00.031893 32200 net.cpp:91] Creating Layer conv2_p
I0714 07:55:00.031919 32200 net.cpp:425] conv2_p <- pool1_p
I0714 07:55:00.031949 32200 net.cpp:399] conv2_p -> conv2_p
I0714 07:55:00.032353 32200 net.cpp:141] Setting up conv2_p
I0714 07:55:00.032394 32200 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0714 07:55:00.032400 32200 net.cpp:156] Memory required for data: 16060000
I0714 07:55:00.032430 32200 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0714 07:55:00.032438 32200 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0714 07:55:00.032444 32200 layer_factory.hpp:77] Creating layer pool2_p
I0714 07:55:00.032456 32200 net.cpp:91] Creating Layer pool2_p
I0714 07:55:00.032461 32200 net.cpp:425] pool2_p <- conv2_p
I0714 07:55:00.032470 32200 net.cpp:399] pool2_p -> pool2_p
I0714 07:55:00.032485 32200 net.cpp:141] Setting up pool2_p
I0714 07:55:00.032491 32200 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0714 07:55:00.032496 32200 net.cpp:156] Memory required for data: 16380000
I0714 07:55:00.032500 32200 layer_factory.hpp:77] Creating layer ip1_p
I0714 07:55:00.032510 32200 net.cpp:91] Creating Layer ip1_p
I0714 07:55:00.032516 32200 net.cpp:425] ip1_p <- pool2_p
I0714 07:55:00.032524 32200 net.cpp:399] ip1_p -> ip1_p
I0714 07:55:00.038075 32200 net.cpp:141] Setting up ip1_p
I0714 07:55:00.038197 32200 net.cpp:148] Top shape: 100 500 (50000)
I0714 07:55:00.038223 32200 net.cpp:156] Memory required for data: 16580000
I0714 07:55:00.038271 32200 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0714 07:55:00.038318 32200 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0714 07:55:00.038353 32200 layer_factory.hpp:77] Creating layer relu1_p
I0714 07:55:00.038380 32200 net.cpp:91] Creating Layer relu1_p
I0714 07:55:00.038403 32200 net.cpp:425] relu1_p <- ip1_p
I0714 07:55:00.038456 32200 net.cpp:386] relu1_p -> ip1_p (in-place)
I0714 07:55:00.038483 32200 net.cpp:141] Setting up relu1_p
I0714 07:55:00.038497 32200 net.cpp:148] Top shape: 100 500 (50000)
I0714 07:55:00.038503 32200 net.cpp:156] Memory required for data: 16780000
I0714 07:55:00.038509 32200 layer_factory.hpp:77] Creating layer ip2_p
I0714 07:55:00.038527 32200 net.cpp:91] Creating Layer ip2_p
I0714 07:55:00.038533 32200 net.cpp:425] ip2_p <- ip1_p
I0714 07:55:00.038545 32200 net.cpp:399] ip2_p -> ip2_p
I0714 07:55:00.038707 32200 net.cpp:141] Setting up ip2_p
I0714 07:55:00.038839 32200 net.cpp:148] Top shape: 100 10 (1000)
I0714 07:55:00.038872 32200 net.cpp:156] Memory required for data: 16784000
I0714 07:55:00.038899 32200 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0714 07:55:00.038918 32200 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0714 07:55:00.038929 32200 layer_factory.hpp:77] Creating layer feat_p
I0714 07:55:00.038962 32200 net.cpp:91] Creating Layer feat_p
I0714 07:55:00.038975 32200 net.cpp:425] feat_p <- ip2_p
I0714 07:55:00.038990 32200 net.cpp:399] feat_p -> feat_p
I0714 07:55:00.039026 32200 net.cpp:141] Setting up feat_p
I0714 07:55:00.039034 32200 net.cpp:148] Top shape: 100 3 (300)
I0714 07:55:00.039039 32200 net.cpp:156] Memory required for data: 16785200
I0714 07:55:00.039103 32200 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0714 07:55:00.039168 32200 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0714 07:55:00.039177 32200 layer_factory.hpp:77] Creating layer loss
I0714 07:55:00.039206 32200 net.cpp:91] Creating Layer loss
I0714 07:55:00.039213 32200 net.cpp:425] loss <- feat
I0714 07:55:00.039222 32200 net.cpp:425] loss <- feat_p
I0714 07:55:00.039230 32200 net.cpp:425] loss <- sim
I0714 07:55:00.039237 32200 net.cpp:399] loss -> loss
I0714 07:55:00.048122 32200 net.cpp:141] Setting up loss
I0714 07:55:00.048172 32200 net.cpp:148] Top shape: (1)
I0714 07:55:00.048177 32200 net.cpp:151]     with loss weight 1
I0714 07:55:00.048189 32200 net.cpp:156] Memory required for data: 16785204
I0714 07:55:00.048195 32200 net.cpp:217] loss needs backward computation.
I0714 07:55:00.048203 32200 net.cpp:217] feat_p needs backward computation.
I0714 07:55:00.048207 32200 net.cpp:217] ip2_p needs backward computation.
I0714 07:55:00.048212 32200 net.cpp:217] relu1_p needs backward computation.
I0714 07:55:00.048214 32200 net.cpp:217] ip1_p needs backward computation.
I0714 07:55:00.048244 32200 net.cpp:217] pool2_p needs backward computation.
I0714 07:55:00.048249 32200 net.cpp:217] conv2_p needs backward computation.
I0714 07:55:00.048252 32200 net.cpp:217] pool1_p needs backward computation.
I0714 07:55:00.048255 32200 net.cpp:217] conv1_p needs backward computation.
I0714 07:55:00.048259 32200 net.cpp:217] feat needs backward computation.
I0714 07:55:00.048262 32200 net.cpp:217] ip2 needs backward computation.
I0714 07:55:00.048266 32200 net.cpp:217] relu1 needs backward computation.
I0714 07:55:00.048269 32200 net.cpp:217] ip1 needs backward computation.
I0714 07:55:00.048272 32200 net.cpp:217] pool2 needs backward computation.
I0714 07:55:00.048275 32200 net.cpp:217] conv2 needs backward computation.
I0714 07:55:00.048279 32200 net.cpp:217] pool1 needs backward computation.
I0714 07:55:00.048281 32200 net.cpp:217] conv1 needs backward computation.
I0714 07:55:00.048285 32200 net.cpp:219] slice_pair does not need backward computation.
I0714 07:55:00.048290 32200 net.cpp:219] pair_data does not need backward computation.
I0714 07:55:00.048291 32200 net.cpp:261] This network produces output loss
I0714 07:55:00.048317 32200 net.cpp:274] Network initialization done.
I0714 07:55:00.048434 32200 solver.cpp:60] Solver scaffolding done.
I0714 07:55:00.048460 32200 caffe.cpp:219] Starting Optimization
I0714 07:55:00.048465 32200 solver.cpp:279] Solving mnist_siamese_train_test_feat3
I0714 07:55:00.048467 32200 solver.cpp:280] Learning Rate Policy: inv
I0714 07:55:00.049077 32200 solver.cpp:337] Iteration 0, Testing net (#0)
I0714 07:55:10.306923 32200 solver.cpp:404]     Test net output #0: loss = 0.0969918 (* 1 = 0.0969918 loss)
I0714 07:55:10.508803 32200 solver.cpp:228] Iteration 0, loss = 0.0989483
I0714 07:55:10.508941 32200 solver.cpp:244]     Train net output #0: loss = 0.0989483 (* 1 = 0.0989483 loss)
I0714 07:55:10.508977 32200 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0714 07:55:26.306154 32200 solver.cpp:228] Iteration 100, loss = 0.00901075
I0714 07:55:26.306246 32200 solver.cpp:244]     Train net output #0: loss = 0.00901076 (* 1 = 0.00901076 loss)
I0714 07:55:26.306258 32200 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0714 07:55:42.289938 32200 solver.cpp:228] Iteration 200, loss = 0.00881466
I0714 07:55:42.290166 32200 solver.cpp:244]     Train net output #0: loss = 0.00881466 (* 1 = 0.00881466 loss)
I0714 07:55:42.290285 32200 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0714 07:55:58.337982 32200 solver.cpp:228] Iteration 300, loss = 0.00643098
I0714 07:55:58.338100 32200 solver.cpp:244]     Train net output #0: loss = 0.00643099 (* 1 = 0.00643099 loss)
I0714 07:55:58.338127 32200 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0714 07:56:14.380517 32200 solver.cpp:228] Iteration 400, loss = 0.00890271
I0714 07:56:14.380705 32200 solver.cpp:244]     Train net output #0: loss = 0.00890272 (* 1 = 0.00890272 loss)
I0714 07:56:14.380718 32200 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0714 07:56:30.244280 32200 solver.cpp:337] Iteration 500, Testing net (#0)
I0714 07:56:40.348080 32200 solver.cpp:404]     Test net output #0: loss = 0.00766528 (* 1 = 0.00766528 loss)
I0714 07:56:40.533543 32200 solver.cpp:228] Iteration 500, loss = 0.0074722
I0714 07:56:40.533732 32200 solver.cpp:244]     Train net output #0: loss = 0.00747221 (* 1 = 0.00747221 loss)
I0714 07:56:40.533781 32200 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0714 07:56:56.389782 32200 solver.cpp:228] Iteration 600, loss = 0.00488822
I0714 07:56:56.389957 32200 solver.cpp:244]     Train net output #0: loss = 0.00488822 (* 1 = 0.00488822 loss)
I0714 07:56:56.389971 32200 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0714 07:57:12.374137 32200 solver.cpp:228] Iteration 700, loss = 0.00461413
I0714 07:57:12.374199 32200 solver.cpp:244]     Train net output #0: loss = 0.00461414 (* 1 = 0.00461414 loss)
I0714 07:57:12.374209 32200 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0714 07:57:28.253988 32200 solver.cpp:228] Iteration 800, loss = 0.00522063
I0714 07:57:28.254423 32200 solver.cpp:244]     Train net output #0: loss = 0.00522064 (* 1 = 0.00522064 loss)
I0714 07:57:28.254492 32200 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0714 07:57:44.344779 32200 solver.cpp:228] Iteration 900, loss = 0.00517143
I0714 07:57:44.344892 32200 solver.cpp:244]     Train net output #0: loss = 0.00517145 (* 1 = 0.00517145 loss)
I0714 07:57:44.344918 32200 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0714 07:58:00.216795 32200 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_1000.caffemodel
I0714 07:58:00.227660 32200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_1000.solverstate
I0714 07:58:00.231153 32200 solver.cpp:337] Iteration 1000, Testing net (#0)
I0714 07:58:10.302417 32200 solver.cpp:404]     Test net output #0: loss = 0.00599274 (* 1 = 0.00599274 loss)
I0714 07:58:10.486008 32200 solver.cpp:228] Iteration 1000, loss = 0.00336147
I0714 07:58:10.486069 32200 solver.cpp:244]     Train net output #0: loss = 0.00336148 (* 1 = 0.00336148 loss)
I0714 07:58:10.486079 32200 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0714 07:58:26.226807 32200 solver.cpp:228] Iteration 1100, loss = 0.00176292
I0714 07:58:26.227002 32200 solver.cpp:244]     Train net output #0: loss = 0.00176293 (* 1 = 0.00176293 loss)
I0714 07:58:26.227072 32200 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0714 07:58:42.306248 32200 solver.cpp:228] Iteration 1200, loss = 0.00891285
I0714 07:58:42.306427 32200 solver.cpp:244]     Train net output #0: loss = 0.00891286 (* 1 = 0.00891286 loss)
I0714 07:58:42.306440 32200 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0714 07:58:58.432407 32200 solver.cpp:228] Iteration 1300, loss = 0.00396985
I0714 07:58:58.432472 32200 solver.cpp:244]     Train net output #0: loss = 0.00396986 (* 1 = 0.00396986 loss)
I0714 07:58:58.432485 32200 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0714 07:59:14.548745 32200 solver.cpp:228] Iteration 1400, loss = 0.00614147
I0714 07:59:14.548925 32200 solver.cpp:244]     Train net output #0: loss = 0.00614148 (* 1 = 0.00614148 loss)
I0714 07:59:14.548940 32200 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0714 07:59:30.374728 32200 solver.cpp:337] Iteration 1500, Testing net (#0)
I0714 07:59:40.365386 32200 solver.cpp:404]     Test net output #0: loss = 0.00517443 (* 1 = 0.00517443 loss)
I0714 07:59:40.553397 32200 solver.cpp:228] Iteration 1500, loss = 0.00860705
I0714 07:59:40.553453 32200 solver.cpp:244]     Train net output #0: loss = 0.00860706 (* 1 = 0.00860706 loss)
I0714 07:59:40.553465 32200 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0714 07:59:56.252336 32200 solver.cpp:228] Iteration 1600, loss = 0.00370167
I0714 07:59:56.252542 32200 solver.cpp:244]     Train net output #0: loss = 0.00370168 (* 1 = 0.00370168 loss)
I0714 07:59:56.252585 32200 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0714 08:00:15.646689 32200 solver.cpp:228] Iteration 1700, loss = 0.00579229
I0714 08:00:15.646816 32200 solver.cpp:244]     Train net output #0: loss = 0.0057923 (* 1 = 0.0057923 loss)
I0714 08:00:15.646852 32200 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0714 08:00:31.478288 32200 solver.cpp:228] Iteration 1800, loss = 0.00570005
I0714 08:00:31.478479 32200 solver.cpp:244]     Train net output #0: loss = 0.00570006 (* 1 = 0.00570006 loss)
I0714 08:00:31.478495 32200 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0714 08:00:47.336468 32200 solver.cpp:228] Iteration 1900, loss = 0.00398762
I0714 08:00:47.336606 32200 solver.cpp:244]     Train net output #0: loss = 0.00398763 (* 1 = 0.00398763 loss)
I0714 08:00:47.336668 32200 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0714 08:01:03.039350 32200 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_2000.caffemodel
I0714 08:01:03.049909 32200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_2000.solverstate
I0714 08:01:03.053056 32200 solver.cpp:337] Iteration 2000, Testing net (#0)
I0714 08:01:12.832366 32200 solver.cpp:404]     Test net output #0: loss = 0.00459519 (* 1 = 0.00459519 loss)
I0714 08:01:13.033675 32200 solver.cpp:228] Iteration 2000, loss = 0.00301442
I0714 08:01:13.033735 32200 solver.cpp:244]     Train net output #0: loss = 0.00301443 (* 1 = 0.00301443 loss)
I0714 08:01:13.033747 32200 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0714 08:01:28.886283 32200 solver.cpp:228] Iteration 2100, loss = 0.00244418
I0714 08:01:28.886492 32200 solver.cpp:244]     Train net output #0: loss = 0.00244419 (* 1 = 0.00244419 loss)
I0714 08:01:28.886534 32200 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0714 08:01:44.720211 32200 solver.cpp:228] Iteration 2200, loss = 0.00264233
I0714 08:01:44.720410 32200 solver.cpp:244]     Train net output #0: loss = 0.00264234 (* 1 = 0.00264234 loss)
I0714 08:01:44.720427 32200 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0714 08:02:00.540628 32200 solver.cpp:228] Iteration 2300, loss = 0.00526712
I0714 08:02:00.540758 32200 solver.cpp:244]     Train net output #0: loss = 0.00526713 (* 1 = 0.00526713 loss)
I0714 08:02:00.540794 32200 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0714 08:02:16.276721 32200 solver.cpp:228] Iteration 2400, loss = 0.00206256
I0714 08:02:16.276955 32200 solver.cpp:244]     Train net output #0: loss = 0.00206257 (* 1 = 0.00206257 loss)
I0714 08:02:16.277001 32200 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0714 08:02:31.892577 32200 solver.cpp:337] Iteration 2500, Testing net (#0)
I0714 08:02:41.623843 32200 solver.cpp:404]     Test net output #0: loss = 0.00444971 (* 1 = 0.00444971 loss)
I0714 08:02:41.815343 32200 solver.cpp:228] Iteration 2500, loss = 0.00249759
I0714 08:02:41.815414 32200 solver.cpp:244]     Train net output #0: loss = 0.0024976 (* 1 = 0.0024976 loss)
I0714 08:02:41.815431 32200 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0714 08:02:57.478391 32200 solver.cpp:228] Iteration 2600, loss = 0.00362862
I0714 08:02:57.478487 32200 solver.cpp:244]     Train net output #0: loss = 0.00362863 (* 1 = 0.00362863 loss)
I0714 08:02:57.478500 32200 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0714 08:03:13.218653 32200 solver.cpp:228] Iteration 2700, loss = 0.00195477
I0714 08:03:13.218938 32200 solver.cpp:244]     Train net output #0: loss = 0.00195478 (* 1 = 0.00195478 loss)
I0714 08:03:13.219058 32200 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0714 08:03:28.954453 32200 solver.cpp:228] Iteration 2800, loss = 0.00122311
I0714 08:03:28.954798 32200 solver.cpp:244]     Train net output #0: loss = 0.00122312 (* 1 = 0.00122312 loss)
I0714 08:03:28.954880 32200 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0714 08:03:44.777325 32200 solver.cpp:228] Iteration 2900, loss = 0.00588221
I0714 08:03:44.777541 32200 solver.cpp:244]     Train net output #0: loss = 0.00588222 (* 1 = 0.00588222 loss)
I0714 08:03:44.777585 32200 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0714 08:04:00.502418 32200 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_3000.caffemodel
I0714 08:04:00.512790 32200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_3000.solverstate
I0714 08:04:00.515599 32200 solver.cpp:337] Iteration 3000, Testing net (#0)
I0714 08:04:10.346454 32200 solver.cpp:404]     Test net output #0: loss = 0.00428875 (* 1 = 0.00428875 loss)
I0714 08:04:10.551440 32200 solver.cpp:228] Iteration 3000, loss = 0.00328278
I0714 08:04:10.551497 32200 solver.cpp:244]     Train net output #0: loss = 0.00328279 (* 1 = 0.00328279 loss)
I0714 08:04:10.551509 32200 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0714 08:04:26.301232 32200 solver.cpp:228] Iteration 3100, loss = 0.00178428
I0714 08:04:26.301460 32200 solver.cpp:244]     Train net output #0: loss = 0.00178429 (* 1 = 0.00178429 loss)
I0714 08:04:26.301550 32200 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0714 08:04:42.071599 32200 solver.cpp:228] Iteration 3200, loss = 0.0046841
I0714 08:04:42.072129 32200 solver.cpp:244]     Train net output #0: loss = 0.00468411 (* 1 = 0.00468411 loss)
I0714 08:04:42.072242 32200 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0714 08:04:57.813930 32200 solver.cpp:228] Iteration 3300, loss = 0.00508784
I0714 08:04:57.814134 32200 solver.cpp:244]     Train net output #0: loss = 0.00508785 (* 1 = 0.00508785 loss)
I0714 08:04:57.814178 32200 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0714 08:05:13.715700 32200 solver.cpp:228] Iteration 3400, loss = 0.00211772
I0714 08:05:13.715853 32200 solver.cpp:244]     Train net output #0: loss = 0.00211773 (* 1 = 0.00211773 loss)
I0714 08:05:13.715889 32200 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0714 08:05:29.238777 32200 solver.cpp:337] Iteration 3500, Testing net (#0)
I0714 08:05:38.989663 32200 solver.cpp:404]     Test net output #0: loss = 0.00416922 (* 1 = 0.00416922 loss)
I0714 08:05:39.186050 32200 solver.cpp:228] Iteration 3500, loss = 0.0018531
I0714 08:05:39.186111 32200 solver.cpp:244]     Train net output #0: loss = 0.00185311 (* 1 = 0.00185311 loss)
I0714 08:05:39.186123 32200 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0714 08:05:54.878917 32200 solver.cpp:228] Iteration 3600, loss = 0.00269181
I0714 08:05:54.879158 32200 solver.cpp:244]     Train net output #0: loss = 0.00269182 (* 1 = 0.00269182 loss)
I0714 08:05:54.879240 32200 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0714 08:06:10.615708 32200 solver.cpp:228] Iteration 3700, loss = 0.00176117
I0714 08:06:10.615772 32200 solver.cpp:244]     Train net output #0: loss = 0.00176118 (* 1 = 0.00176118 loss)
I0714 08:06:10.615783 32200 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0714 08:06:26.266418 32200 solver.cpp:228] Iteration 3800, loss = 0.00166823
I0714 08:06:26.266508 32200 solver.cpp:244]     Train net output #0: loss = 0.00166823 (* 1 = 0.00166823 loss)
I0714 08:06:26.266521 32200 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0714 08:06:41.902609 32200 solver.cpp:228] Iteration 3900, loss = 0.0023913
I0714 08:06:41.902667 32200 solver.cpp:244]     Train net output #0: loss = 0.00239131 (* 1 = 0.00239131 loss)
I0714 08:06:41.902678 32200 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0714 08:06:57.339131 32200 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_4000.caffemodel
I0714 08:06:57.349220 32200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_4000.solverstate
I0714 08:06:57.352035 32200 solver.cpp:337] Iteration 4000, Testing net (#0)
I0714 08:07:07.188127 32200 solver.cpp:404]     Test net output #0: loss = 0.00376952 (* 1 = 0.00376952 loss)
I0714 08:07:07.378525 32200 solver.cpp:228] Iteration 4000, loss = 0.00199515
I0714 08:07:07.378597 32200 solver.cpp:244]     Train net output #0: loss = 0.00199516 (* 1 = 0.00199516 loss)
I0714 08:07:07.378612 32200 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0714 08:07:23.007025 32200 solver.cpp:228] Iteration 4100, loss = 0.00226005
I0714 08:07:23.007091 32200 solver.cpp:244]     Train net output #0: loss = 0.00226006 (* 1 = 0.00226006 loss)
I0714 08:07:23.007103 32200 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0714 08:07:38.720290 32200 solver.cpp:228] Iteration 4200, loss = 0.00219137
I0714 08:07:38.720644 32200 solver.cpp:244]     Train net output #0: loss = 0.00219138 (* 1 = 0.00219138 loss)
I0714 08:07:38.720728 32200 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0714 08:07:54.344507 32200 solver.cpp:228] Iteration 4300, loss = 0.00139752
I0714 08:07:54.344575 32200 solver.cpp:244]     Train net output #0: loss = 0.00139753 (* 1 = 0.00139753 loss)
I0714 08:07:54.344588 32200 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0714 08:08:10.066826 32200 solver.cpp:228] Iteration 4400, loss = 0.00374106
I0714 08:08:10.067479 32200 solver.cpp:244]     Train net output #0: loss = 0.00374107 (* 1 = 0.00374107 loss)
I0714 08:08:10.067556 32200 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0714 08:08:25.609721 32200 solver.cpp:337] Iteration 4500, Testing net (#0)
I0714 08:08:35.348273 32200 solver.cpp:404]     Test net output #0: loss = 0.00372989 (* 1 = 0.00372989 loss)
I0714 08:08:35.539247 32200 solver.cpp:228] Iteration 4500, loss = 0.00217408
I0714 08:08:35.539311 32200 solver.cpp:244]     Train net output #0: loss = 0.00217409 (* 1 = 0.00217409 loss)
I0714 08:08:35.539324 32200 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0714 08:08:51.180941 32200 solver.cpp:228] Iteration 4600, loss = 0.000970954
I0714 08:08:51.181383 32200 solver.cpp:244]     Train net output #0: loss = 0.000970964 (* 1 = 0.000970964 loss)
I0714 08:08:51.181476 32200 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0714 08:09:06.910403 32200 solver.cpp:228] Iteration 4700, loss = 0.00148578
I0714 08:09:06.910464 32200 solver.cpp:244]     Train net output #0: loss = 0.00148579 (* 1 = 0.00148579 loss)
I0714 08:09:06.910477 32200 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0714 08:09:23.480835 32200 solver.cpp:228] Iteration 4800, loss = 0.00104766
I0714 08:09:23.480927 32200 solver.cpp:244]     Train net output #0: loss = 0.00104767 (* 1 = 0.00104767 loss)
I0714 08:09:23.480940 32200 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0714 08:09:39.197898 32200 solver.cpp:228] Iteration 4900, loss = 0.00244116
I0714 08:09:39.197959 32200 solver.cpp:244]     Train net output #0: loss = 0.00244118 (* 1 = 0.00244118 loss)
I0714 08:09:39.197971 32200 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0714 08:09:54.710840 32200 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_5000.caffemodel
I0714 08:09:54.727188 32200 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to2_feat3_iter_5000.solverstate
I0714 08:09:54.812911 32200 solver.cpp:317] Iteration 5000, loss = 0.001859
I0714 08:09:54.813249 32200 solver.cpp:337] Iteration 5000, Testing net (#0)
I0714 08:10:04.950062 32200 solver.cpp:404]     Test net output #0: loss = 0.00356825 (* 1 = 0.00356825 loss)
I0714 08:10:04.950109 32200 solver.cpp:322] Optimization Done.
I0714 08:10:04.950114 32200 caffe.cpp:222] Optimization Done.
