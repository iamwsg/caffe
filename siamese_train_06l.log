I0712 21:17:25.158618 14570 caffe.cpp:178] Use CPU.
I0712 21:17:25.158908 14570 solver.cpp:48] Initializing solver from parameters: 
test_iter: 100
test_interval: 500
base_lr: 0.01
display: 100
max_iter: 5000
lr_policy: "inv"
gamma: 0.0001
power: 0.75
momentum: 0.9
weight_decay: 0
snapshot: 5000
snapshot_prefix: "examples/siamese/My_mnist_siamese_0to6l"
solver_mode: CPU
net: "examples/siamese/mnist_siamese_train_test.prototxt"
I0712 21:17:25.159049 14570 solver.cpp:91] Creating training net from net file: examples/siamese/mnist_siamese_train_test.prototxt
I0712 21:17:25.159582 14570 net.cpp:313] The NetState phase (0) differed from the phase (1) specified by a rule in layer pair_data
I0712 21:17:25.159720 14570 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TRAIN
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TRAIN
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_train_leveldb_0to6_l"
    batch_size: 64
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0712 21:17:25.159826 14570 layer_factory.hpp:77] Creating layer pair_data
I0712 21:17:25.160320 14570 net.cpp:91] Creating Layer pair_data
I0712 21:17:25.160333 14570 net.cpp:399] pair_data -> pair_data
I0712 21:17:25.160356 14570 net.cpp:399] pair_data -> sim
I0712 21:17:25.169147 14574 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_train_leveldb_0to6_l
I0712 21:17:25.172732 14570 data_layer.cpp:41] output data size: 64,2,28,28
I0712 21:17:25.173310 14570 net.cpp:141] Setting up pair_data
I0712 21:17:25.173360 14570 net.cpp:148] Top shape: 64 2 28 28 (100352)
I0712 21:17:25.173382 14570 net.cpp:148] Top shape: 64 (64)
I0712 21:17:25.173398 14570 net.cpp:156] Memory required for data: 401664
I0712 21:17:25.173420 14570 layer_factory.hpp:77] Creating layer slice_pair
I0712 21:17:25.173449 14570 net.cpp:91] Creating Layer slice_pair
I0712 21:17:25.173467 14570 net.cpp:425] slice_pair <- pair_data
I0712 21:17:25.173491 14570 net.cpp:399] slice_pair -> data
I0712 21:17:25.173516 14570 net.cpp:399] slice_pair -> data_p
I0712 21:17:25.173542 14570 net.cpp:141] Setting up slice_pair
I0712 21:17:25.173560 14570 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0712 21:17:25.173574 14570 net.cpp:148] Top shape: 64 1 28 28 (50176)
I0712 21:17:25.173588 14570 net.cpp:156] Memory required for data: 803072
I0712 21:17:25.173600 14570 layer_factory.hpp:77] Creating layer conv1
I0712 21:17:25.173626 14570 net.cpp:91] Creating Layer conv1
I0712 21:17:25.173641 14570 net.cpp:425] conv1 <- data
I0712 21:17:25.173658 14570 net.cpp:399] conv1 -> conv1
I0712 21:17:25.173720 14570 net.cpp:141] Setting up conv1
I0712 21:17:25.173740 14570 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0712 21:17:25.173753 14570 net.cpp:156] Memory required for data: 3752192
I0712 21:17:25.173773 14570 layer_factory.hpp:77] Creating layer pool1
I0712 21:17:25.173790 14570 net.cpp:91] Creating Layer pool1
I0712 21:17:25.173804 14570 net.cpp:425] pool1 <- conv1
I0712 21:17:25.173820 14570 net.cpp:399] pool1 -> pool1
I0712 21:17:25.173847 14570 net.cpp:141] Setting up pool1
I0712 21:17:25.173864 14570 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0712 21:17:25.173877 14570 net.cpp:156] Memory required for data: 4489472
I0712 21:17:25.173889 14570 layer_factory.hpp:77] Creating layer conv2
I0712 21:17:25.173907 14570 net.cpp:91] Creating Layer conv2
I0712 21:17:25.173921 14570 net.cpp:425] conv2 <- pool1
I0712 21:17:25.173941 14570 net.cpp:399] conv2 -> conv2
I0712 21:17:25.174156 14570 net.cpp:141] Setting up conv2
I0712 21:17:25.174176 14570 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0712 21:17:25.174190 14570 net.cpp:156] Memory required for data: 5308672
I0712 21:17:25.174206 14570 layer_factory.hpp:77] Creating layer pool2
I0712 21:17:25.174228 14570 net.cpp:91] Creating Layer pool2
I0712 21:17:25.174255 14570 net.cpp:425] pool2 <- conv2
I0712 21:17:25.174273 14570 net.cpp:399] pool2 -> pool2
I0712 21:17:25.174294 14570 net.cpp:141] Setting up pool2
I0712 21:17:25.174310 14570 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0712 21:17:25.174322 14570 net.cpp:156] Memory required for data: 5513472
I0712 21:17:25.174335 14570 layer_factory.hpp:77] Creating layer ip1
I0712 21:17:25.174358 14570 net.cpp:91] Creating Layer ip1
I0712 21:17:25.174373 14570 net.cpp:425] ip1 <- pool2
I0712 21:17:25.174389 14570 net.cpp:399] ip1 -> ip1
I0712 21:17:25.177670 14570 net.cpp:141] Setting up ip1
I0712 21:17:25.177748 14570 net.cpp:148] Top shape: 64 500 (32000)
I0712 21:17:25.177772 14570 net.cpp:156] Memory required for data: 5641472
I0712 21:17:25.177798 14570 layer_factory.hpp:77] Creating layer relu1
I0712 21:17:25.177821 14570 net.cpp:91] Creating Layer relu1
I0712 21:17:25.177839 14570 net.cpp:425] relu1 <- ip1
I0712 21:17:25.177860 14570 net.cpp:386] relu1 -> ip1 (in-place)
I0712 21:17:25.177883 14570 net.cpp:141] Setting up relu1
I0712 21:17:25.177902 14570 net.cpp:148] Top shape: 64 500 (32000)
I0712 21:17:25.177917 14570 net.cpp:156] Memory required for data: 5769472
I0712 21:17:25.177932 14570 layer_factory.hpp:77] Creating layer ip2
I0712 21:17:25.177953 14570 net.cpp:91] Creating Layer ip2
I0712 21:17:25.177968 14570 net.cpp:425] ip2 <- ip1
I0712 21:17:25.177989 14570 net.cpp:399] ip2 -> ip2
I0712 21:17:25.178058 14570 net.cpp:141] Setting up ip2
I0712 21:17:25.178079 14570 net.cpp:148] Top shape: 64 10 (640)
I0712 21:17:25.178093 14570 net.cpp:156] Memory required for data: 5772032
I0712 21:17:25.178109 14570 layer_factory.hpp:77] Creating layer feat
I0712 21:17:25.178129 14570 net.cpp:91] Creating Layer feat
I0712 21:17:25.178143 14570 net.cpp:425] feat <- ip2
I0712 21:17:25.178160 14570 net.cpp:399] feat -> feat
I0712 21:17:25.178185 14570 net.cpp:141] Setting up feat
I0712 21:17:25.178202 14570 net.cpp:148] Top shape: 64 2 (128)
I0712 21:17:25.178215 14570 net.cpp:156] Memory required for data: 5772544
I0712 21:17:25.178236 14570 layer_factory.hpp:77] Creating layer conv1_p
I0712 21:17:25.178257 14570 net.cpp:91] Creating Layer conv1_p
I0712 21:17:25.178272 14570 net.cpp:425] conv1_p <- data_p
I0712 21:17:25.178288 14570 net.cpp:399] conv1_p -> conv1_p
I0712 21:17:25.178328 14570 net.cpp:141] Setting up conv1_p
I0712 21:17:25.178347 14570 net.cpp:148] Top shape: 64 20 24 24 (737280)
I0712 21:17:25.178361 14570 net.cpp:156] Memory required for data: 8721664
I0712 21:17:25.178375 14570 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0712 21:17:25.178390 14570 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0712 21:17:25.178405 14570 layer_factory.hpp:77] Creating layer pool1_p
I0712 21:17:25.178421 14570 net.cpp:91] Creating Layer pool1_p
I0712 21:17:25.178434 14570 net.cpp:425] pool1_p <- conv1_p
I0712 21:17:25.178453 14570 net.cpp:399] pool1_p -> pool1_p
I0712 21:17:25.178475 14570 net.cpp:141] Setting up pool1_p
I0712 21:17:25.178491 14570 net.cpp:148] Top shape: 64 20 12 12 (184320)
I0712 21:17:25.178504 14570 net.cpp:156] Memory required for data: 9458944
I0712 21:17:25.178517 14570 layer_factory.hpp:77] Creating layer conv2_p
I0712 21:17:25.178537 14570 net.cpp:91] Creating Layer conv2_p
I0712 21:17:25.178552 14570 net.cpp:425] conv2_p <- pool1_p
I0712 21:17:25.178571 14570 net.cpp:399] conv2_p -> conv2_p
I0712 21:17:25.178897 14570 net.cpp:141] Setting up conv2_p
I0712 21:17:25.178930 14570 net.cpp:148] Top shape: 64 50 8 8 (204800)
I0712 21:17:25.178944 14570 net.cpp:156] Memory required for data: 10278144
I0712 21:17:25.178959 14570 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0712 21:17:25.178974 14570 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0712 21:17:25.178988 14570 layer_factory.hpp:77] Creating layer pool2_p
I0712 21:17:25.179005 14570 net.cpp:91] Creating Layer pool2_p
I0712 21:17:25.179019 14570 net.cpp:425] pool2_p <- conv2_p
I0712 21:17:25.179044 14570 net.cpp:399] pool2_p -> pool2_p
I0712 21:17:25.179075 14570 net.cpp:141] Setting up pool2_p
I0712 21:17:25.179091 14570 net.cpp:148] Top shape: 64 50 4 4 (51200)
I0712 21:17:25.179105 14570 net.cpp:156] Memory required for data: 10482944
I0712 21:17:25.179117 14570 layer_factory.hpp:77] Creating layer ip1_p
I0712 21:17:25.179137 14570 net.cpp:91] Creating Layer ip1_p
I0712 21:17:25.179152 14570 net.cpp:425] ip1_p <- pool2_p
I0712 21:17:25.179170 14570 net.cpp:399] ip1_p -> ip1_p
I0712 21:17:25.182709 14570 net.cpp:141] Setting up ip1_p
I0712 21:17:25.185075 14570 net.cpp:148] Top shape: 64 500 (32000)
I0712 21:17:25.185202 14570 net.cpp:156] Memory required for data: 10610944
I0712 21:17:25.185228 14570 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0712 21:17:25.185247 14570 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0712 21:17:25.185262 14570 layer_factory.hpp:77] Creating layer relu1_p
I0712 21:17:25.185283 14570 net.cpp:91] Creating Layer relu1_p
I0712 21:17:25.185298 14570 net.cpp:425] relu1_p <- ip1_p
I0712 21:17:25.185315 14570 net.cpp:386] relu1_p -> ip1_p (in-place)
I0712 21:17:25.185338 14570 net.cpp:141] Setting up relu1_p
I0712 21:17:25.185354 14570 net.cpp:148] Top shape: 64 500 (32000)
I0712 21:17:25.185367 14570 net.cpp:156] Memory required for data: 10738944
I0712 21:17:25.185380 14570 layer_factory.hpp:77] Creating layer ip2_p
I0712 21:17:25.185405 14570 net.cpp:91] Creating Layer ip2_p
I0712 21:17:25.185421 14570 net.cpp:425] ip2_p <- ip1_p
I0712 21:17:25.185438 14570 net.cpp:399] ip2_p -> ip2_p
I0712 21:17:25.185518 14570 net.cpp:141] Setting up ip2_p
I0712 21:17:25.185537 14570 net.cpp:148] Top shape: 64 10 (640)
I0712 21:17:25.185550 14570 net.cpp:156] Memory required for data: 10741504
I0712 21:17:25.185567 14570 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0712 21:17:25.185583 14570 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0712 21:17:25.185597 14570 layer_factory.hpp:77] Creating layer feat_p
I0712 21:17:25.185616 14570 net.cpp:91] Creating Layer feat_p
I0712 21:17:25.185631 14570 net.cpp:425] feat_p <- ip2_p
I0712 21:17:25.185647 14570 net.cpp:399] feat_p -> feat_p
I0712 21:17:25.185670 14570 net.cpp:141] Setting up feat_p
I0712 21:17:25.185688 14570 net.cpp:148] Top shape: 64 2 (128)
I0712 21:17:25.185700 14570 net.cpp:156] Memory required for data: 10742016
I0712 21:17:25.185714 14570 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0712 21:17:25.185729 14570 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0712 21:17:25.185744 14570 layer_factory.hpp:77] Creating layer loss
I0712 21:17:25.185770 14570 net.cpp:91] Creating Layer loss
I0712 21:17:25.185784 14570 net.cpp:425] loss <- feat
I0712 21:17:25.185799 14570 net.cpp:425] loss <- feat_p
I0712 21:17:25.185814 14570 net.cpp:425] loss <- sim
I0712 21:17:25.185832 14570 net.cpp:399] loss -> loss
I0712 21:17:25.185859 14570 net.cpp:141] Setting up loss
I0712 21:17:25.185876 14570 net.cpp:148] Top shape: (1)
I0712 21:17:25.185889 14570 net.cpp:151]     with loss weight 1
I0712 21:17:25.185920 14570 net.cpp:156] Memory required for data: 10742020
I0712 21:17:25.185933 14570 net.cpp:217] loss needs backward computation.
I0712 21:17:25.185947 14570 net.cpp:217] feat_p needs backward computation.
I0712 21:17:25.185961 14570 net.cpp:217] ip2_p needs backward computation.
I0712 21:17:25.185974 14570 net.cpp:217] relu1_p needs backward computation.
I0712 21:17:25.185987 14570 net.cpp:217] ip1_p needs backward computation.
I0712 21:17:25.186000 14570 net.cpp:217] pool2_p needs backward computation.
I0712 21:17:25.186013 14570 net.cpp:217] conv2_p needs backward computation.
I0712 21:17:25.186028 14570 net.cpp:217] pool1_p needs backward computation.
I0712 21:17:25.186040 14570 net.cpp:217] conv1_p needs backward computation.
I0712 21:17:25.186054 14570 net.cpp:217] feat needs backward computation.
I0712 21:17:25.186076 14570 net.cpp:217] ip2 needs backward computation.
I0712 21:17:25.186100 14570 net.cpp:217] relu1 needs backward computation.
I0712 21:17:25.186115 14570 net.cpp:217] ip1 needs backward computation.
I0712 21:17:25.186127 14570 net.cpp:217] pool2 needs backward computation.
I0712 21:17:25.186141 14570 net.cpp:217] conv2 needs backward computation.
I0712 21:17:25.186154 14570 net.cpp:217] pool1 needs backward computation.
I0712 21:17:25.186168 14570 net.cpp:217] conv1 needs backward computation.
I0712 21:17:25.186182 14570 net.cpp:219] slice_pair does not need backward computation.
I0712 21:17:25.186197 14570 net.cpp:219] pair_data does not need backward computation.
I0712 21:17:25.186208 14570 net.cpp:261] This network produces output loss
I0712 21:17:25.186353 14570 net.cpp:274] Network initialization done.
I0712 21:17:25.186918 14570 solver.cpp:181] Creating test net (#0) specified by net file: examples/siamese/mnist_siamese_train_test.prototxt
I0712 21:17:25.186976 14570 net.cpp:313] The NetState phase (1) differed from the phase (0) specified by a rule in layer pair_data
I0712 21:17:25.187134 14570 net.cpp:49] Initializing net from parameters: 
name: "mnist_siamese_train_test"
state {
  phase: TEST
}
layer {
  name: "pair_data"
  type: "Data"
  top: "pair_data"
  top: "sim"
  include {
    phase: TEST
  }
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: "examples/siamese/mnist_siamese_test_leveldb_789"
    batch_size: 100
  }
}
layer {
  name: "slice_pair"
  type: "Slice"
  bottom: "pair_data"
  top: "data"
  top: "data_p"
  slice_param {
    slice_dim: 1
    slice_point: 1
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "data"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "pool1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1"
  type: "InnerProduct"
  bottom: "pool2"
  top: "ip1"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}
layer {
  name: "ip2"
  type: "InnerProduct"
  bottom: "ip1"
  top: "ip2"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat"
  type: "InnerProduct"
  bottom: "ip2"
  top: "feat"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "conv1_p"
  type: "Convolution"
  bottom: "data_p"
  top: "conv1_p"
  param {
    name: "conv1_w"
    lr_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool1_p"
  type: "Pooling"
  bottom: "conv1_p"
  top: "pool1_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "conv2_p"
  type: "Convolution"
  bottom: "pool1_p"
  top: "conv2_p"
  param {
    name: "conv2_w"
    lr_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
  }
  convolution_param {
    num_output: 50
    kernel_size: 5
    stride: 1
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "pool2_p"
  type: "Pooling"
  bottom: "conv2_p"
  top: "pool2_p"
  pooling_param {
    pool: MAX
    kernel_size: 2
    stride: 2
  }
}
layer {
  name: "ip1_p"
  type: "InnerProduct"
  bottom: "pool2_p"
  top: "ip1_p"
  param {
    name: "ip1_w"
    lr_mult: 1
  }
  param {
    name: "ip1_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "relu1_p"
  type: "ReLU"
  bottom: "ip1_p"
  top: "ip1_p"
}
layer {
  name: "ip2_p"
  type: "InnerProduct"
  bottom: "ip1_p"
  top: "ip2_p"
  param {
    name: "ip2_w"
    lr_mult: 1
  }
  param {
    name: "ip2_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "feat_p"
  type: "InnerProduct"
  bottom: "ip2_p"
  top: "feat_p"
  param {
    name: "feat_w"
    lr_mult: 1
  }
  param {
    name: "feat_b"
    lr_mult: 2
  }
  inner_product_param {
    num_output: 2
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "loss"
  type: "ContrastiveLoss"
  bottom: "feat"
  bottom: "feat_p"
  bottom: "sim"
  top: "loss"
  contrastive_loss_param {
    margin: 1
  }
}
I0712 21:17:25.187978 14570 layer_factory.hpp:77] Creating layer pair_data
I0712 21:17:25.188302 14570 net.cpp:91] Creating Layer pair_data
I0712 21:17:25.188339 14570 net.cpp:399] pair_data -> pair_data
I0712 21:17:25.188364 14570 net.cpp:399] pair_data -> sim
I0712 21:17:25.199409 14576 db_leveldb.cpp:18] Opened leveldb examples/siamese/mnist_siamese_test_leveldb_789
I0712 21:17:25.199827 14570 data_layer.cpp:41] output data size: 100,2,28,28
I0712 21:17:25.200918 14570 net.cpp:141] Setting up pair_data
I0712 21:17:25.200949 14570 net.cpp:148] Top shape: 100 2 28 28 (156800)
I0712 21:17:25.200958 14570 net.cpp:148] Top shape: 100 (100)
I0712 21:17:25.200963 14570 net.cpp:156] Memory required for data: 627600
I0712 21:17:25.200973 14570 layer_factory.hpp:77] Creating layer slice_pair
I0712 21:17:25.200990 14570 net.cpp:91] Creating Layer slice_pair
I0712 21:17:25.200996 14570 net.cpp:425] slice_pair <- pair_data
I0712 21:17:25.201006 14570 net.cpp:399] slice_pair -> data
I0712 21:17:25.201020 14570 net.cpp:399] slice_pair -> data_p
I0712 21:17:25.201035 14570 net.cpp:141] Setting up slice_pair
I0712 21:17:25.201041 14570 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0712 21:17:25.201048 14570 net.cpp:148] Top shape: 100 1 28 28 (78400)
I0712 21:17:25.201052 14570 net.cpp:156] Memory required for data: 1254800
I0712 21:17:25.201057 14570 layer_factory.hpp:77] Creating layer conv1
I0712 21:17:25.201073 14570 net.cpp:91] Creating Layer conv1
I0712 21:17:25.201078 14570 net.cpp:425] conv1 <- data
I0712 21:17:25.201086 14570 net.cpp:399] conv1 -> conv1
I0712 21:17:25.201128 14570 net.cpp:141] Setting up conv1
I0712 21:17:25.201135 14570 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0712 21:17:25.201139 14570 net.cpp:156] Memory required for data: 5862800
I0712 21:17:25.201150 14570 layer_factory.hpp:77] Creating layer pool1
I0712 21:17:25.201159 14570 net.cpp:91] Creating Layer pool1
I0712 21:17:25.201164 14570 net.cpp:425] pool1 <- conv1
I0712 21:17:25.201170 14570 net.cpp:399] pool1 -> pool1
I0712 21:17:25.201182 14570 net.cpp:141] Setting up pool1
I0712 21:17:25.201189 14570 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0712 21:17:25.201194 14570 net.cpp:156] Memory required for data: 7014800
I0712 21:17:25.201198 14570 layer_factory.hpp:77] Creating layer conv2
I0712 21:17:25.201210 14570 net.cpp:91] Creating Layer conv2
I0712 21:17:25.201215 14570 net.cpp:425] conv2 <- pool1
I0712 21:17:25.201282 14570 net.cpp:399] conv2 -> conv2
I0712 21:17:25.201609 14570 net.cpp:141] Setting up conv2
I0712 21:17:25.201637 14570 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0712 21:17:25.201642 14570 net.cpp:156] Memory required for data: 8294800
I0712 21:17:25.201658 14570 layer_factory.hpp:77] Creating layer pool2
I0712 21:17:25.201675 14570 net.cpp:91] Creating Layer pool2
I0712 21:17:25.201681 14570 net.cpp:425] pool2 <- conv2
I0712 21:17:25.201690 14570 net.cpp:399] pool2 -> pool2
I0712 21:17:25.201706 14570 net.cpp:141] Setting up pool2
I0712 21:17:25.201714 14570 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0712 21:17:25.201717 14570 net.cpp:156] Memory required for data: 8614800
I0712 21:17:25.201721 14570 layer_factory.hpp:77] Creating layer ip1
I0712 21:17:25.201735 14570 net.cpp:91] Creating Layer ip1
I0712 21:17:25.201740 14570 net.cpp:425] ip1 <- pool2
I0712 21:17:25.201748 14570 net.cpp:399] ip1 -> ip1
I0712 21:17:25.205469 14570 net.cpp:141] Setting up ip1
I0712 21:17:25.205500 14570 net.cpp:148] Top shape: 100 500 (50000)
I0712 21:17:25.205503 14570 net.cpp:156] Memory required for data: 8814800
I0712 21:17:25.205516 14570 layer_factory.hpp:77] Creating layer relu1
I0712 21:17:25.205526 14570 net.cpp:91] Creating Layer relu1
I0712 21:17:25.205530 14570 net.cpp:425] relu1 <- ip1
I0712 21:17:25.205535 14570 net.cpp:386] relu1 -> ip1 (in-place)
I0712 21:17:25.205543 14570 net.cpp:141] Setting up relu1
I0712 21:17:25.205546 14570 net.cpp:148] Top shape: 100 500 (50000)
I0712 21:17:25.205549 14570 net.cpp:156] Memory required for data: 9014800
I0712 21:17:25.205551 14570 layer_factory.hpp:77] Creating layer ip2
I0712 21:17:25.205559 14570 net.cpp:91] Creating Layer ip2
I0712 21:17:25.205562 14570 net.cpp:425] ip2 <- ip1
I0712 21:17:25.205567 14570 net.cpp:399] ip2 -> ip2
I0712 21:17:25.205620 14570 net.cpp:141] Setting up ip2
I0712 21:17:25.205624 14570 net.cpp:148] Top shape: 100 10 (1000)
I0712 21:17:25.205626 14570 net.cpp:156] Memory required for data: 9018800
I0712 21:17:25.205631 14570 layer_factory.hpp:77] Creating layer feat
I0712 21:17:25.205636 14570 net.cpp:91] Creating Layer feat
I0712 21:17:25.205639 14570 net.cpp:425] feat <- ip2
I0712 21:17:25.205643 14570 net.cpp:399] feat -> feat
I0712 21:17:25.205652 14570 net.cpp:141] Setting up feat
I0712 21:17:25.205656 14570 net.cpp:148] Top shape: 100 2 (200)
I0712 21:17:25.205658 14570 net.cpp:156] Memory required for data: 9019600
I0712 21:17:25.205663 14570 layer_factory.hpp:77] Creating layer conv1_p
I0712 21:17:25.205672 14570 net.cpp:91] Creating Layer conv1_p
I0712 21:17:25.205674 14570 net.cpp:425] conv1_p <- data_p
I0712 21:17:25.205679 14570 net.cpp:399] conv1_p -> conv1_p
I0712 21:17:25.205703 14570 net.cpp:141] Setting up conv1_p
I0712 21:17:25.205708 14570 net.cpp:148] Top shape: 100 20 24 24 (1152000)
I0712 21:17:25.205710 14570 net.cpp:156] Memory required for data: 13627600
I0712 21:17:25.205713 14570 net.cpp:484] Sharing parameters 'conv1_w' owned by layer 'conv1', param index 0
I0712 21:17:25.205718 14570 net.cpp:484] Sharing parameters 'conv1_b' owned by layer 'conv1', param index 1
I0712 21:17:25.205719 14570 layer_factory.hpp:77] Creating layer pool1_p
I0712 21:17:25.205724 14570 net.cpp:91] Creating Layer pool1_p
I0712 21:17:25.205726 14570 net.cpp:425] pool1_p <- conv1_p
I0712 21:17:25.205730 14570 net.cpp:399] pool1_p -> pool1_p
I0712 21:17:25.205737 14570 net.cpp:141] Setting up pool1_p
I0712 21:17:25.205741 14570 net.cpp:148] Top shape: 100 20 12 12 (288000)
I0712 21:17:25.205744 14570 net.cpp:156] Memory required for data: 14779600
I0712 21:17:25.205746 14570 layer_factory.hpp:77] Creating layer conv2_p
I0712 21:17:25.205754 14570 net.cpp:91] Creating Layer conv2_p
I0712 21:17:25.205756 14570 net.cpp:425] conv2_p <- pool1_p
I0712 21:17:25.205761 14570 net.cpp:399] conv2_p -> conv2_p
I0712 21:17:25.205957 14570 net.cpp:141] Setting up conv2_p
I0712 21:17:25.205962 14570 net.cpp:148] Top shape: 100 50 8 8 (320000)
I0712 21:17:25.205965 14570 net.cpp:156] Memory required for data: 16059600
I0712 21:17:25.205993 14570 net.cpp:484] Sharing parameters 'conv2_w' owned by layer 'conv2', param index 0
I0712 21:17:25.205997 14570 net.cpp:484] Sharing parameters 'conv2_b' owned by layer 'conv2', param index 1
I0712 21:17:25.205999 14570 layer_factory.hpp:77] Creating layer pool2_p
I0712 21:17:25.206004 14570 net.cpp:91] Creating Layer pool2_p
I0712 21:17:25.206007 14570 net.cpp:425] pool2_p <- conv2_p
I0712 21:17:25.206012 14570 net.cpp:399] pool2_p -> pool2_p
I0712 21:17:25.206018 14570 net.cpp:141] Setting up pool2_p
I0712 21:17:25.206022 14570 net.cpp:148] Top shape: 100 50 4 4 (80000)
I0712 21:17:25.206023 14570 net.cpp:156] Memory required for data: 16379600
I0712 21:17:25.206027 14570 layer_factory.hpp:77] Creating layer ip1_p
I0712 21:17:25.206032 14570 net.cpp:91] Creating Layer ip1_p
I0712 21:17:25.206033 14570 net.cpp:425] ip1_p <- pool2_p
I0712 21:17:25.206038 14570 net.cpp:399] ip1_p -> ip1_p
I0712 21:17:25.213330 14570 net.cpp:141] Setting up ip1_p
I0712 21:17:25.214711 14570 net.cpp:148] Top shape: 100 500 (50000)
I0712 21:17:25.214776 14570 net.cpp:156] Memory required for data: 16579600
I0712 21:17:25.214802 14570 net.cpp:484] Sharing parameters 'ip1_w' owned by layer 'ip1', param index 0
I0712 21:17:25.214824 14570 net.cpp:484] Sharing parameters 'ip1_b' owned by layer 'ip1', param index 1
I0712 21:17:25.214843 14570 layer_factory.hpp:77] Creating layer relu1_p
I0712 21:17:25.214869 14570 net.cpp:91] Creating Layer relu1_p
I0712 21:17:25.214889 14570 net.cpp:425] relu1_p <- ip1_p
I0712 21:17:25.214910 14570 net.cpp:386] relu1_p -> ip1_p (in-place)
I0712 21:17:25.214938 14570 net.cpp:141] Setting up relu1_p
I0712 21:17:25.214962 14570 net.cpp:148] Top shape: 100 500 (50000)
I0712 21:17:25.214980 14570 net.cpp:156] Memory required for data: 16779600
I0712 21:17:25.214999 14570 layer_factory.hpp:77] Creating layer ip2_p
I0712 21:17:25.215029 14570 net.cpp:91] Creating Layer ip2_p
I0712 21:17:25.215047 14570 net.cpp:425] ip2_p <- ip1_p
I0712 21:17:25.215072 14570 net.cpp:399] ip2_p -> ip2_p
I0712 21:17:25.215169 14570 net.cpp:141] Setting up ip2_p
I0712 21:17:25.215194 14570 net.cpp:148] Top shape: 100 10 (1000)
I0712 21:17:25.215212 14570 net.cpp:156] Memory required for data: 16783600
I0712 21:17:25.215234 14570 net.cpp:484] Sharing parameters 'ip2_w' owned by layer 'ip2', param index 0
I0712 21:17:25.215260 14570 net.cpp:484] Sharing parameters 'ip2_b' owned by layer 'ip2', param index 1
I0712 21:17:25.215278 14570 layer_factory.hpp:77] Creating layer feat_p
I0712 21:17:25.215302 14570 net.cpp:91] Creating Layer feat_p
I0712 21:17:25.215322 14570 net.cpp:425] feat_p <- ip2_p
I0712 21:17:25.215343 14570 net.cpp:399] feat_p -> feat_p
I0712 21:17:25.215374 14570 net.cpp:141] Setting up feat_p
I0712 21:17:25.215396 14570 net.cpp:148] Top shape: 100 2 (200)
I0712 21:17:25.215414 14570 net.cpp:156] Memory required for data: 16784400
I0712 21:17:25.215431 14570 net.cpp:484] Sharing parameters 'feat_w' owned by layer 'feat', param index 0
I0712 21:17:25.215451 14570 net.cpp:484] Sharing parameters 'feat_b' owned by layer 'feat', param index 1
I0712 21:17:25.215468 14570 layer_factory.hpp:77] Creating layer loss
I0712 21:17:25.215494 14570 net.cpp:91] Creating Layer loss
I0712 21:17:25.215565 14570 net.cpp:425] loss <- feat
I0712 21:17:25.215589 14570 net.cpp:425] loss <- feat_p
I0712 21:17:25.215608 14570 net.cpp:425] loss <- sim
I0712 21:17:25.215629 14570 net.cpp:399] loss -> loss
I0712 21:17:25.215659 14570 net.cpp:141] Setting up loss
I0712 21:17:25.215682 14570 net.cpp:148] Top shape: (1)
I0712 21:17:25.215698 14570 net.cpp:151]     with loss weight 1
I0712 21:17:25.215838 14570 net.cpp:156] Memory required for data: 16784404
I0712 21:17:25.215864 14570 net.cpp:217] loss needs backward computation.
I0712 21:17:25.215884 14570 net.cpp:217] feat_p needs backward computation.
I0712 21:17:25.215903 14570 net.cpp:217] ip2_p needs backward computation.
I0712 21:17:25.215919 14570 net.cpp:217] relu1_p needs backward computation.
I0712 21:17:25.215937 14570 net.cpp:217] ip1_p needs backward computation.
I0712 21:17:25.215979 14570 net.cpp:217] pool2_p needs backward computation.
I0712 21:17:25.215997 14570 net.cpp:217] conv2_p needs backward computation.
I0712 21:17:25.216015 14570 net.cpp:217] pool1_p needs backward computation.
I0712 21:17:25.216032 14570 net.cpp:217] conv1_p needs backward computation.
I0712 21:17:25.216050 14570 net.cpp:217] feat needs backward computation.
I0712 21:17:25.216069 14570 net.cpp:217] ip2 needs backward computation.
I0712 21:17:25.216087 14570 net.cpp:217] relu1 needs backward computation.
I0712 21:17:25.216105 14570 net.cpp:217] ip1 needs backward computation.
I0712 21:17:25.216120 14570 net.cpp:217] pool2 needs backward computation.
I0712 21:17:25.216132 14570 net.cpp:217] conv2 needs backward computation.
I0712 21:17:25.216145 14570 net.cpp:217] pool1 needs backward computation.
I0712 21:17:25.216157 14570 net.cpp:217] conv1 needs backward computation.
I0712 21:17:25.216171 14570 net.cpp:219] slice_pair does not need backward computation.
I0712 21:17:25.216183 14570 net.cpp:219] pair_data does not need backward computation.
I0712 21:17:25.216195 14570 net.cpp:261] This network produces output loss
I0712 21:17:25.216224 14570 net.cpp:274] Network initialization done.
I0712 21:17:25.216331 14570 solver.cpp:60] Solver scaffolding done.
I0712 21:17:25.216362 14570 caffe.cpp:219] Starting Optimization
I0712 21:17:25.216367 14570 solver.cpp:279] Solving mnist_siamese_train_test
I0712 21:17:25.216369 14570 solver.cpp:280] Learning Rate Policy: inv
I0712 21:17:25.216675 14570 solver.cpp:337] Iteration 0, Testing net (#0)
I0712 21:17:25.228021 14577 blocking_queue.cpp:50] Waiting for data
I0712 21:17:35.522250 14570 solver.cpp:404]     Test net output #0: loss = 0.195107 (* 1 = 0.195107 loss)
I0712 21:17:35.718438 14570 solver.cpp:228] Iteration 0, loss = 0.194362
I0712 21:17:35.718597 14570 solver.cpp:244]     Train net output #0: loss = 0.194362 (* 1 = 0.194362 loss)
I0712 21:17:35.718632 14570 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I0712 21:17:52.053339 14570 solver.cpp:228] Iteration 100, loss = 0.0458093
I0712 21:17:52.053488 14570 solver.cpp:244]     Train net output #0: loss = 0.0458093 (* 1 = 0.0458093 loss)
I0712 21:17:52.053514 14570 sgd_solver.cpp:106] Iteration 100, lr = 0.00992565
I0712 21:18:08.224051 14570 solver.cpp:228] Iteration 200, loss = 0.0244547
I0712 21:18:08.224282 14570 solver.cpp:244]     Train net output #0: loss = 0.0244547 (* 1 = 0.0244547 loss)
I0712 21:18:08.224313 14570 sgd_solver.cpp:106] Iteration 200, lr = 0.00985258
I0712 21:18:26.266974 14570 solver.cpp:228] Iteration 300, loss = 0.0224128
I0712 21:18:26.267170 14570 solver.cpp:244]     Train net output #0: loss = 0.0224128 (* 1 = 0.0224128 loss)
I0712 21:18:26.267199 14570 sgd_solver.cpp:106] Iteration 300, lr = 0.00978075
I0712 21:18:44.056150 14570 solver.cpp:228] Iteration 400, loss = 0.012724
I0712 21:18:44.056325 14570 solver.cpp:244]     Train net output #0: loss = 0.012724 (* 1 = 0.012724 loss)
I0712 21:18:44.056339 14570 sgd_solver.cpp:106] Iteration 400, lr = 0.00971013
I0712 21:19:00.005910 14570 solver.cpp:337] Iteration 500, Testing net (#0)
I0712 21:19:11.057685 14570 solver.cpp:404]     Test net output #0: loss = 0.12127 (* 1 = 0.12127 loss)
I0712 21:19:11.294075 14570 solver.cpp:228] Iteration 500, loss = 0.0284784
I0712 21:19:11.294203 14570 solver.cpp:244]     Train net output #0: loss = 0.0284784 (* 1 = 0.0284784 loss)
I0712 21:19:11.294225 14570 sgd_solver.cpp:106] Iteration 500, lr = 0.00964069
I0712 21:19:28.067682 14570 solver.cpp:228] Iteration 600, loss = 0.0218722
I0712 21:19:28.068014 14570 solver.cpp:244]     Train net output #0: loss = 0.0218722 (* 1 = 0.0218722 loss)
I0712 21:19:28.068086 14570 sgd_solver.cpp:106] Iteration 600, lr = 0.0095724
I0712 21:19:43.954432 14570 solver.cpp:228] Iteration 700, loss = 0.0199245
I0712 21:19:43.954488 14570 solver.cpp:244]     Train net output #0: loss = 0.0199245 (* 1 = 0.0199245 loss)
I0712 21:19:43.954499 14570 sgd_solver.cpp:106] Iteration 700, lr = 0.00950522
I0712 21:19:59.797129 14570 solver.cpp:228] Iteration 800, loss = 0.0216117
I0712 21:19:59.797602 14570 solver.cpp:244]     Train net output #0: loss = 0.0216117 (* 1 = 0.0216117 loss)
I0712 21:19:59.797673 14570 sgd_solver.cpp:106] Iteration 800, lr = 0.00943913
I0712 21:20:16.002032 14570 solver.cpp:228] Iteration 900, loss = 0.0251292
I0712 21:20:16.002202 14570 solver.cpp:244]     Train net output #0: loss = 0.0251292 (* 1 = 0.0251292 loss)
I0712 21:20:16.002230 14570 sgd_solver.cpp:106] Iteration 900, lr = 0.00937411
I0712 21:20:31.951104 14570 solver.cpp:337] Iteration 1000, Testing net (#0)
I0712 21:20:42.068326 14570 solver.cpp:404]     Test net output #0: loss = 0.129991 (* 1 = 0.129991 loss)
I0712 21:20:42.255992 14570 solver.cpp:228] Iteration 1000, loss = 0.0123402
I0712 21:20:42.256253 14570 solver.cpp:244]     Train net output #0: loss = 0.0123402 (* 1 = 0.0123402 loss)
I0712 21:20:42.256315 14570 sgd_solver.cpp:106] Iteration 1000, lr = 0.00931012
I0712 21:20:58.164875 14570 solver.cpp:228] Iteration 1100, loss = 0.0152619
I0712 21:20:58.164993 14570 solver.cpp:244]     Train net output #0: loss = 0.0152619 (* 1 = 0.0152619 loss)
I0712 21:20:58.165019 14570 sgd_solver.cpp:106] Iteration 1100, lr = 0.00924715
I0712 21:21:14.227571 14570 solver.cpp:228] Iteration 1200, loss = 0.011204
I0712 21:21:14.227754 14570 solver.cpp:244]     Train net output #0: loss = 0.011204 (* 1 = 0.011204 loss)
I0712 21:21:14.227766 14570 sgd_solver.cpp:106] Iteration 1200, lr = 0.00918515
I0712 21:21:30.110028 14570 solver.cpp:228] Iteration 1300, loss = 0.00347156
I0712 21:21:30.110085 14570 solver.cpp:244]     Train net output #0: loss = 0.00347158 (* 1 = 0.00347158 loss)
I0712 21:21:30.110095 14570 sgd_solver.cpp:106] Iteration 1300, lr = 0.00912412
I0712 21:21:46.158831 14570 solver.cpp:228] Iteration 1400, loss = 0.0148366
I0712 21:21:46.159147 14570 solver.cpp:244]     Train net output #0: loss = 0.0148366 (* 1 = 0.0148366 loss)
I0712 21:21:46.159178 14570 sgd_solver.cpp:106] Iteration 1400, lr = 0.00906403
I0712 21:22:02.610122 14570 solver.cpp:337] Iteration 1500, Testing net (#0)
I0712 21:22:13.016273 14570 solver.cpp:404]     Test net output #0: loss = 0.121543 (* 1 = 0.121543 loss)
I0712 21:22:13.207818 14570 solver.cpp:228] Iteration 1500, loss = 0.0155288
I0712 21:22:13.208022 14570 solver.cpp:244]     Train net output #0: loss = 0.0155288 (* 1 = 0.0155288 loss)
I0712 21:22:13.208077 14570 sgd_solver.cpp:106] Iteration 1500, lr = 0.00900485
I0712 21:22:37.687105 14570 solver.cpp:228] Iteration 1600, loss = 0.0160987
I0712 21:22:37.687222 14570 solver.cpp:244]     Train net output #0: loss = 0.0160987 (* 1 = 0.0160987 loss)
I0712 21:22:37.687234 14570 sgd_solver.cpp:106] Iteration 1600, lr = 0.00894657
I0712 21:22:56.827772 14570 solver.cpp:228] Iteration 1700, loss = 0.00923702
I0712 21:22:56.827844 14570 solver.cpp:244]     Train net output #0: loss = 0.00923704 (* 1 = 0.00923704 loss)
I0712 21:22:56.827862 14570 sgd_solver.cpp:106] Iteration 1700, lr = 0.00888916
I0712 21:23:13.014992 14570 solver.cpp:228] Iteration 1800, loss = 0.0107407
I0712 21:23:13.015075 14570 solver.cpp:244]     Train net output #0: loss = 0.0107407 (* 1 = 0.0107407 loss)
I0712 21:23:13.015086 14570 sgd_solver.cpp:106] Iteration 1800, lr = 0.0088326
I0712 21:23:29.814903 14570 solver.cpp:228] Iteration 1900, loss = 0.00647233
I0712 21:23:29.814962 14570 solver.cpp:244]     Train net output #0: loss = 0.00647234 (* 1 = 0.00647234 loss)
I0712 21:23:29.814973 14570 sgd_solver.cpp:106] Iteration 1900, lr = 0.00877687
I0712 21:23:47.868218 14570 solver.cpp:337] Iteration 2000, Testing net (#0)
I0712 21:23:59.766338 14570 solver.cpp:404]     Test net output #0: loss = 0.119711 (* 1 = 0.119711 loss)
I0712 21:23:59.957373 14570 solver.cpp:228] Iteration 2000, loss = 0.0064321
I0712 21:23:59.957617 14570 solver.cpp:244]     Train net output #0: loss = 0.00643211 (* 1 = 0.00643211 loss)
I0712 21:23:59.957689 14570 sgd_solver.cpp:106] Iteration 2000, lr = 0.00872196
I0712 21:24:17.465878 14570 solver.cpp:228] Iteration 2100, loss = 0.0144977
I0712 21:24:17.466027 14570 solver.cpp:244]     Train net output #0: loss = 0.0144977 (* 1 = 0.0144977 loss)
I0712 21:24:17.466064 14570 sgd_solver.cpp:106] Iteration 2100, lr = 0.00866784
I0712 21:24:36.052991 14570 solver.cpp:228] Iteration 2200, loss = 0.018704
I0712 21:24:36.053458 14570 solver.cpp:244]     Train net output #0: loss = 0.018704 (* 1 = 0.018704 loss)
I0712 21:24:36.053534 14570 sgd_solver.cpp:106] Iteration 2200, lr = 0.0086145
I0712 21:24:52.811558 14570 solver.cpp:228] Iteration 2300, loss = 0.0178259
I0712 21:24:52.811615 14570 solver.cpp:244]     Train net output #0: loss = 0.0178259 (* 1 = 0.0178259 loss)
I0712 21:24:52.811626 14570 sgd_solver.cpp:106] Iteration 2300, lr = 0.00856192
I0712 21:25:08.708799 14570 solver.cpp:228] Iteration 2400, loss = 0.0166433
I0712 21:25:08.709383 14570 solver.cpp:244]     Train net output #0: loss = 0.0166434 (* 1 = 0.0166434 loss)
I0712 21:25:08.709400 14570 sgd_solver.cpp:106] Iteration 2400, lr = 0.00851008
I0712 21:25:24.554023 14570 solver.cpp:337] Iteration 2500, Testing net (#0)
I0712 21:25:34.569751 14570 solver.cpp:404]     Test net output #0: loss = 0.124 (* 1 = 0.124 loss)
I0712 21:25:34.763003 14570 solver.cpp:228] Iteration 2500, loss = 0.00681522
I0712 21:25:34.763062 14570 solver.cpp:244]     Train net output #0: loss = 0.00681523 (* 1 = 0.00681523 loss)
I0712 21:25:34.763072 14570 sgd_solver.cpp:106] Iteration 2500, lr = 0.00845897
I0712 21:25:50.551301 14570 solver.cpp:228] Iteration 2600, loss = 0.00843293
I0712 21:25:50.551839 14570 solver.cpp:244]     Train net output #0: loss = 0.00843295 (* 1 = 0.00843295 loss)
I0712 21:25:50.551853 14570 sgd_solver.cpp:106] Iteration 2600, lr = 0.00840857
I0712 21:26:06.329802 14570 solver.cpp:228] Iteration 2700, loss = 0.0144226
I0712 21:26:06.329995 14570 solver.cpp:244]     Train net output #0: loss = 0.0144226 (* 1 = 0.0144226 loss)
I0712 21:26:06.330027 14570 sgd_solver.cpp:106] Iteration 2700, lr = 0.00835886
I0712 21:26:22.353924 14570 solver.cpp:228] Iteration 2800, loss = 0.00810728
I0712 21:26:22.354362 14570 solver.cpp:244]     Train net output #0: loss = 0.00810729 (* 1 = 0.00810729 loss)
I0712 21:26:22.354411 14570 sgd_solver.cpp:106] Iteration 2800, lr = 0.00830984
I0712 21:26:38.392068 14570 solver.cpp:228] Iteration 2900, loss = 0.0181031
I0712 21:26:38.392271 14570 solver.cpp:244]     Train net output #0: loss = 0.0181031 (* 1 = 0.0181031 loss)
I0712 21:26:38.392302 14570 sgd_solver.cpp:106] Iteration 2900, lr = 0.00826148
I0712 21:26:54.029409 14570 solver.cpp:337] Iteration 3000, Testing net (#0)
I0712 21:27:03.918357 14570 solver.cpp:404]     Test net output #0: loss = 0.123762 (* 1 = 0.123762 loss)
I0712 21:27:04.101351 14570 solver.cpp:228] Iteration 3000, loss = 0.00708447
I0712 21:27:04.101411 14570 solver.cpp:244]     Train net output #0: loss = 0.00708448 (* 1 = 0.00708448 loss)
I0712 21:27:04.101421 14570 sgd_solver.cpp:106] Iteration 3000, lr = 0.00821377
I0712 21:27:19.998373 14570 solver.cpp:228] Iteration 3100, loss = 0.00752305
I0712 21:27:19.998548 14570 solver.cpp:244]     Train net output #0: loss = 0.00752306 (* 1 = 0.00752306 loss)
I0712 21:27:19.998579 14570 sgd_solver.cpp:106] Iteration 3100, lr = 0.0081667
I0712 21:27:35.782135 14570 solver.cpp:228] Iteration 3200, loss = 0.0161517
I0712 21:27:35.782220 14570 solver.cpp:244]     Train net output #0: loss = 0.0161517 (* 1 = 0.0161517 loss)
I0712 21:27:35.782230 14570 sgd_solver.cpp:106] Iteration 3200, lr = 0.00812025
I0712 21:27:51.679491 14570 solver.cpp:228] Iteration 3300, loss = 0.0110173
I0712 21:27:51.679555 14570 solver.cpp:244]     Train net output #0: loss = 0.0110173 (* 1 = 0.0110173 loss)
I0712 21:27:51.679566 14570 sgd_solver.cpp:106] Iteration 3300, lr = 0.00807442
I0712 21:28:07.547864 14570 solver.cpp:228] Iteration 3400, loss = 0.0222057
I0712 21:28:07.548069 14570 solver.cpp:244]     Train net output #0: loss = 0.0222057 (* 1 = 0.0222057 loss)
I0712 21:28:07.548099 14570 sgd_solver.cpp:106] Iteration 3400, lr = 0.00802918
I0712 21:28:23.448781 14570 solver.cpp:337] Iteration 3500, Testing net (#0)
I0712 21:28:33.329844 14570 solver.cpp:404]     Test net output #0: loss = 0.120337 (* 1 = 0.120337 loss)
I0712 21:28:33.516593 14570 solver.cpp:228] Iteration 3500, loss = 0.0100801
I0712 21:28:33.516649 14570 solver.cpp:244]     Train net output #0: loss = 0.0100801 (* 1 = 0.0100801 loss)
I0712 21:28:33.516660 14570 sgd_solver.cpp:106] Iteration 3500, lr = 0.00798454
I0712 21:28:49.677835 14570 solver.cpp:228] Iteration 3600, loss = 0.00676061
I0712 21:28:49.681931 14570 solver.cpp:244]     Train net output #0: loss = 0.00676061 (* 1 = 0.00676061 loss)
I0712 21:28:49.681957 14570 sgd_solver.cpp:106] Iteration 3600, lr = 0.00794046
I0712 21:29:05.382200 14570 solver.cpp:228] Iteration 3700, loss = 0.00808711
I0712 21:29:05.382256 14570 solver.cpp:244]     Train net output #0: loss = 0.0080871 (* 1 = 0.0080871 loss)
I0712 21:29:05.382266 14570 sgd_solver.cpp:106] Iteration 3700, lr = 0.00789695
I0712 21:29:20.988739 14570 solver.cpp:228] Iteration 3800, loss = 0.00711956
I0712 21:29:20.988862 14570 solver.cpp:244]     Train net output #0: loss = 0.00711956 (* 1 = 0.00711956 loss)
I0712 21:29:20.988873 14570 sgd_solver.cpp:106] Iteration 3800, lr = 0.007854
I0712 21:29:36.599630 14570 solver.cpp:228] Iteration 3900, loss = 0.00901701
I0712 21:29:36.599865 14570 solver.cpp:244]     Train net output #0: loss = 0.00901701 (* 1 = 0.00901701 loss)
I0712 21:29:36.599983 14570 sgd_solver.cpp:106] Iteration 3900, lr = 0.00781158
I0712 21:29:52.033798 14570 solver.cpp:337] Iteration 4000, Testing net (#0)
I0712 21:30:02.163712 14570 solver.cpp:404]     Test net output #0: loss = 0.127009 (* 1 = 0.127009 loss)
I0712 21:30:02.351729 14570 solver.cpp:228] Iteration 4000, loss = 0.00878836
I0712 21:30:02.351793 14570 solver.cpp:244]     Train net output #0: loss = 0.00878836 (* 1 = 0.00878836 loss)
I0712 21:30:02.351804 14570 sgd_solver.cpp:106] Iteration 4000, lr = 0.0077697
I0712 21:30:18.058738 14570 solver.cpp:228] Iteration 4100, loss = 0.0112064
I0712 21:30:18.058794 14570 solver.cpp:244]     Train net output #0: loss = 0.0112064 (* 1 = 0.0112064 loss)
I0712 21:30:18.058805 14570 sgd_solver.cpp:106] Iteration 4100, lr = 0.00772833
I0712 21:30:33.766106 14570 solver.cpp:228] Iteration 4200, loss = 0.00721028
I0712 21:30:33.766186 14570 solver.cpp:244]     Train net output #0: loss = 0.00721029 (* 1 = 0.00721029 loss)
I0712 21:30:33.766197 14570 sgd_solver.cpp:106] Iteration 4200, lr = 0.00768748
I0712 21:30:49.377938 14570 solver.cpp:228] Iteration 4300, loss = 0.0186559
I0712 21:30:49.377997 14570 solver.cpp:244]     Train net output #0: loss = 0.0186559 (* 1 = 0.0186559 loss)
I0712 21:30:49.378008 14570 sgd_solver.cpp:106] Iteration 4300, lr = 0.00764712
I0712 21:31:05.419921 14570 solver.cpp:228] Iteration 4400, loss = 0.00645328
I0712 21:31:05.420173 14570 solver.cpp:244]     Train net output #0: loss = 0.00645329 (* 1 = 0.00645329 loss)
I0712 21:31:05.420245 14570 sgd_solver.cpp:106] Iteration 4400, lr = 0.00760726
I0712 21:31:21.175815 14570 solver.cpp:337] Iteration 4500, Testing net (#0)
I0712 21:31:31.538048 14570 solver.cpp:404]     Test net output #0: loss = 0.127361 (* 1 = 0.127361 loss)
I0712 21:31:31.737704 14570 solver.cpp:228] Iteration 4500, loss = 0.00848432
I0712 21:31:31.737936 14570 solver.cpp:244]     Train net output #0: loss = 0.00848433 (* 1 = 0.00848433 loss)
I0712 21:31:31.738009 14570 sgd_solver.cpp:106] Iteration 4500, lr = 0.00756788
I0712 21:31:47.687533 14570 solver.cpp:228] Iteration 4600, loss = 0.0110403
I0712 21:31:47.687721 14570 solver.cpp:244]     Train net output #0: loss = 0.0110403 (* 1 = 0.0110403 loss)
I0712 21:31:47.687736 14570 sgd_solver.cpp:106] Iteration 4600, lr = 0.00752897
I0712 21:32:06.177189 14570 solver.cpp:228] Iteration 4700, loss = 0.00366106
I0712 21:32:06.177247 14570 solver.cpp:244]     Train net output #0: loss = 0.00366106 (* 1 = 0.00366106 loss)
I0712 21:32:06.177256 14570 sgd_solver.cpp:106] Iteration 4700, lr = 0.00749052
I0712 21:32:23.453017 14570 solver.cpp:228] Iteration 4800, loss = 0.00707395
I0712 21:32:23.453768 14570 solver.cpp:244]     Train net output #0: loss = 0.00707395 (* 1 = 0.00707395 loss)
I0712 21:32:23.453809 14570 sgd_solver.cpp:106] Iteration 4800, lr = 0.00745253
I0712 21:32:39.656713 14570 solver.cpp:228] Iteration 4900, loss = 0.00897119
I0712 21:32:39.656771 14570 solver.cpp:244]     Train net output #0: loss = 0.00897118 (* 1 = 0.00897118 loss)
I0712 21:32:39.656781 14570 sgd_solver.cpp:106] Iteration 4900, lr = 0.00741498
I0712 21:32:55.437173 14570 solver.cpp:454] Snapshotting to binary proto file examples/siamese/My_mnist_siamese_0to6l_iter_5000.caffemodel
I0712 21:32:55.446854 14570 sgd_solver.cpp:273] Snapshotting solver state to binary proto file examples/siamese/My_mnist_siamese_0to6l_iter_5000.solverstate
I0712 21:32:55.545656 14570 solver.cpp:317] Iteration 5000, loss = 0.00864311
I0712 21:32:55.545753 14570 solver.cpp:337] Iteration 5000, Testing net (#0)
I0712 21:33:05.648120 14570 solver.cpp:404]     Test net output #0: loss = 0.118746 (* 1 = 0.118746 loss)
I0712 21:33:05.648165 14570 solver.cpp:322] Optimization Done.
I0712 21:33:05.648170 14570 caffe.cpp:222] Optimization Done.
